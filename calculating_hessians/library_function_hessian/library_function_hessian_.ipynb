{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"library_function_hessian_.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO9FhVvx0IsRKuHWOr14wPZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"DkJHwqCq6EWp","colab":{"base_uri":"https://localhost:8080/","height":88},"executionInfo":{"status":"ok","timestamp":1614329042631,"user_tz":0,"elapsed":558,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}},"outputId":"e63e1ef5-36f6-4b14-d00c-ccd499d119ba"},"source":["\"\"\"\n","---------------------------------------------------------------------------------------------------------------------------------------------------------\n","This document contains a library-based method for calculating the Hessian. The approach works, although I have kept the code of my failed\n","attempts towards the end of file.\n","---------------------------------------------------------------------------------------------------------------------------------------------------------\n","\"\"\""],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\nThis document contains a library-based method for calculating the Hessian. The approach appears to work, although I have kept the code of my failed\\nattempts towards the end of file.\\n---------------------------------------------------------------------------------------------------------------------------------------------------------\\n'"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"id":"k84Sxg_o2xCR"},"source":["import torch\n","from torch import nn\n","import numpy as np\n","from torch.nn import Module\n","import torch.nn.functional as F\n","import time\n","import matplotlib.pyplot as plt\n","import torch.optim as optim"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m9Fv0VNcpp2j"},"source":["device = torch.device('cuda:' + str(0) if torch.cuda.is_available() else 'cpu')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l8l-CWsThCtd"},"source":["class ODEFunc(nn.Module):\n","\n","    def __init__(self):\n","        super(ODEFunc, self).__init__()\n","\n","        self.net = nn.Sequential(\n","            nn.Linear(2, 50),\n","            nn.Tanh(),\n","            nn.Linear(50, 2),\n","        )\n","\n","        for m in self.net.modules():\n","          if isinstance(m, nn.Linear):\n","            nn.init.normal_(m.weight, mean=0, std=0.1)\n","            nn.init.constant_(m.bias, val=0)\n","\n","    def forward(self, y):\n","        return self.net(y**3)\n","\n","func = ODEFunc()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TK0Ov5FiDF3w"},"source":["\"\"\"\n","------------------------------------------------------------------------------------------------------------------------\n","This approach works, and produces a 252 x 252 matrix. It is a slightly less broken-down version of the code used below.\n","------------------------------------------------------------------------------------------------------------------------\n","\"\"\"\n","\n","def get_loss_square(params_vector):\n","\n","  a = params_vector[:100].reshape([50, 2])\n","  b = params_vector[100:150].reshape([50])\n","  c = params_vector[150:250].reshape([2, 50])\n","  d = params_vector[250:252].reshape([2])\n","\n","  input = torch.tensor([1.0,1.0])\n","  y = torch.tensor([2.0,0.0])\n","\n","  x = F.linear(input, a, b)\n","  m = nn.Tanh()\n","  x = m(x)\n","  x = F.linear(x, c, d)\n","\n","  loss = torch.linalg.norm(y-x)\n","  return loss\n","\n","def get_hessian(net):\n","\n","  param_tensors = net.parameters()\n","  params_vector = torch.tensor([])\n","  for param in param_tensors:\n","    vec = torch.reshape(param, (-1,))\n","    params_vector = torch.cat((params_vector, vec))\n","\n","  hessian = torch.autograd.functional.hessian(get_loss_square, params_vector)\n","  return hessian"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nu5s07pkiTnh"},"source":["\"\"\"\n","------------------------------------------------------------------------------------------------------------------------\n","This approach works, and produces a 252 x 252 matrix. It is a slightly more broken-down version of the code above.\n","------------------------------------------------------------------------------------------------------------------------\n","\"\"\"\n","def net(a, b, c, d):\n","\n","  input = torch.tensor([1.0,1.0])\n","\n","  x = F.linear(input, a, b)\n","  m = nn.Tanh()\n","  x = m(x)\n","  x = F.linear(x, c, d)\n","\n","  return x\n","\n","\n","def get_loss_square_1(params_vector):\n","\n","  a = params_vector[:100].reshape([50, 2])\n","  b = params_vector[100:150].reshape([50])\n","  c = params_vector[150:250].reshape([2, 50])\n","  d = params_vector[250:252].reshape([2])\n","\n","  y = torch.tensor([2.0,0.0])\n","  x = net(a, b, c, d)\n","  \n","  loss = torch.linalg.norm(y-x)\n","  return loss\n","\n","def get_hessian_1(net):\n","\n","  param_tensors = net.parameters()\n","  params_vector = torch.tensor([])\n","  for param in param_tensors:\n","    vec = torch.reshape(param, (-1,))\n","    params_vector = torch.cat((params_vector, vec))\n","\n","  hessian = torch.autograd.functional.hessian(get_loss_square_1, params_vector)\n","  return hessian"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IzQn2c4tzM_y"},"source":["\"\"\"\n","-------------------------------------------------------------------------------------------------------------------------------------\n","This approach works, and produces a 252 x 252 matrix. It is built using library functions and a nn.Module, which means \n","it can more easily be used in the context of NODEs. This is the method that I have implemented during the training of \n","simple NODEs.\n","-------------------------------------------------------------------------------------------------------------------------------------\n","\"\"\"\n","\n","class Network(nn.Module):\n","\n","  def __init__(self, a, b, c, d):\n","    super(Network, self).__init__()\n","    self.a = a\n","    self.b = b\n","    self.c = c\n","    self.d = d\n","\n","  def forward(self, y):\n","    x = F.linear(y, self.a, self.b)\n","    m = nn.Tanh()\n","    x = m(x)\n","    x = F.linear(x, self.c, self.d)\n","    return x\n","\n","\n","def get_loss_square_2(params_vector):\n","\n","  a = params_vector[0:100].reshape([50, 2])\n","  b = params_vector[100:150].reshape([50])\n","  c = params_vector[150:250].reshape([2, 50])\n","  d = params_vector[250:252].reshape([2])\n","\n","  neural_net = Network(a, b, c, d).to(device)\n","  input = torch.tensor([1.0,1.0]).to(device)\n","  target = torch.tensor([2.0,0.0]).to(device)\n","  pred_y = neural_net(input)\n","  \n","  loss = torch.linalg.norm(pred_y - target) \n","  return loss\n","\n","def get_library_hessian(net):\n","\n","  param_tensors = net.parameters()\n","  params_vector = torch.tensor([]).to(device)   \n","  for param in param_tensors:\n","    vec = torch.reshape(param, (-1,)).to(device)\n","    params_vector = torch.cat((params_vector, vec))\n","\n","  hessian = torch.autograd.functional.hessian(get_loss_square_2, params_vector)\n","  return hessian"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":324},"id":"CfhaHZFTf1TD","executionInfo":{"status":"error","timestamp":1614329161079,"user_tz":0,"elapsed":434,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}},"outputId":"27deffd2-1eb1-443c-962a-1b19797ed4d7"},"source":["func = ODEFunc()\n","hessian = get_library_hessian(func)\n"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-6b4cfaef885b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mODEFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhessian\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_library_hessian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-11-2625d9958a95>\u001b[0m in \u001b[0;36mget_library_hessian\u001b[0;34m(net)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m   \u001b[0mhessian\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhessian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_loss_square_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mhessian\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/functional.py\u001b[0m in \u001b[0;36mhessian\u001b[0;34m(func, inputs, create_graph, strict)\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjacobian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjac_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_tuple_postprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_inputs_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_inputs_tuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/functional.py\u001b[0m in \u001b[0;36mjacobian\u001b[0;34m(func, inputs, create_graph, strict)\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_grad_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m     is_outputs_tuple, outputs = _as_tuple(outputs,\n\u001b[1;32m    427\u001b[0m                                           \u001b[0;34m\"outputs of the user-provided function\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/functional.py\u001b[0m in \u001b[0;36mjac_func\u001b[0;34m(*inp)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mjac_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m         \u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjacobian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_single_output_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m         \u001b[0m_check_requires_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"jacobian\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/functional.py\u001b[0m in \u001b[0;36mjacobian\u001b[0;34m(func, inputs, create_graph, strict)\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_grad_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m     is_outputs_tuple, outputs = _as_tuple(outputs,\n\u001b[1;32m    427\u001b[0m                                           \u001b[0;34m\"outputs of the user-provided function\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/functional.py\u001b[0m in \u001b[0;36mensure_single_output_function\u001b[0;34m(*inp)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mensure_single_output_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m         \u001b[0mis_out_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_as_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"outputs of the user-provided function\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hessian\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0m_check_requires_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"outputs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-2625d9958a95>\u001b[0m in \u001b[0;36mget_loss_square_2\u001b[0;34m(params_vector)\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams_vector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m252\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m   \u001b[0mneural_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m   \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'param_indices' is not defined"]}]},{"cell_type":"code","metadata":{"id":"NMQIridq6Yw2"},"source":["\"\"\"\n","------------------------------------------------------------------------------------------------------------------------\n","All of the below are my failed attempts!\n","------------------------------------------------------------------------------------------------------------------------\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PiCjYOYcoryN"},"source":["\"\"\"\n","---------------------------------------------------------------------------------------------------------------------------\n","This is my attempted adaption of the above code to using a nn.Module function, so that it can be used with the adjoint \n","method for odeint(). It (erroneously) produces a matrix of zeros.\n","---------------------------------------------------------------------------------------------------------------------------\n","\"\"\"\n","\n","def net(input, a, b, c, d):\n","\n","  x = F.linear(input**3, a, b)\n","  m = nn.Tanh()\n","  x = m(x)\n","  x = F.linear(x, c, d)\n","\n","  return x\n","\n","class Network(nn.Module):\n","\n","  def __init__(self, a, b, c, d):\n","    super(Network, self).__init__()\n","    self.a = a\n","    self.b = b\n","    self.c = c\n","    self.d = d\n","\n","  def forward(self, t, y):\n","    x = net(y, self.a, self.b, self.c, self.d)\n","    return x\n","\n","def get_loss_square(params_vector):\n","\n","  a = params_vector[:100].reshape([50, 2]).to(device)\n","  b = params_vector[100:150].reshape([50]).to(device)\n","  c = params_vector[150:250].reshape([2, 50]).to(device)\n","  d = params_vector[250:252].reshape([2]).to(device)\n","\n","  true_y = torch.tensor([2.0,2.0]).to(device)\n","  neural_net = Network(a,b,c,d)\n","  pred_y = odeint(neural_net, true_y0, t)\n","\n","  loss = torch.linalg.norm(true_y-pred_y)\n","  return loss\n","\n","def get_hessian(net):\n","\n","  param_tensors = net.parameters()\n","  params_vector = torch.tensor([]).to(device)\n","  for param in param_tensors:\n","    vec = torch.reshape(param, (-1,)).to(device)\n","    params_vector = torch.cat((params_vector, vec)).to(device)\n","\n","  hessian = torch.autograd.functional.hessian(get_loss_square, params_vector)\n","  return hessian\n","\n","get_hessian(func)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_41VXzoClNq2"},"source":["\"\"\"\n","------------------------------------------------------------------------------------------------------------------------\n","This approach seems to work fine, but I don't yet understand how to interpret the outcome.\n","------------------------------------------------------------------------------------------------------------------------\n","\"\"\"\n","\n","def get_loss(w, x, y, z):\n","  \"\"\"\n","  Calculates the loss by taking the network parameters as inputs.\n","  Inputs: w, x, y, z: these are torch.tensor() objects of shape [50, 2], [50], [2, 50] and [2] respectively.\n","  \"\"\"\n","\n","  #Calculate the network input and target values.\n","  input = torch.tensor([1.0,1.0])\n","  target = torch.tensor([2.0,0.0])\n","\n","  #Calculate the network output.\n","  v = F.linear(input, w, x)\n","  m = nn.Tanh()\n","  v = m(v)\n","  v = F.linear(v, y, z)\n","  \n","  #Obtain the loss.\n","  loss = torch.linalg.norm(target-v)\n","  return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sny_ES6Gknxs"},"source":["if __name__ == '__main__':\n","\n","  #functional.hessian() requires a tuple of tensors that are the inputs to the function get_loss())\n","  inputs = tuple([param for param in func.parameters()])\n","  hessians = torch.autograd.functional.hessian(get_loss, inputs) \n","\n","  \"\"\"\n","  param_names = ('w', 'x', 'y', 'z')\n","  for d_name, d_hessians in zip(param_names, hessians):\n","    for dd_name, dd_hessian in zip(param_names, d_hessians):\n","        print(f'dl/d{dd_name}d{d_name} = ' + str(dd_hessian.shape))\n","        #print(f'dl/d{dd_name}d{d_name} = \\n{dd_hessian}\\n')\n","  \"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZusK_kP_nC0N"},"source":["\"\"\"\n","------------------------------------------------------------------------------------------------------------------------\n","This approach uses nn.Module parameters and does not seem to work; the Hessian obtained is simply a matrix of zeroes.\n","More detail on this can perhaps be found here:\n","https://discuss.pytorch.org/t/using-autograd-functional-jacobian-hessian-with-respect-to-nn-module-parameters/103994\n","------------------------------------------------------------------------------------------------------------------------\n","\"\"\"\n","\n","func = ODEFunc()\n","param_tensors = nn.ParameterList(func.parameters())\n","\n","params_vector = torch.tensor([])\n","for param in param_tensors:\n","  vec = torch.reshape(param, (-1,))\n","  params_vector = torch.cat((params_vector, vec))\n","\n","def loss(params_vector):\n","  parameters_list = nn.ParameterList([])\n","  for param in func.parameters():\n","    nels = torch.numel(param)\n","    elements = params_vector[:nels]\n","    elements = torch.reshape(elements, param.shape)\n","    elements = torch.nn.parameter.Parameter(elements)\n","\n","    parameters_list.append(elements)\n","    params_vector = params_vector[nels:]\n","\n","  net = ODEFunc(parameters_list) \n","  input = torch.rand(2,2)\n","  output = net(input)\n","  target = torch.rand(2,2)\n","  return torch.linalg.norm(target - output)\n","\n","hessian = torch.autograd.functional.hessian(loss, params_vector)\n","print(hessian)\n","\n","#parameters = torch.tensor([])\n","#for param in func.parameters():\n","  #params = torch.reshape(param.data, (-1,))  #Reshapes the gradient to a 1D vector.\n","  #parameters = torch.cat((params, parameters), 0) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y8a1TN_q2hPy"},"source":["\"\"\"\n","------------------------------------------------------------------------------------------------------------------------\n","This has the same problem as above.\n","------------------------------------------------------------------------------------------------------------------------\n","\"\"\"\n","\n","func = ODEFunc()\n","param_tensors = func.parameters()\n","\n","params_vector = torch.tensor([])\n","for param in param_tensors:\n","  vec = torch.reshape(param, (-1,))\n","  params_vector = torch.cat((params_vector, vec))\n","\n","parameters_list = []\n","func = ODEFunc()\n","\n","def get_loss(params_vector):\n","\n","  for param in func.parameters():\n","    nels = torch.numel(param)\n","    elements = params_vector[:nels]\n","    elements = torch.reshape(elements, param.shape)\n","    elements = torch.nn.parameter.Parameter(elements)\n","\n","    parameters_list.append(elements)\n","    params_vector = params_vector[nels:]\n","\n","  a = parameters_list[0]\n","  b = parameters_list[1]\n","  c = parameters_list[2]\n","  d = parameters_list[3]\n","  \n","  input = torch.rand(2)\n","  y = torch.tensor([2,0])\n","\n","  x = F.linear(input, a, b)\n","  m = nn.Tanh()\n","  x = m(x)\n","  x = F.linear(x, c, d)\n","\n","  loss = torch.linalg.norm(y-x)\n","  return loss\n","\n","loss = get_loss(params_vector)\n","hessian = torch.autograd.functional.hessian(get_loss_1, params_vector)\n","print(hessian)\n","\n"],"execution_count":null,"outputs":[]}]}