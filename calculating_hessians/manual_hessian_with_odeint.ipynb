{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"manual_hessian_with_odeint.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPbeJ0gqKMObhWjnzxQFVpG"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"_4kI51iy1KbT"},"source":["%%bash \n","pip install torchdiffeq"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CUNwqAQY1PnF"},"source":["import torch\n","from torch import nn\n","import numpy as np\n","from torch.nn import Module\n","import torch.nn.functional as F\n","import time\n","import matplotlib.pyplot as plt\n","import torch.optim as optim\n","\n","adjoint = True\n","\n","if adjoint == True:\n","  from torchdiffeq import odeint_adjoint as odeint\n","if adjoint == False:\n","  from torchdiffeq import odeint"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EaGXwM8d1UTJ"},"source":["device = torch.device('cuda:' + str(0) if torch.cuda.is_available() else 'cpu')\n","\n","true_y0 = torch.tensor([[2., 0.]]).to(device)\n","t = torch.linspace(0., 25., 1000).to(device)\n","true_A = torch.tensor([[-0.1, 2.0], [-2.0, -0.1]]).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"50kmjSteRjC2"},"source":["class ODEFunc(nn.Module):\n","\n","    def __init__(self):\n","        super(ODEFunc, self).__init__()\n","\n","        #Define a very simple neural network architecture with 1 hidden layer.\n","        self.net = nn.Sequential(\n","            nn.Linear(2, 50),\n","            nn.Tanh(),\n","            nn.Linear(50, 2),\n","        )\n","\n","        #Initialise the weights and biases for the linear layers.\n","        #The isinstance functions checks that the first input is an instance or subclass of the second argument.\n","        \n","        for m in self.net.modules():\n","          if isinstance(m, nn.Linear):\n","            nn.init.normal_(m.weight, mean=0, std=0.1)\n","            nn.init.constant_(m.bias, val=0)\n","        \n","    #The forward function defines how the data is passed through the neural net. \n","    #In particular, it is called when you apply the neural net to an input variable.\n","    #We act the net on y**3 such that it is only learning to represent the matrix (see class Lambda)\n","\n","    def forward(self, y):\n","        return self.net(y**3)\n","\n","func = ODEFunc()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4fAaEsdXtu7O"},"source":["\"\"\"\n","------------------------------------------------------------------------------------------------------------------------\n","This is a \"manual\" approach to calculating the Hessian, which uses nested for loops. It produces a symmetric square \n","matrix that is the same as that obtained with the approach given below (that uses library functions).\n","------------------------------------------------------------------------------------------------------------------------\n","\"\"\"\n","\n","def get_manual_hessian(grads, parameters):\n","  \"\"\"\n","  Calculation of the Hessian using nested for loops.\n","  \"\"\"\n","  start = time.time()                       #Begin timer.\n","  grads2 = torch.zeros(252,252)             #Create an matrix of zeros thas has the same shape as the Hessian.\n","\n","  y_counter = 0                             #y_direction refers to row number in the Hessian.\n","\n","  for grad in grads:\n","      grad = torch.reshape(grad, [-1])                                  #Rearrange the gradient information into a vector.        \n","\n","      for j, g in enumerate(grad):\n","        x_counter = 0                                                   #x_direction refers to column number in the Hessian.\n","\n","        for l, param in enumerate(parameters):\n","          g2 = torch.autograd.grad(g, param, retain_graph=True)[0]      #Calculate the gradient of an element of the gradient wrt one layer's parameters.\n","          g2 = torch.reshape(g2, [-1])                                  #Reshape this into a vector.\n","          len = g2.shape[0]                       \n","          grads2[j+y_counter, x_counter:x_counter+len] = g2             #Indexing ensures that the second order derivatives are placed in the correct positions.\n","          x_counter += len\n","\n","      grads2 = grads2.to(device)\n","      y_counter += grad.shape[0]\n","  \n","  print('Time used is ', time.time() - start)\n","\n","  return grads2\n","\n","input = torch.tensor([1.0,1.0])\n","target = torch.tensor([2.0,0.0])\n","optimizer = optim.RMSprop(func.parameters(), lr=1e-3) #func.parameters are the parameters to optimise.\n","optimizer.zero_grad()\n","parameters = optimizer.param_groups[0]['params']\n","loss = torch.linalg.norm(func(input) - target)\n","\n","grads = torch.autograd.grad(loss, parameters, create_graph=True)  # first order gradients\n","\n","hessian = get_manual_hessian(grads, parameters)  # calculate hessian\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZXaXEutJr3A0"},"source":["\"\"\"\n","------------------------------------------------------------------------------------------------------------------------\n","This approach works, and produces a 252 x 252 matrix. It is built using library functions and, whilst\n","there is no guarantee that it gives the correct answer, can be used to check the more \"manual\" approach used above.\n","------------------------------------------------------------------------------------------------------------------------\n","\"\"\"\n","\n","def get_loss_square(params_vector):\n","\n","  a = params_vector[:100].reshape([50, 2])\n","  b = params_vector[100:150].reshape([50])\n","  c = params_vector[150:250].reshape([2, 50])\n","  d = params_vector[250:252].reshape([2])\n","\n","  input = torch.tensor([1.0,1.0])\n","  y = torch.tensor([2.0,0.0])\n","\n","  x = F.linear(input**3, a, b)\n","  m = nn.Tanh()\n","  x = m(x)\n","  x = F.linear(x, c, d)\n","\n","  loss = torch.linalg.norm(y-x)\n","  return loss\n","\n","def get_hessian(net):\n","  \n","  start = time.time()\n","  param_tensors = net.parameters()\n","  params_vector = torch.tensor([])\n","  for param in param_tensors:\n","    vec = torch.reshape(param, (-1,))\n","    params_vector = torch.cat((params_vector, vec))\n","\n","  hessian = torch.autograd.functional.hessian(get_loss_square, params_vector)\n","  print('Time used is ', time.time() - start)\n","  return hessian\n","\n","hessian = get_hessian(func)\n","\n","\n","#print(hessian[:5,:5])\n","\n","eigenvalues, v = torch.symeig(hessian)\n","\n","n, bins, patches = plt.hist(eigenvalues, bins=150)"],"execution_count":null,"outputs":[]}]}