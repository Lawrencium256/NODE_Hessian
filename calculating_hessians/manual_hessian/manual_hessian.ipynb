{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"manual_hessian.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP4eHf4L2DY8HUHoPjqcO7B"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"07ZI3Z4C1Yas"},"source":["\"\"\"\n","---------------------------------------------------------------------------------------------------------------------------------------------------------\n","This document contains a \"manual\" method for calculating the Hessian, which works using nested for loops.\n","---------------------------------------------------------------------------------------------------------------------------------------------------------\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CUNwqAQY1PnF"},"source":["import torch\n","from torch import nn\n","import numpy as np\n","from torch.nn import Module\n","import torch.nn.functional as F\n","import time\n","import matplotlib.pyplot as plt\n","import torch.optim as optim\n","\n","device = torch.device('cuda:' + str(0) if torch.cuda.is_available() else 'cpu')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"50kmjSteRjC2"},"source":["class ODEFunc(nn.Module):\n","\n","    def __init__(self):\n","        super(ODEFunc, self).__init__()\n","\n","        #Define a very simple neural network architecture with 1 hidden layer.\n","        self.net = nn.Sequential(\n","            nn.Linear(2, 50),\n","            nn.Tanh(),\n","            nn.Linear(50, 2),\n","        )\n","\n","        #Initialise the weights and biases for the linear layers.\n","        #The isinstance functions checks that the first input is an instance or subclass of the second argument.\n","        \n","        for m in self.net.modules():\n","          if isinstance(m, nn.Linear):\n","            nn.init.normal_(m.weight, mean=0, std=0.1)\n","            nn.init.constant_(m.bias, val=0)\n","        \n","    #The forward function defines how the data is passed through the neural net. \n","    #In particular, it is called when you apply the neural net to an input variable.\n","    #We act the net on y**3 such that it is only learning to represent the matrix (see class Lambda)\n","\n","    def forward(self, y):\n","        return self.net(y**3)\n","\n","func = ODEFunc()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4fAaEsdXtu7O"},"source":["\"\"\"\n","-----------------------------------------------------------------------------------------------------------------------------------------\n","This produces a symmetric square matrix that is the same as that obtained with the approach given below (that uses library functions).\n","-----------------------------------------------------------------------------------------------------------------------------------------\n","\"\"\"\n","\n","def get_manual_hessian(grads, parameters):\n","  \"\"\"\n","  Calculation of the Hessian using nested for loops.\n","  Inputs: \n","    grads:      tuple of gradient tensors. Created using something like grads = torch.autograd.grad(loss, parameters, create_graph=True).\n","    parameters: list of parameter objects. Created using something like parameters = optimizer.param_groups[0]['params'].\n","  \"\"\"\n","  start = time.time()                       #Begin timer.\n","\n","  n_params = 0\n","  for param in parameters:\n","    n_params += torch.numel(param)\n","  grads2 = torch.zeros(n_params,n_params)             #Create an matrix of zeros thas has the same shape as the Hessian.\n","\n","  y_counter = 0                             #y_direction refers to row number in the Hessian.\n","\n","  for grad in grads:\n","      grad = torch.reshape(grad, [-1])                                  #Rearrange the gradient information into a vector.        \n","\n","      for j, g in enumerate(grad):\n","        x_counter = 0                                                   #x_direction refers to column number in the Hessian.\n","\n","        for l, param in enumerate(parameters):\n","          g2 = torch.autograd.grad(g, param, retain_graph=True)[0]      #Calculate the gradient of an element of the gradient wrt one layer's parameters.\n","          g2 = torch.reshape(g2, [-1])                                  #Reshape this into a vector.\n","          len = g2.shape[0]                       \n","          grads2[j+y_counter, x_counter:x_counter+len] = g2             #Indexing ensures that the second order derivatives are placed in the correct positions.\n","          x_counter += len\n","\n","      grads2 = grads2.to(device)\n","      y_counter += grad.shape[0]\n","  \n","  print('Time used is ', time.time() - start)\n","\n","  return grads2\n","\n","#Prepare optimizer and network.\n","input = torch.tensor([1.0,1.0])\n","target = torch.tensor([2.0,0.0])\n","optimizer = optim.RMSprop(func.parameters(), lr=1e-3) #func.parameters are the parameters to optimise.\n","optimizer.zero_grad()\n","parameters = optimizer.param_groups[0]['params']\n","loss = torch.linalg.norm(func(input) - target)\n","loss.backward(create_graph=True)\n","\n","#Obtaining the gradients in this way is preferable (over torch.autograd.grad()) since it can be used after loss.backward().\n","grads = [0,0,0,0] \n","for counter, param in enumerate(func.parameters()):\n","  grads[counter] = param.grad\n","grads = tuple(grads)\n","\n","#grads = torch.autograd.grad(loss, parameters, create_graph=True)  # first order gradients\n","\n","hessian = get_manual_hessian(grads, parameters)"],"execution_count":null,"outputs":[]}]}