{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"manual_hessian_with_odeint.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM/G1GJtJRIL4pu2p3X+3Et"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"hy2OTutQ2qao"},"source":["\"\"\"\n","---------------------------------------------------------------------------------------------------------------------------------------------------------\n","This file is used to assess performance of \"manual\" hessian calculations with a loss containing the odeint() function from torchdiffeq.\n","---------------------------------------------------------------------------------------------------------------------------------------------------------\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_4kI51iy1KbT"},"source":["%%bash \n","pip install torchdiffeq"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CUNwqAQY1PnF"},"source":["#Libraries\n","import torch\n","from torch import nn\n","import numpy as np\n","from torch.nn import Module\n","import torch.nn.functional as F\n","import time\n","import matplotlib.pyplot as plt\n","import torch.optim as optim\n","\n","device = torch.device('cuda:' + str(0) if torch.cuda.is_available() else 'cpu')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iO8SVwTEChjF"},"source":["#Problem set up\n","\n","true_y0 = torch.tensor([[2., 0.]]).to(device)\n","t = torch.linspace(0., 25., 1000).to(device)\n","true_A = torch.tensor([[-0.1, 2.0], [-2.0, -0.1]]).to(device)\n","\n","adjoint = False\n","\n","if adjoint == True:\n","    from torchdiffeq import odeint_adjoint as odeint\n","else:\n","    from torchdiffeq import odeint\n","\n","class Lambda(nn.Module):\n","\n","    def forward(self, t, y):\n","        return torch.mm(y**3, true_A)\n","\n","#It's not obvious, but this true_y solution defines a spiral in the x-y plane (I verified this computationally).\n","with torch.no_grad():\n","    true_y = odeint(Lambda(), true_y0, t, method='dopri5')    #Produces 1000 2D vectors, i.e. the evolution of y at 1000 points through time.\n","                                                              #This comes in the form of an array with shape [1000,1,2]            "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_icTj8PcDdHl"},"source":["batch_time = 10\n","batch_size = 20\n","data_size = 1000\n","\n","def get_batch():\n","\n","    #Generates a random list of integers in the range (data_size - batch_time), of length (batch_size).\n","    s = torch.from_numpy(np.random.choice(np.arange(data_size - batch_time, dtype=np.int64), batch_size, replace=False)) \n","\n","    #Creates the random batch. batch_y will be our ground truth when optimising the neural net.\n","    batch_y0 = true_y[s]  # (M, D)\n","    batch_t = t[:batch_time]  # (T)\n","    batch_y = torch.stack([true_y[s + i] for i in range(batch_time)], dim=0)  # (T, M, D)\n","    return batch_y0.to(device), batch_t.to(device), batch_y.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"50kmjSteRjC2"},"source":["class ODEFunc(nn.Module):\n","\n","    def __init__(self):\n","        super(ODEFunc, self).__init__()\n","\n","        #Define a very simple neural network architecture with 1 hidden layer.\n","        self.net = nn.Sequential(\n","            nn.Linear(2, 50),\n","            nn.Tanh(),\n","            nn.Linear(50, 2),\n","        )\n","\n","        #Initialise the weights and biases for the linear layers.\n","        #The isinstance functions checks that the first input is an instance or subclass of the second argument.\n","        \n","        for m in self.net.modules():\n","          if isinstance(m, nn.Linear):\n","            nn.init.normal_(m.weight, mean=0, std=0.1)\n","            nn.init.constant_(m.bias, val=0)\n","        \n","    #The forward function defines how the data is passed through the neural net. \n","    #In particular, it is called when you apply the neural net to an input variable.\n","    #We act the net on y**3 such that it is only learning to represent the matrix (see class Lambda)\n","\n","    def forward(self, t, y):\n","        return self.net(y**3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4fAaEsdXtu7O"},"source":["def get_manual_hessian(grads, parameters):\n","  \"\"\"\n","  Calculation of the Hessian using nested for loops.\n","  Inputs: \n","    grads:      tuple of gradient tensors. Created using something like grads = torch.autograd.grad(loss, parameters, create_graph=True).\n","    parameters: list of parameter objects. Created using something like parameters = optimizer.param_groups[0]['params'].\n","  \"\"\"\n","  start = time.time()                       #Begin timer.\n","\n","  n_params = 0\n","  for param in parameters:\n","    n_params += torch.numel(param)\n","  grads2 = torch.zeros(n_params,n_params)             #Create an matrix of zeros thas has the same shape as the Hessian.\n","\n","  y_counter = 0                                       #y_direction refers to row number in the Hessian.\n","\n","  for grad in grads:\n","      grad = torch.reshape(grad, [-1])                                  #Rearrange the gradient information into a vector.        \n","\n","      for j, g in enumerate(grad):\n","        x_counter = 0                                                   #x_direction refers to column number in the Hessian.\n","\n","        for l, param in enumerate(parameters):\n","          g2 = torch.autograd.grad(g, param, retain_graph=True)[0]      #Calculate the gradient of an element of the gradient wrt one layer's parameters.\n","          g2 = torch.reshape(g2, [-1])                                  #Reshape this into a vector.\n","          len = g2.shape[0]                       \n","          grads2[j+y_counter, x_counter:x_counter+len] = g2             #Indexing ensures that the second order derivatives are placed in the correct positions.\n","          x_counter += len\n","\n","      grads2 = grads2.to(device)\n","      y_counter += grad.shape[0]\n","      print(\"Gradients calculated for row number \" + str(y_counter) + \".\")\n","  \n","  print('Time used is ', time.time() - start)\n","\n","  return grads2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uq7SHfOlIR-n"},"source":["func = ODEFunc().to(device)\n","batch_y0, batch_t, batch_y = get_batch()\n","\n","optimizer = optim.RMSprop(func.parameters(), lr=1e-3) #func.parameters are the parameters to optimise.\n","optimizer.zero_grad()\n","pred_y = odeint(func, batch_y0, batch_t).to(device)\n","\n","parameters = optimizer.param_groups[0]['params']\n","loss = torch.mean(torch.abs(pred_y-batch_y))\n","loss.backward(create_graph=True)\n","\n","#Calculating the gradients like this allows you to do so after calling loss.backward(). This is not possible if they were calculated with autograd.grad().\n","grads = [0,0,0,0] \n","for counter, param in enumerate(func.parameters()):\n","  grads[counter] = param.grad\n","grads = tuple(grads)\n","\n","hessian = get_manual_hessian(grads, parameters)\n","optimizer.step()"],"execution_count":null,"outputs":[]}]}