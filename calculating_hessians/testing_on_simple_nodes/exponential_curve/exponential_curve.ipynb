{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"exponential_curve.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1BuibUxUNo1GNSb66oNUGyX087MtOq5fZ","authorship_tag":"ABX9TyNBMSITLIbZaXvQQg0dPgXr"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"SQBS7KTR83Bw"},"source":["%%capture\n","\"\"\"\n","-----------------------------------------------------------------------------------------------------------------------------------------------------------\n","This file is used to compute the hessian during training for fitting simple 1D exponential ODEs.\n","It contains functionality to do so by using either \"manual\" or library-based approaches.\n","-----------------------------------------------------------------------------------------------------------------------------------------------------------\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uqZsIwjwmG2p"},"source":["%%capture\n","%%bash \n","pip install torchdiffeq"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_iqOcAXt2wMy"},"source":["import os\n","import argparse\n","import time\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N4O95yfu3BLY"},"source":["parser = argparse.ArgumentParser()\n","parser.add_argument('--method', type=str, choices=['dopri5', 'adams'], default='dopri5')\n","parser.add_argument('--data_size', type=int, default=1000)\n","parser.add_argument('--batch_time', type=int, default=10)\n","parser.add_argument('--batch_size', type=int, default=20)\n","parser.add_argument('--niters', type=int, default=100)\n","parser.add_argument('--test_freq', type=int, default=20)\n","parser.add_argument('--viz', action='store_true')\n","parser.add_argument('--gpu', type=int, default=0)\n","parser.add_argument('--adjoint', action='store_true')\n","parser.add_argument('--manual_hessian', action='store_true')\n","parser.add_argument('--library_hessian', action='store_true')\n","parser.add_argument('--hessian_freq', type=int, default=20)\n","args = parser.parse_args(args=[])\n","\n","args.batch_size = 750\n","args.batch_time = 50\n","args.niters=6000\n","args.test_freq=100\n","args.library_hessian = True\n","args.manual_hessian = False\n","args.viz = True\n","args.hessian_freq = 100\n","args.method = 'dopri5'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P2TQOgZL-ncp"},"source":["#The technique only works when the adjoint method is not used. If it is used, the Hessian returned is a matrix of zeros.\n","adjoint = False\n","\n","if adjoint == True:\n","    from torchdiffeq import odeint_adjoint as odeint\n","if adjoint == False:\n","    from torchdiffeq import odeint\n","\n","device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')\n","\n","true_y0 = torch.tensor([2.]).to(device)\n","t_0, t_1 = 0., 2.\n","t = torch.linspace(t_0, t_1, args.data_size).to(device)\n","\n","class Lambda(nn.Module):\n","\n","    def forward(self, t, y):\n","        return torch.exp(t)\n","\n","#The true solution defines an exponential.\n","with torch.no_grad():\n","    true_y = odeint(Lambda(), true_y0, t, method = args.method)\n","\n","def get_batch():\n","\n","    s = torch.from_numpy(np.random.choice(np.arange(args.data_size - args.batch_time, dtype=np.int64), args.batch_size, replace=False)) \n","    batch_y0 = true_y[s]  # (M, D)    Random set of (100) values from true_y\n","    batch_t = t[:args.batch_time]  # (T)      #The first (10) values from t.\n","    batch_y = torch.stack([true_y[s + i] for i in range(args.batch_time)], dim=0)  # (T, M, D)    Set of 20 lots of 10 sequential points in true_y.\n","    return batch_y0.to(device), batch_t.to(device), batch_y.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b-3krz_a_bgU"},"source":["%%capture\n","def makedirs(dirname):\n","    if not os.path.exists(dirname):\n","        os.makedirs(dirname)\n","        \n","if args.viz:\n","    makedirs('png')\n","    import matplotlib.pyplot as plt\n","    fig = plt.figure(figsize=(12, 4), facecolor='white')    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2ImySTI6ghqO"},"source":["def visualize(true_y, pred_y, odefunc, itr):\n","\n","  \"\"\"\n","  This slightly altered version of the function visualize() seems to work fine. The only change is that I have moved the plt.figure() part of the code\n","  inside the function itself, i.e. I am creating a new figure environment for every figure, instead of editing the same environment multiple times.\n","  \"\"\"\n","\n","  if args.viz:\n","\n","    fig = plt.figure(figsize=(12, 4), facecolor='white')  #facecolor is the background colour.\n","    plt.plot(t.cpu().numpy(), true_y.cpu().numpy(), 'g-', label='True_y')\n","    plt.plot(t.cpu().numpy(), pred_y.cpu().detach().numpy(), 'b--', label='Predicted y')\n","    plt.xlabel('t')\n","    plt.ylabel('y')\n","    plt.legend()\n","    \n","    #plt.savefig('png/{:03d}'.format(itr))\n","    plt.draw()\n","    plt.pause(0.001)\n","    plt.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Uo8kpZybTYMh"},"source":["class ODEFunc(nn.Module):\n","    \"\"\"\n","    Defines a very simple neural net with a 1D latent space. It has 4 parameters (2 weights and 2 biases).\n","    There is a Tanh() activation function used on the hidden layer.\n","    \"\"\"\n","\n","    def __init__(self):\n","        super(ODEFunc, self).__init__()\n","\n","        self.net = nn.Sequential(\n","            nn.Linear(1, 1),\n","            nn.Tanh(),\n","            nn.Linear(1, 1),\n","        )\n","\n","        for m in self.net.modules():\n","            if isinstance(m, nn.Linear):\n","                nn.init.normal_(m.weight, mean=0, std=0.1)\n","                nn.init.constant_(m.bias, val=0)\n","\n","    def forward(self, t, y):\n","        return self.net(y)\n","\n","func = ODEFunc().to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-6AVeG8_klef"},"source":["\"\"\"\n","------------------------------------------------------------------------------------------------------------------------\n","This code can be used to compare the computationally-obtained gradient values to those obtained analytically.\n","See journal entry from 26/02 for more details.\n","------------------------------------------------------------------------------------------------------------------------\n","\"\"\"\n","\n","output = odeint(func, true_y0, t)[-1]  \n","print(output.item())\n","target = true_y[-1] \n","print(target.item())                             \n","\n","optimizer = optim.RMSprop(func.parameters(), lr=1e-3) #func.parameters are the parameters to optimise.\n","optimizer.zero_grad()\n","\n","loss = torch.linalg.norm(output-target)**2\n","print('Loss is: ' + str(loss.item()))\n","\n","loss.backward()\n","\n","print('-------------------\\n' + 'Gradients are:')\n","for param in func.parameters():\n","  print(param.grad.data.item())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fn4lPrpagmOx"},"source":["class Network(nn.Module):\n","\n","  def __init__(self, a, b, c, d):\n","    super(Network, self).__init__()\n","    self.a = a\n","    self.b = b\n","    self.c = c\n","    self.d = d\n","\n","  def forward(self, t, y):\n","    x = F.linear(y, self.a, self.b)\n","    m = nn.Tanh()\n","    x = m(x)\n","    x = F.linear(x, self.c, self.d)\n","    return x\n","\n","\n","def get_loss_square(params_vector):\n","\n","  a = params_vector[:1].reshape([1, 1])\n","  b = params_vector[1:2].reshape([1])\n","  c = params_vector[2:3].reshape([1, 1])\n","  d = params_vector[3:4].reshape([1])\n","  \n","  neural_net = Network(a, b, c, d).to(device)\n","  pred_y = odeint(neural_net, true_y0, t, method= args.method)\n","  loss = torch.mean(torch.abs(pred_y - true_y))\n","  return loss\n","\n","def get_library_hessian(net):\n","\n","  param_tensors = net.parameters()\n","  params_vector = torch.tensor([]).to(device)\n","  for param in param_tensors:\n","    vec = torch.reshape(param, (-1,)).to(device)\n","    params_vector = torch.cat((params_vector, vec))\n","\n","  hessian = torch.autograd.functional.hessian(get_loss_square, params_vector)\n","  return hessian"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y77p15C1G7Sa"},"source":["def get_manual_hessian(grads, parameters):\n","  \"\"\"\n","  Calculation of the Hessian using nested for loops.\n","  Inputs: \n","    grads:      tuple of gradient tensors. Created using something like grads = torch.autograd.grad(loss, parameters, create_graph=True).\n","    parameters: list of parameter objects. Created using something like parameters = optimizer.param_groups[0]['params'].\n","  \"\"\"\n","  start = time.time()                       #Begin timer.\n","\n","  n_params = 0\n","  for param in parameters:\n","    n_params += torch.numel(param)\n","  grads2 = torch.zeros(n_params,n_params)             #Create an matrix of zeros thas has the same shape as the Hessian.\n","\n","  y_counter = 0                             #y_direction refers to row number in the Hessian.\n","\n","  for grad in grads:\n","      grad = torch.reshape(grad, [-1])                                  #Rearrange the gradient information into a vector.        \n","\n","      for j, g in enumerate(grad):\n","        x_counter = 0                                                   #x_direction refers to column number in the Hessian.\n","\n","        for l, param in enumerate(parameters):\n","          g2 = torch.autograd.grad(g, param, retain_graph=True)[0]      #Calculate the gradient of an element of the gradient wrt one layer's parameters.\n","          g2 = torch.reshape(g2, [-1])                                  #Reshape this into a vector.\n","          len = g2.shape[0]                       \n","          grads2[j+y_counter, x_counter:x_counter+len] = g2             #Indexing ensures that the second order derivatives are placed in the correct positions.\n","          x_counter += len\n","\n","      grads2 = grads2.to(device)\n","      y_counter += grad.shape[0]\n","      print(\"Gradients calculated for row number \" + str(y_counter) + \".\")\n","  \n","  print('Time used was ', time.time() - start)\n","\n","  return grads2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GP337SbeTYl6"},"source":["if __name__ == '__main__':\n","    \"\"\"\n","    Executes the programme. This includes doing the following:\n","\n","      - Trains the network;\n","      - Outputs the results in a series of png files (if desired);\n","      - Outputs hessian matrix information in list form.\n","    \"\"\"\n","\n","    ii = 0\n","\n","    func = ODEFunc().to(device)\n","    \n","    optimizer = optim.RMSprop(func.parameters(), lr=1e-3) #func.parameters are the parameters to optimise.\n","\n","    #Lists in which to store hessian data.\n","    #These will be lists of tuples like (iteration number, time, loss, hessian data).\n","    manual_hessian_data = []\n","    library_hessian_data = []\n","    loss_data = []\n","\n","    for itr in range(1, args.niters + 1):\n","        optimizer.zero_grad()                                 \n","        batch_y0, batch_t, batch_y = get_batch()             \n","        pred_y = odeint(func, batch_y0, batch_t, method = args.method)\n","        loss = torch.mean(torch.abs(pred_y - batch_y))        \n","        loss.backward(create_graph=True)                                                                     \n","        \n","        if itr % args.hessian_freq == 0 or itr==1:\n","          if args.library_hessian:\n","            print('Obtaining library hessian...')\n","            library_start = time.time()\n","            library_hessian = get_library_hessian(func)                       #get hessian with library functions   \n","            library_end = time.time()\n","            print(\"Time taken for library-based approach was \" + str(round(library_end-library_start,2)) + \"s.\")\n","            library_hessian_data.append((itr, library_end-library_start, loss.item(), library_hessian))\n","\n","          if args.manual_hessian:\n","            \n","            print('Obtaining manual hessian...')\n","            manual_start = time.time()\n","\n","            pred_y = odeint(func, true_y0, t, method = args.method)\n","            loss = torch.mean(torch.abs(pred_y - true_y))\n","            grads = torch.autograd.grad(loss, func.parameters(), create_graph=True)\n","            parameters = optimizer.param_groups[0]['params']\n","          \n","            manual_hessian = get_manual_hessian(grads, parameters)           #get hessian with manual approach.\n","            manual_end = time.time()\n","            print(\"Time taken for manual approach was \" + str(round(manual_end-manual_start,2)) + \"s.\")\n","            manual_hessian_data.append((itr, manual_end-manual_start, loss.item(), manual_hessian))\n","      \n","        if itr % args.test_freq == 0:\n","          ii += 1       \n","          with torch.no_grad():\n","              pred_y = odeint(func, true_y0, t, method= args.method)\n","              loss = torch.mean(torch.abs(pred_y - true_y))\n","              loss_data.append((itr, loss.item()))\n","              print('Iter {:04d} | Total Loss {:.6f}'.format(itr, loss.item()))\n","              visualize(true_y, pred_y, func, ii)\n","        \"\"\"\n","        else:\n","          if itr % 50 == 0 or itr==1:\n","            if args.library_hessian:\n","              print('Obtaining library hessian...')\n","              library_start = time.time()\n","              library_hessian = get_library_hessian(func)                       #get hessian with library functions   \n","              library_end = time.time()\n","              print(\"Time taken for library-based approach was \" + str(round(library_end-library_start,2)) + \"s.\")\n","              library_hessian_data.append((itr, library_end-library_start, loss.item(), library_hessian))\n","        \n","          if itr % 50 == 0:\n","            ii += 1       \n","            with torch.no_grad():\n","                pred_y = odeint(func, true_y0, t)\n","                loss = torch.mean(torch.abs(pred_y - true_y))\n","                loss_data.append((itr, loss.item()))\n","                print('Iter {:04d} | Total Loss {:.6f}'.format(itr, loss.item()))\n","                visualize(true_y, pred_y, func, ii)\n","        \"\"\"  \n","              \n","\n","        optimizer.step()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QVFB8t6HKxl_"},"source":["#Create a plot of the loss curve.\n","\n","itrs = []\n","data = []\n","\n","for item in loss_data:\n","  itrs.append(item[0])\n","  data.append(item[1])\n","\n","plt.figure(figsize=(10,8))\n","plt.rcParams.update({'font.size': 14})\n","plt.plot(itrs, data)\n","plt.title('Loss function for ' + str(args.method) + ' solver\\nBatch Size, Time = ' + str(args.batch_size) + ', ' + str(args.batch_t) )\n","plt.xlabel('Iterations')\n","plt.ylabel('Loss')\n","#plt.savefig('/content/drive/MyDrive/colab_notebooks/calculating_hessians/testing_on_simple_nodes/exponential_curve/batch_size_investigation/batch_size_' \n"," #           + str(args.batch_size) + '/loss_curve_' + str(args.batch_size) + '.png')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LaFs4psFHd_I"},"source":["#Create histogram plots of the eigenvalue density.\n","\n","for item in library_hessian_data:\n","  e, v = torch.symeig(item[3])\n","  plt.hist(e.cpu().numpy(), bins=150)\n","  plt.title(\"Iteration: \" + str(item[0]))\n","  plt.xlabel('Eigenvalue')\n","  plt.ylabel('Density')\n","  #plt.savefig('/content/drive/MyDrive/colab_notebooks/calculating_hessians/testing_on_simple_nodes/exponential_curve/batch_size_investigation/batch_size_' \n","              #+ str(args.batch_size) + '/eigenvalue_density_plots_' + str(args.batch_size) + '/eigenvalue_density_' + str(args.batch_size) + '_'\n","              #+ str(item[0]) + '.png')\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9xrdLBOmIYAZ"},"source":["#Create a plot of the extremal eigenvalue through training.\n","\n","itrs = []\n","values = []\n","for item in library_hessian_data:\n","  itrs.append(item[0])\n","  e, v = torch.symeig(item[3])\n","  value = max(e, key=abs)\n","  values.append(value)\n","\n","plt.figure(figsize=(10,8))\n","plt.rcParams.update({'font.size': 14})\n","plt.plot(itrs, values)\n","plt.title('Extremal Eigenvalues for ' + str(args.method).title() + ' Solver\\nBatch Size = ' + str(args.batch_size))\n","plt.xlabel('Iterations')\n","plt.ylabel('Extremal Eigenvalue')\n","#plt.savefig('/content/drive/MyDrive/colab_notebooks/calculating_hessians/testing_on_simple_nodes/exponential_curve/batch_size_investigation/batch_size_' \n","           # + str(args.batch_size) + '/extremal_eigenvalues_' + str(args.batch_size) + '.png')\n","plt.show()"],"execution_count":null,"outputs":[]}]}