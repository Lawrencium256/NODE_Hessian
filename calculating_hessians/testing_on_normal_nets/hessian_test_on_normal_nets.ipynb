{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"hessian_test_on_normal_nets.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1LFBpXpR_eFfNF6tvzp1RkxkGpJ_FAJTX","authorship_tag":"ABX9TyODfo4LAMRMyJlXZ7Kby6wg"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"07ZI3Z4C1Yas"},"source":["\"\"\"\n","---------------------------------------------------------------------------------------------------------------------------------------------------------\n","This document contains code for calculating the Hessian using both a \"manual\" library-based approach. These were compared using the network ODEFunc, and \n","appear to give identical results. The class Simple_Net is designed to allow comparison of these methods to analytical approaches.\n","---------------------------------------------------------------------------------------------------------------------------------------------------------\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JUCvyMJp_L6o"},"source":["%%bash\n","cd drive/MyDrive/colab_notebooks/calculating_hessians/testing_on_normal_nets/network_1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CUNwqAQY1PnF"},"source":["import torch\n","from torch import nn\n","import numpy as np\n","from torch.nn import Module\n","import torch.nn.functional as F\n","import time\n","import matplotlib.pyplot as plt\n","import torch.optim as optim\n","\n","device = torch.device('cuda:' + str(0) if torch.cuda.is_available() else 'cpu')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"50kmjSteRjC2"},"source":["class ODEFunc(nn.Module):\n","\n","    def __init__(self):\n","        super(ODEFunc, self).__init__()\n","\n","        #Define a very simple neural network architecture with 1 hidden layer.\n","        self.net = nn.Sequential(\n","            nn.Linear(2, 50),\n","            nn.Tanh(),\n","            nn.Linear(50, 2),\n","        )\n","\n","        #Initialise the weights and biases for the linear layers.\n","        #The isinstance functions checks that the first input is an instance or subclass of the second argument.\n","        \n","        for m in self.net.modules():\n","          if isinstance(m, nn.Linear):\n","            nn.init.normal_(m.weight, mean=0, std=0.1)\n","            nn.init.constant_(m.bias, val=0)\n","        \n","    #The forward function defines how the data is passed through the neural net. \n","    #In particular, it is called when you apply the neural net to an input variable.\n","    #We act the net on y**3 such that it is only learning to represent the matrix (see class Lambda)\n","\n","    def forward(self, y):\n","        return self.net(y**3)\n","\n","func = ODEFunc()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4fAaEsdXtu7O"},"source":["\"\"\"\n","-----------------------------------------------------------------------------------------------------------------------------------------\n","This produces a symmetric square matrix that is the same as that obtained with the approach given below (that uses library functions).\n","-----------------------------------------------------------------------------------------------------------------------------------------\n","\"\"\"\n","\n","def get_manual_hessian(grads, parameters):\n","  \"\"\"\n","  Calculation of the Hessian using nested for loops.\n","  Inputs: \n","    grads:      tuple of gradient tensors. Created using something like grads = torch.autograd.grad(loss, parameters, create_graph=True).\n","    parameters: list of parameter objects. Created using something like parameters = optimizer.param_groups[0]['params'].\n","  \"\"\"\n","  start = time.time()                       #Begin timer.\n","\n","  n_params = 0\n","  for param in parameters:\n","    n_params += torch.numel(param)\n","  grads2 = torch.zeros(n_params,n_params)             #Create an matrix of zeros thas has the same shape as the Hessian.\n","\n","  y_counter = 0                                       #y_direction refers to row number in the Hessian.\n","\n","  for grad in grads:\n","      grad = torch.reshape(grad, [-1])                                  #Rearrange the gradient information into a vector.        \n","\n","      for j, g in enumerate(grad):\n","        x_counter = 0                                                   #x_direction refers to column number in the Hessian.\n","\n","        for l, param in enumerate(parameters):\n","          g2 = torch.autograd.grad(g, param, retain_graph=True)[0]      #Calculate the gradient of an element of the gradient wrt one layer's parameters.\n","          g2 = torch.reshape(g2, [-1])                                  #Reshape this into a vector.\n","          len = g2.shape[0]                       \n","          grads2[j+y_counter, x_counter:x_counter+len] = g2             #Indexing ensures that the second order derivatives are placed in the correct positions.\n","          x_counter += len\n","\n","      grads2 = grads2.to(device)\n","      y_counter += grad.shape[0]\n","  \n","  print('Time used is ', time.time() - start)\n","\n","  return grads2\n","\n","#Prepare optimizer and network.\n","input = torch.tensor([1.0,1.0])\n","target = torch.tensor([2.0,0.0])\n","optimizer = optim.RMSprop(func.parameters(), lr=1e-3) #func.parameters are the parameters to optimise.\n","optimizer.zero_grad()\n","parameters = optimizer.param_groups[0]['params']\n","pred_y = func(input)\n","loss = torch.linalg.norm(pred_y - target)\n","loss.backward(create_graph=True)\n","\n","#Obtaining the gradients in this way is preferable (over torch.autograd.grad()) since it can be used after loss.backward().\n","grads = [0,0,0,0] \n","for counter, param in enumerate(func.parameters()):\n","  grads[counter] = param.grad\n","grads = tuple(grads)\n","\n","get_manual_hessian(grads, parameters)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZXaXEutJr3A0"},"source":["\"\"\"\n","-------------------------------------------------------------------------------------------------------------------------------------\n","This approach works, and produces a 252 x 252 matrix. It is built using library functions and a nn.Module, which means \n","it can more easily be used in the context of NODEs. This is the method that I have implemented during the training of \n","simple NODEs.\n","Note: It requires tailoring to each neural network\n","-------------------------------------------------------------------------------------------------------------------------------------\n","\"\"\"\n","\n","class Network(nn.Module):\n","\n","  def __init__(self, a, b, c, d):\n","    super(Network, self).__init__()\n","    self.a = a\n","    self.b = b\n","    self.c = c\n","    self.d = d\n","\n","  def forward(self, y):\n","    x = F.linear(y, self.a, self.b)\n","    m = nn.Tanh()\n","    x = m(x)\n","    x = F.linear(x, self.c, self.d)\n","    return x\n","\n","\n","def get_loss_square_2(params_vector):\n","\n","  a = params_vector[0:100].reshape([50, 2])\n","  b = params_vector[100:150].reshape([50])\n","  c = params_vector[150:250].reshape([2, 50])\n","  d = params_vector[250:252].reshape([2])\n","  \n","  neural_net = Network(a, b, c, d).to(device)\n","  input = torch.tensor([1.0,1.0]).to(device)\n","  target = torch.tensor([2.0,0.0]).to(device)\n","  pred_y = neural_net(input)\n","  \n","  loss = torch.linalg.norm(pred_y - target) \n","  return loss\n","\n","def get_library_hessian(net):\n","\n","  param_tensors = net.parameters()\n","  params_vector = torch.tensor([]).to(device)   \n","  for param in param_tensors:\n","    vec = torch.reshape(param, (-1,)).to(device)\n","    params_vector = torch.cat((params_vector, vec))\n","    \n","\n","  hessian = torch.autograd.functional.hessian(get_loss_square_2, params_vector)\n","  return hessian"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oTWqpHPV92p3"},"source":["\"\"\"\n","--------------------------------------------------------------------------------------------------------------------------------------------\n","The following code is used to test some very simple examples that can be compared to analytical methods.\n","--------------------------------------------------------------------------------------------------------------------------------------------\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JdmYW7eE7x4N"},"source":["class Simple_Net(nn.Module):\n","    \"\"\"\n","    Defines a very simple neural network that can be evaluated analytically.\n","    \"\"\"\n","\n","    def __init__(self):\n","        super(Simple_Net, self).__init__()\n","\n","        #Define a very simple neural network architecture with no hidden layers.\n","        self.net = nn.Sequential(\n","            nn.Linear(2, 2),\n","            nn.Sigmoid()\n","        )\n","\n","        #Initialise the weights and biases for the linear layers.\n","        #The isinstance functions checks that the first input is an instance or subclass of the second argument.\n","        \n","        for m in self.net.modules():\n","          if isinstance(m, nn.Linear):\n","            nn.init.normal_(m.weight, mean=0, std=0.1)\n","            nn.init.constant_(m.bias, val=0)\n","        \n","    #The forward function defines how the data is passed through the neural net. \n","    #In particular, it is called when you apply the neural net to an input variable.\n","    #We act the net on y**3 such that it is only learning to represent the matrix (see class Lambda)\n","\n","    def forward(self, y):\n","        return self.net(y)\n","\n","simple_func = Simple_Net()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Aw26VPlQB_9g"},"source":["#Show the parameters of the network.\n","for param in simple_func.parameters():\n","  print(param)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k6RBWLdALhcd"},"source":["#Data input and model set up.\n","input = torch.tensor([1.0,1.0])\n","target = torch.tensor([2.0,0.0])\n","optimizer = optim.RMSprop(simple_func.parameters(), lr=1e-3) #net.parameters are the parameters to optimise.\n","optimizer.zero_grad()\n","parameters = optimizer.param_groups[0]['params']\n","loss = torch.linalg.norm(simple_func(input) - target)\n","print(\"Loss is \" + str(loss.item()))\n","print('Output is ' + str(simple_func(input)[0].item()) + \", \" + str(simple_func(input)[1].item()))\n","\n","grads = torch.autograd.grad(loss, parameters, create_graph=True)  # first order gradients\n","print(\"Gradients are: \" + str(grads[0]))\n","hessian = get_manual_hessian(grads, parameters)  # calculate hessian"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8potn2t4UDJN"},"source":["\"\"\"\n","-------------------------------------------------------------------------------------------------------------------------------------\n","This approach works, and produces a 252 x 252 matrix. It is built using library functions and a nn.Module, which means \n","it can more easily be used in the context of NODEs. This is the method that I have implemented during the training of \n","simple NODEs.\n","It is not quite \"automated\", since it still requires that you define the function with the correct number of parameters 'groups',\n","which is individual to each network.\n","-------------------------------------------------------------------------------------------------------------------------------------\n","\"\"\"\n","\n","class Network(nn.Module):\n","\n","  def __init__(self, a, b):\n","    super(Network, self).__init__()\n","    self.a = a\n","    self.b = b\n","\n","  def forward(self, y):\n","    x = F.linear(y, self.a, self.b)\n","    m = nn.Tanh()\n","    x = m(x)\n","    return x\n","\n","\n","def get_simple_loss_square(params_vector):\n","\n","  a = params_vector[0:4].reshape([2, 2])\n","  b = params_vector[4:6].reshape([2])\n","  \n","  neural_net = Network(a, b).to(device)\n","  input = torch.tensor([1.0,1.0]).to(device)\n","  target = torch.tensor([2.0,0.0]).to(device)\n","  pred_y = neural_net(input)\n","  \n","  loss = torch.linalg.norm(pred_y - target)**2\n","  return loss\n","\n","def get_simple_library_hessian(net):\n","\n","  param_tensors = net.parameters()\n","  params_vector = torch.tensor([]).to(device)   \n","  for param in param_tensors:\n","    vec = torch.reshape(param, (-1,)).to(device)\n","    params_vector = torch.cat((params_vector, vec))\n","    \n","\n","  hessian = torch.autograd.functional.hessian(get_simple_loss_square, params_vector)\n","  return hessian"],"execution_count":null,"outputs":[]}]}