/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mnist_node.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1sLUaZ_DEMKH30zwclvQKQ2FNZUI_mF3K","authorship_tag":"ABX9TyOFAmF1ieeE5tKH6M1dXYs0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"950091007f4b44d9a71db065b0adb84d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_adcdb6878348426993bdbec79164b02f","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_7c967cef3e69444d8f9d3e3bd1b94307","IPY_MODEL_2bcc97d26ac7436c8eceae24f12a3ec0"]}},"adcdb6878348426993bdbec79164b02f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7c967cef3e69444d8f9d3e3bd1b94307":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c51eade195eb425b9ef787c716c6225c","_dom_classes":[],"description":" 13%","_model_name":"FloatProgressModel","bar_style":"danger","max":9912422,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1277952,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_10aa6d55e043411ab8a091c6eb2e3d39"}},"2bcc97d26ac7436c8eceae24f12a3ec0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b5a3e8a8e26a457899e96c6cac3da18a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1277952/9912422 [00:29&lt;03:20, 43002.37it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_14b806c30e32401088e7d14b0b06f0da"}},"c51eade195eb425b9ef787c716c6225c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"10aa6d55e043411ab8a091c6eb2e3d39":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b5a3e8a8e26a457899e96c6cac3da18a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"14b806c30e32401088e7d14b0b06f0da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"sWmsXi5BIAtb","executionInfo":{"status":"ok","timestamp":1617963280508,"user_tz":-60,"elapsed":4957,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["%%capture\n","%%bash\n","pip install torchdiffeq"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"H353RG8fIDd3","executionInfo":{"status":"ok","timestamp":1617964378446,"user_tz":-60,"elapsed":1170,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["import os\n","import argparse\n","import logging\n","import time\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","\n","\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')\n","parser.add_argument('--tol', type=float, default=1e-3)\n","parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])\n","parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])\n","parser.add_argument('--nepochs', type=int, default=120)\n","parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])\n","parser.add_argument('--lr', type=float, default=0.1)\n","parser.add_argument('--batch_size', type=int, default=128)\n","parser.add_argument('--test_batch_size', type=int, default=1000)\n","\n","parser.add_argument('--save', type=str, default='./experiment_node1')\n","parser.add_argument('--gpu', type=int, default=0)\n","parser.add_argument('--debug', action='store_true')\n","args = parser.parse_args(args=[])\n","\n","args.save = '/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/experiment_node1'\n","\n","if args.adjoint:\n","    from torchdiffeq import odeint_adjoint as odeint\n","else:\n","    from torchdiffeq import odeint"],"execution_count":57,"outputs":[]},{"cell_type":"code","metadata":{"id":"agpZtaKwIKnA","executionInfo":{"status":"ok","timestamp":1617963497293,"user_tz":-60,"elapsed":412,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["def conv3x3(in_planes, out_planes, stride=1):\n","    \"\"\"3x3 convolution with padding\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","\n","def conv1x1(in_planes, out_planes, stride=1):\n","    \"\"\"1x1 convolution\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n","\n","\n","def norm(dim):\n","    return nn.GroupNorm(min(32, dim), dim)\n","\n","\n","class ResBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None):\n","        super(ResBlock, self).__init__()\n","        self.norm1 = norm(inplanes)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.downsample = downsample\n","        self.conv1 = conv3x3(inplanes, planes, stride)\n","        self.norm2 = norm(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","\n","    def forward(self, x):\n","        shortcut = x\n","\n","        out = self.relu(self.norm1(x))\n","\n","        if self.downsample is not None:\n","            shortcut = self.downsample(out)\n","\n","        out = self.conv1(out)\n","        out = self.norm2(out)\n","        out = self.relu(out)\n","        out = self.conv2(out)\n","\n","        return out + shortcut\n","\n","\n","class ConcatConv2d(nn.Module):\n","\n","    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):\n","        super(ConcatConv2d, self).__init__()\n","        module = nn.ConvTranspose2d if transpose else nn.Conv2d\n","        self._layer = module(\n","            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,\n","            bias=bias\n","        )\n","\n","    def forward(self, t, x):\n","        tt = torch.ones_like(x[:, :1, :, :]) * t\n","        ttx = torch.cat([tt, x], 1)\n","        return self._layer(ttx)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"1-rlM_3sI-1k","executionInfo":{"status":"ok","timestamp":1617963513110,"user_tz":-60,"elapsed":974,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["class ODEfunc(nn.Module):\n","\n","    def __init__(self, dim):\n","        super(ODEfunc, self).__init__()\n","        self.norm1 = norm(dim)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv1 = ConcatConv2d(dim, dim, 3, 1, 1)\n","        self.norm2 = norm(dim)\n","        self.conv2 = ConcatConv2d(dim, dim, 3, 1, 1)\n","        self.norm3 = norm(dim)\n","        self.nfe = 0\n","\n","    def forward(self, t, x):\n","        self.nfe += 1\n","        out = self.norm1(x)\n","        out = self.relu(out)\n","        out = self.conv1(t, out)\n","        out = self.norm2(out)\n","        out = self.relu(out)\n","        out = self.conv2(t, out)\n","        out = self.norm3(out)\n","        return out\n","\n","\n","class ODEBlock(nn.Module):\n","\n","    def __init__(self, odefunc):\n","        super(ODEBlock, self).__init__()\n","        self.odefunc = odefunc\n","        self.integration_time = torch.tensor([0, 1]).float()\n","\n","    def forward(self, x):\n","        self.integration_time = self.integration_time.type_as(x)\n","        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)\n","        return out[1]\n","\n","    @property\n","    def nfe(self):\n","        return self.odefunc.nfe\n","\n","    @nfe.setter\n","    def nfe(self, value):\n","        self.odefunc.nfe = value"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"xhFiCzNaJC0x","executionInfo":{"status":"ok","timestamp":1617963529496,"user_tz":-60,"elapsed":991,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["class Flatten(nn.Module):\n","\n","    def __init__(self):\n","        super(Flatten, self).__init__()\n","\n","    def forward(self, x):\n","        shape = torch.prod(torch.tensor(x.shape[1:])).item()\n","        return x.view(-1, shape)\n","\n","\n","class RunningAverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","\n","    def __init__(self, momentum=0.99):\n","        self.momentum = momentum\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = None\n","        self.avg = 0\n","\n","    def update(self, val):\n","        if self.val is None:\n","            self.avg = val\n","        else:\n","            self.avg = self.avg * self.momentum + val * (1 - self.momentum)\n","        self.val = val"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"ltcgl1RIJG7U","executionInfo":{"status":"ok","timestamp":1617963610116,"user_tz":-60,"elapsed":397,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["def get_mnist_loaders(data_aug=False, batch_size=128, test_batch_size=1000, perc=1.0):\n","    if data_aug:\n","        transform_train = transforms.Compose([\n","            transforms.RandomCrop(28, padding=4),\n","            transforms.ToTensor(),\n","        ])\n","    else:\n","        transform_train = transforms.Compose([\n","            transforms.ToTensor(),\n","        ])\n","\n","    transform_test = transforms.Compose([\n","        transforms.ToTensor(),\n","    ])\n","\n","    train_loader = DataLoader(\n","        datasets.MNIST(root='.data/mnist', train=True, download=True, transform=transform_train), batch_size=batch_size,\n","        shuffle=True, num_workers=2, drop_last=True\n","    )\n","\n","    train_eval_loader = DataLoader(\n","        datasets.MNIST(root='.data/mnist', train=True, download=True, transform=transform_test),\n","        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True\n","    )\n","\n","    test_loader = DataLoader(\n","        datasets.MNIST(root='.data/mnist', train=False, download=True, transform=transform_test),\n","        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True\n","    )\n","\n","    return train_loader, test_loader, train_eval_loader\n","\n","\n","def inf_generator(iterable):\n","    \"\"\"Allows training with DataLoaders in a single infinite loop:\n","        for i, (x, y) in enumerate(inf_generator(train_loader)):\n","    \"\"\"\n","    iterator = iterable.__iter__()\n","    while True:\n","        try:\n","            yield iterator.__next__()\n","        except StopIteration:\n","            iterator = iterable.__iter__()\n","\n","\n","def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):\n","    initial_learning_rate = args.lr * batch_size / batch_denom\n","\n","    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]\n","    vals = [initial_learning_rate * decay for decay in decay_rates]\n","\n","    def learning_rate_fn(itr):\n","        lt = [itr < b for b in boundaries] + [True]\n","        i = np.argmax(lt)\n","        return vals[i]\n","\n","    return learning_rate_fn\n","\n","\n","def one_hot(x, K):\n","    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)\n","\n","\n","def accuracy(model, dataset_loader):\n","    total_correct = 0\n","    for x, y in dataset_loader:\n","        x = x.to(device)\n","        y = one_hot(np.array(y.numpy()), 10)\n","\n","        target_class = np.argmax(y, axis=1)\n","        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)\n","        total_correct += np.sum(predicted_class == target_class)\n","    return total_correct / len(dataset_loader.dataset)\n","\n","\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","\n","def makedirs(dirname):\n","    if not os.path.exists(dirname):\n","        os.makedirs(dirname)\n","\n","\n","def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):\n","    logger = logging.getLogger()\n","    if debug:\n","        level = logging.DEBUG\n","    else:\n","        level = logging.INFO\n","    logger.setLevel(level)\n","    if saving:\n","        info_file_handler = logging.FileHandler(logpath, mode=\"a\")\n","        info_file_handler.setLevel(level)\n","        logger.addHandler(info_file_handler)\n","    if displaying:\n","        console_handler = logging.StreamHandler()\n","        console_handler.setLevel(level)\n","        logger.addHandler(console_handler)\n","    logger.info(filepath)\n","    with open(filepath, \"r\") as f:\n","        logger.info(f.read())\n","\n","    for f in package_files:\n","        logger.info(f)\n","        with open(f, \"r\") as package_f:\n","            logger.info(package_f.read())\n","\n","    return logger\n"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["950091007f4b44d9a71db065b0adb84d","adcdb6878348426993bdbec79164b02f","7c967cef3e69444d8f9d3e3bd1b94307","2bcc97d26ac7436c8eceae24f12a3ec0","c51eade195eb425b9ef787c716c6225c","10aa6d55e043411ab8a091c6eb2e3d39","b5a3e8a8e26a457899e96c6cac3da18a","14b806c30e32401088e7d14b0b06f0da"]},"id":"42RIwJ37JKQr","executionInfo":{"status":"error","timestamp":1617964346150,"user_tz":-60,"elapsed":31238,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}},"outputId":"886a21d9-8e90-48d6-90a9-ac9b544a32f6"},"source":["if __name__ == '__main__':\n","    __file__ = '/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb'\n","    makedirs(args.save)\n","    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))\n","    logger.info(args)\n","\n","    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')\n","\n","    is_odenet = args.network == 'odenet'\n","\n","    if args.downsampling_method == 'conv':\n","        downsampling_layers = [\n","            nn.Conv2d(1, 64, 3, 1),\n","            norm(64),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(64, 64, 4, 2, 1),\n","            norm(64),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(64, 64, 4, 2, 1),\n","        ]\n","    elif args.downsampling_method == 'res':\n","        downsampling_layers = [\n","            nn.Conv2d(1, 64, 3, 1),\n","            ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\n","            ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\n","        ]\n","\n","    feature_layers = [ODEBlock(ODEfunc(64))] if is_odenet else [ResBlock(64, 64) for _ in range(6)]\n","    fc_layers = [norm(64), nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d((1, 1)), Flatten(), nn.Linear(64, 10)]\n","\n","    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)\n","\n","    logger.info(model)\n","    logger.info('Number of parameters: {}'.format(count_parameters(model)))\n","\n","    criterion = nn.CrossEntropyLoss().to(device)\n","\n","    train_loader, test_loader, train_eval_loader = get_mnist_loaders(\n","        args.data_aug, args.batch_size, args.test_batch_size\n","    )\n","\n","    data_gen = inf_generator(train_loader)\n","    batches_per_epoch = len(train_loader)\n","\n","    lr_fn = learning_rate_with_decay(\n","        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],\n","        decay_rates=[1, 0.1, 0.01, 0.001]\n","    )\n","\n","    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)\n","\n","    best_acc = 0\n","    batch_time_meter = RunningAverageMeter()\n","    f_nfe_meter = RunningAverageMeter()\n","    b_nfe_meter = RunningAverageMeter()\n","    end = time.time()\n","    \n","    epoch_arr = []\n","    time_val_arr = []\n","    time_avg_arr = []\n","    nfe_f_arr = []\n","    nfe_b_arr = []\n","    train_acc_arr = []\n","    test_acc_arr = []\n","\n","    for itr in range(args.nepochs * batches_per_epoch):\n","\n","        for param_group in optimizer.param_groups:\n","            param_group['lr'] = lr_fn(itr)\n","\n","        optimizer.zero_grad()\n","        x, y = data_gen.__next__()\n","        x = x.to(device)\n","        y = y.to(device)\n","        logits = model(x)\n","        loss = criterion(logits, y)\n","\n","        if is_odenet:\n","            nfe_forward = feature_layers[0].nfe\n","            feature_layers[0].nfe = 0\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        if is_odenet:\n","            nfe_backward = feature_layers[0].nfe\n","            feature_layers[0].nfe = 0\n","\n","        batch_time_meter.update(time.time() - end)\n","        if is_odenet:\n","            f_nfe_meter.update(nfe_forward)\n","            b_nfe_meter.update(nfe_backward)\n","        end = time.time()\n","\n","        if itr % batches_per_epoch == 0:\n","            with torch.no_grad():\n","                train_acc = accuracy(model, train_eval_loader)\n","                val_acc = accuracy(model, test_loader)\n","                if val_acc > best_acc:\n","                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))\n","                    best_acc = val_acc\n","                logger.info(\n","                    \"Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | \"\n","                    \"Train Acc {:.4f} | Test Acc {:.4f}\".format(\n","                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,\n","                        b_nfe_meter.avg, train_acc, val_acc\n","                    )\n","                )\n","                epoch_arr += [itr // batches_per_epoch]\n","                time_val_arr += [batch_time_meter.val]\n","                time_avg_arr += [batch_time_meter.avg]\n","                nfe_f_arr += [f_nfe_meter.avg]\n","                nfe_b_arr += [b_nfe_meter.avg]\n","                train_acc_arr += [train_acc]\n","                test_acc_arr += [val_acc]\n","                    \n","    epoch_arr = np.asarray(epoch_arr)\n","    time_val_arr = np.asarray(time_val_arr)\n","    time_avg_arr = np.asarray(time_avg_arr)\n","    nfe_f_arr = np.asarray(nfe_f_arr)\n","    nfe_b_arr = np.asarray(nfe_b_arr)\n","    train_acc_arr = np.asarray(train_acc_arr)\n","    test_acc_arr = np.asarray(test_acc_arr)\n","    \n","    np.save(os.path.join(args.save, 'epoch_arr.npy'), epoch_arr)\n","    np.save(os.path.join(args.save, 'time_val_arr.npy'), time_val_arr)\n","    np.save(os.path.join(args.save, 'time_avg_arr.npy'), time_avg_arr)\n","    np.save(os.path.join(args.save, 'nfe_f_arr.npy'), nfe_f_arr)\n","    np.save(os.path.join(args.save, 'nfe_b_arr.npy'), nfe_b_arr)\n","    np.save(os.path.join(args.save, 'train_acc_arr.npy'), train_acc_arr)\n","    np.save(os.path.join(args.save, 'test_acc_arr.npy'), test_acc_arr)"],"execution_count":56,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\n","/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\n","/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\n","/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\n","/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\n","/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\n","{\"nbformat\":4,\"nbformat_minor\":0,\"metadata\":{\"colab\":{\"name\":\"mnist_node.ipynb\",\"provenance\":[],\"collapsed_sections\":[],\"mount_file_id\":\"1sLUaZ_DEMKH30zwclvQKQ2FNZUI_mF3K\",\"authorship_tag\":\"ABX9TyM4RT5ovZ+PC2JziHsLNjTc\"},\"kernelspec\":{\"name\":\"python3\",\"display_name\":\"Python 3\"},\"language_info\":{\"name\":\"python\"}},\"cells\":[{\"cell_type\":\"code\",\"metadata\":{\"id\":\"sWmsXi5BIAtb\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617963280508,\"user_tz\":-60,\"elapsed\":4957,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"%%capture\\n\",\"%%bash\\n\",\"pip install torchdiffeq\"],\"execution_count\":1,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"H353RG8fIDd3\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617963632522,\"user_tz\":-60,\"elapsed\":546,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"import os\\n\",\"import argparse\\n\",\"import logging\\n\",\"import time\\n\",\"import numpy as np\\n\",\"import torch\\n\",\"import torch.nn as nn\\n\",\"from torch.utils.data import DataLoader\\n\",\"import torchvision.datasets as datasets\\n\",\"import torchvision.transforms as transforms\\n\",\"\\n\",\"\\n\",\"parser = argparse.ArgumentParser()\\n\",\"parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')\\n\",\"parser.add_argument('--tol', type=float, default=1e-3)\\n\",\"parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])\\n\",\"parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])\\n\",\"parser.add_argument('--nepochs', type=int, default=120)\\n\",\"parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])\\n\",\"parser.add_argument('--lr', type=float, default=0.1)\\n\",\"parser.add_argument('--batch_size', type=int, default=128)\\n\",\"parser.add_argument('--test_batch_size', type=int, default=1000)\\n\",\"\\n\",\"parser.add_argument('--save', type=str, default='./experiment_node1')\\n\",\"parser.add_argument('--gpu', type=int, default=0)\\n\",\"parser.add_argument('--debug', action='store_true')\\n\",\"args = parser.parse_args(args=[])\\n\",\"\\n\",\"if args.adjoint:\\n\",\"    from torchdiffeq import odeint_adjoint as odeint\\n\",\"else:\\n\",\"    from torchdiffeq import odeint\"],\"execution_count\":22,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"agpZtaKwIKnA\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617963497293,\"user_tz\":-60,\"elapsed\":412,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"def conv3x3(in_planes, out_planes, stride=1):\\n\",\"    \\\"\\\"\\\"3x3 convolution with padding\\\"\\\"\\\"\\n\",\"    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\\n\",\"\\n\",\"\\n\",\"def conv1x1(in_planes, out_planes, stride=1):\\n\",\"    \\\"\\\"\\\"1x1 convolution\\\"\\\"\\\"\\n\",\"    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\\n\",\"\\n\",\"\\n\",\"def norm(dim):\\n\",\"    return nn.GroupNorm(min(32, dim), dim)\\n\",\"\\n\",\"\\n\",\"class ResBlock(nn.Module):\\n\",\"    expansion = 1\\n\",\"\\n\",\"    def __init__(self, inplanes, planes, stride=1, downsample=None):\\n\",\"        super(ResBlock, self).__init__()\\n\",\"        self.norm1 = norm(inplanes)\\n\",\"        self.relu = nn.ReLU(inplace=True)\\n\",\"        self.downsample = downsample\\n\",\"        self.conv1 = conv3x3(inplanes, planes, stride)\\n\",\"        self.norm2 = norm(planes)\\n\",\"        self.conv2 = conv3x3(planes, planes)\\n\",\"\\n\",\"    def forward(self, x):\\n\",\"        shortcut = x\\n\",\"\\n\",\"        out = self.relu(self.norm1(x))\\n\",\"\\n\",\"        if self.downsample is not None:\\n\",\"            shortcut = self.downsample(out)\\n\",\"\\n\",\"        out = self.conv1(out)\\n\",\"        out = self.norm2(out)\\n\",\"        out = self.relu(out)\\n\",\"        out = self.conv2(out)\\n\",\"\\n\",\"        return out + shortcut\\n\",\"\\n\",\"\\n\",\"class ConcatConv2d(nn.Module):\\n\",\"\\n\",\"    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):\\n\",\"        super(ConcatConv2d, self).__init__()\\n\",\"        module = nn.ConvTranspose2d if transpose else nn.Conv2d\\n\",\"        self._layer = module(\\n\",\"            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,\\n\",\"            bias=bias\\n\",\"        )\\n\",\"\\n\",\"    def forward(self, t, x):\\n\",\"        tt = torch.ones_like(x[:, :1, :, :]) * t\\n\",\"        ttx = torch.cat([tt, x], 1)\\n\",\"        return self._layer(ttx)\"],\"execution_count\":15,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"1-rlM_3sI-1k\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617963513110,\"user_tz\":-60,\"elapsed\":974,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"class ODEfunc(nn.Module):\\n\",\"\\n\",\"    def __init__(self, dim):\\n\",\"        super(ODEfunc, self).__init__()\\n\",\"        self.norm1 = norm(dim)\\n\",\"        self.relu = nn.ReLU(inplace=True)\\n\",\"        self.conv1 = ConcatConv2d(dim, dim, 3, 1, 1)\\n\",\"        self.norm2 = norm(dim)\\n\",\"        self.conv2 = ConcatConv2d(dim, dim, 3, 1, 1)\\n\",\"        self.norm3 = norm(dim)\\n\",\"        self.nfe = 0\\n\",\"\\n\",\"    def forward(self, t, x):\\n\",\"        self.nfe += 1\\n\",\"        out = self.norm1(x)\\n\",\"        out = self.relu(out)\\n\",\"        out = self.conv1(t, out)\\n\",\"        out = self.norm2(out)\\n\",\"        out = self.relu(out)\\n\",\"        out = self.conv2(t, out)\\n\",\"        out = self.norm3(out)\\n\",\"        return out\\n\",\"\\n\",\"\\n\",\"class ODEBlock(nn.Module):\\n\",\"\\n\",\"    def __init__(self, odefunc):\\n\",\"        super(ODEBlock, self).__init__()\\n\",\"        self.odefunc = odefunc\\n\",\"        self.integration_time = torch.tensor([0, 1]).float()\\n\",\"\\n\",\"    def forward(self, x):\\n\",\"        self.integration_time = self.integration_time.type_as(x)\\n\",\"        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)\\n\",\"        return out[1]\\n\",\"\\n\",\"    @property\\n\",\"    def nfe(self):\\n\",\"        return self.odefunc.nfe\\n\",\"\\n\",\"    @nfe.setter\\n\",\"    def nfe(self, value):\\n\",\"        self.odefunc.nfe = value\"],\"execution_count\":16,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"xhFiCzNaJC0x\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617963529496,\"user_tz\":-60,\"elapsed\":991,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"class Flatten(nn.Module):\\n\",\"\\n\",\"    def __init__(self):\\n\",\"        super(Flatten, self).__init__()\\n\",\"\\n\",\"    def forward(self, x):\\n\",\"        shape = torch.prod(torch.tensor(x.shape[1:])).item()\\n\",\"        return x.view(-1, shape)\\n\",\"\\n\",\"\\n\",\"class RunningAverageMeter(object):\\n\",\"    \\\"\\\"\\\"Computes and stores the average and current value\\\"\\\"\\\"\\n\",\"\\n\",\"    def __init__(self, momentum=0.99):\\n\",\"        self.momentum = momentum\\n\",\"        self.reset()\\n\",\"\\n\",\"    def reset(self):\\n\",\"        self.val = None\\n\",\"        self.avg = 0\\n\",\"\\n\",\"    def update(self, val):\\n\",\"        if self.val is None:\\n\",\"            self.avg = val\\n\",\"        else:\\n\",\"            self.avg = self.avg * self.momentum + val * (1 - self.momentum)\\n\",\"        self.val = val\"],\"execution_count\":17,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"ltcgl1RIJG7U\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617963610116,\"user_tz\":-60,\"elapsed\":397,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"def get_mnist_loaders(data_aug=False, batch_size=128, test_batch_size=1000, perc=1.0):\\n\",\"    if data_aug:\\n\",\"        transform_train = transforms.Compose([\\n\",\"            transforms.RandomCrop(28, padding=4),\\n\",\"            transforms.ToTensor(),\\n\",\"        ])\\n\",\"    else:\\n\",\"        transform_train = transforms.Compose([\\n\",\"            transforms.ToTensor(),\\n\",\"        ])\\n\",\"\\n\",\"    transform_test = transforms.Compose([\\n\",\"        transforms.ToTensor(),\\n\",\"    ])\\n\",\"\\n\",\"    train_loader = DataLoader(\\n\",\"        datasets.MNIST(root='.data/mnist', train=True, download=True, transform=transform_train), batch_size=batch_size,\\n\",\"        shuffle=True, num_workers=2, drop_last=True\\n\",\"    )\\n\",\"\\n\",\"    train_eval_loader = DataLoader(\\n\",\"        datasets.MNIST(root='.data/mnist', train=True, download=True, transform=transform_test),\\n\",\"        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True\\n\",\"    )\\n\",\"\\n\",\"    test_loader = DataLoader(\\n\",\"        datasets.MNIST(root='.data/mnist', train=False, download=True, transform=transform_test),\\n\",\"        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True\\n\",\"    )\\n\",\"\\n\",\"    return train_loader, test_loader, train_eval_loader\\n\",\"\\n\",\"\\n\",\"def inf_generator(iterable):\\n\",\"    \\\"\\\"\\\"Allows training with DataLoaders in a single infinite loop:\\n\",\"        for i, (x, y) in enumerate(inf_generator(train_loader)):\\n\",\"    \\\"\\\"\\\"\\n\",\"    iterator = iterable.__iter__()\\n\",\"    while True:\\n\",\"        try:\\n\",\"            yield iterator.__next__()\\n\",\"        except StopIteration:\\n\",\"            iterator = iterable.__iter__()\\n\",\"\\n\",\"\\n\",\"def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):\\n\",\"    initial_learning_rate = args.lr * batch_size / batch_denom\\n\",\"\\n\",\"    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]\\n\",\"    vals = [initial_learning_rate * decay for decay in decay_rates]\\n\",\"\\n\",\"    def learning_rate_fn(itr):\\n\",\"        lt = [itr < b for b in boundaries] + [True]\\n\",\"        i = np.argmax(lt)\\n\",\"        return vals[i]\\n\",\"\\n\",\"    return learning_rate_fn\\n\",\"\\n\",\"\\n\",\"def one_hot(x, K):\\n\",\"    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)\\n\",\"\\n\",\"\\n\",\"def accuracy(model, dataset_loader):\\n\",\"    total_correct = 0\\n\",\"    for x, y in dataset_loader:\\n\",\"        x = x.to(device)\\n\",\"        y = one_hot(np.array(y.numpy()), 10)\\n\",\"\\n\",\"        target_class = np.argmax(y, axis=1)\\n\",\"        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)\\n\",\"        total_correct += np.sum(predicted_class == target_class)\\n\",\"    return total_correct / len(dataset_loader.dataset)\\n\",\"\\n\",\"\\n\",\"def count_parameters(model):\\n\",\"    return sum(p.numel() for p in model.parameters() if p.requires_grad)\\n\",\"\\n\",\"\\n\",\"def makedirs(dirname):\\n\",\"    if not os.path.exists(dirname):\\n\",\"        os.makedirs(dirname)\\n\",\"\\n\",\"\\n\",\"def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):\\n\",\"    logger = logging.getLogger()\\n\",\"    if debug:\\n\",\"        level = logging.DEBUG\\n\",\"    else:\\n\",\"        level = logging.INFO\\n\",\"    logger.setLevel(level)\\n\",\"    if saving:\\n\",\"        info_file_handler = logging.FileHandler(logpath, mode=\\\"a\\\")\\n\",\"        info_file_handler.setLevel(level)\\n\",\"        logger.addHandler(info_file_handler)\\n\",\"    if displaying:\\n\",\"        console_handler = logging.StreamHandler()\\n\",\"        console_handler.setLevel(level)\\n\",\"        logger.addHandler(console_handler)\\n\",\"    logger.info(filepath)\\n\",\"    with open(filepath, \\\"r\\\") as f:\\n\",\"        logger.info(f.read())\\n\",\"\\n\",\"    for f in package_files:\\n\",\"        logger.info(f)\\n\",\"        with open(f, \\\"r\\\") as package_f:\\n\",\"            logger.info(package_f.read())\\n\",\"\\n\",\"    return logger\\n\"],\"execution_count\":20,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"colab\":{\"base_uri\":\"https://localhost:8080/\",\"height\":470},\"id\":\"42RIwJ37JKQr\",\"executionInfo\":{\"status\":\"error\",\"timestamp\":1617964221740,\"user_tz\":-60,\"elapsed\":825,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}},\"outputId\":\"076c12d7-0015-4846-a548-eeaec45004fd\"},\"source\":[\"if __name__ == '__main__':\\n\",\"    __file__ = '/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb'\\n\",\"    makedirs(args.save)\\n\",\"    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))\\n\",\"    logger.info(args)\\n\",\"\\n\",\"    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')\\n\",\"\\n\",\"    is_odenet = args.network == 'odenet'\\n\",\"\\n\",\"    if args.downsampling_method == 'conv':\\n\",\"        downsampling_layers = [\\n\",\"            nn.Conv2d(1, 64, 3, 1),\\n\",\"            norm(64),\\n\",\"            nn.ReLU(inplace=True),\\n\",\"            nn.Conv2d(64, 64, 4, 2, 1),\\n\",\"            norm(64),\\n\",\"            nn.ReLU(inplace=True),\\n\",\"            nn.Conv2d(64, 64, 4, 2, 1),\\n\",\"        ]\\n\",\"    elif args.downsampling_method == 'res':\\n\",\"        downsampling_layers = [\\n\",\"            nn.Conv2d(1, 64, 3, 1),\\n\",\"            ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\\n\",\"            ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\\n\",\"        ]\\n\",\"\\n\",\"    feature_layers = [ODEBlock(ODEfunc(64))] if is_odenet else [ResBlock(64, 64) for _ in range(6)]\\n\",\"    fc_layers = [norm(64), nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d((1, 1)), Flatten(), nn.Linear(64, 10)]\\n\",\"\\n\",\"    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)\\n\",\"\\n\",\"    logger.info(model)\\n\",\"    logger.info('Number of parameters: {}'.format(count_parameters(model)))\\n\",\"\\n\",\"    criterion = nn.CrossEntropyLoss().to(device)\\n\",\"\\n\",\"    train_loader, test_loader, train_eval_loader = get_mnist_loaders(\\n\",\"        args.data_aug, args.batch_size, args.test_batch_size\\n\",\"    )\\n\",\"\\n\",\"    data_gen = inf_generator(train_loader)\\n\",\"    batches_per_epoch = len(train_loader)\\n\",\"\\n\",\"    lr_fn = learning_rate_with_decay(\\n\",\"        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],\\n\",\"        decay_rates=[1, 0.1, 0.01, 0.001]\\n\",\"    )\\n\",\"\\n\",\"    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)\\n\",\"\\n\",\"    best_acc = 0\\n\",\"    batch_time_meter = RunningAverageMeter()\\n\",\"    f_nfe_meter = RunningAverageMeter()\\n\",\"    b_nfe_meter = RunningAverageMeter()\\n\",\"    end = time.time()\\n\",\"    \\n\",\"    epoch_arr = []\\n\",\"    time_val_arr = []\\n\",\"    time_avg_arr = []\\n\",\"    nfe_f_arr = []\\n\",\"    nfe_b_arr = []\\n\",\"    train_acc_arr = []\\n\",\"    test_acc_arr = []\\n\",\"\\n\",\"    for itr in range(args.nepochs * batches_per_epoch):\\n\",\"\\n\",\"        for param_group in optimizer.param_groups:\\n\",\"            param_group['lr'] = lr_fn(itr)\\n\",\"\\n\",\"        optimizer.zero_grad()\\n\",\"        x, y = data_gen.__next__()\\n\",\"        x = x.to(device)\\n\",\"        y = y.to(device)\\n\",\"        logits = model(x)\\n\",\"        loss = criterion(logits, y)\\n\",\"\\n\",\"        if is_odenet:\\n\",\"            nfe_forward = feature_layers[0].nfe\\n\",\"            feature_layers[0].nfe = 0\\n\",\"\\n\",\"        loss.backward()\\n\",\"        optimizer.step()\\n\",\"\\n\",\"        if is_odenet:\\n\",\"            nfe_backward = feature_layers[0].nfe\\n\",\"            feature_layers[0].nfe = 0\\n\",\"\\n\",\"        batch_time_meter.update(time.time() - end)\\n\",\"        if is_odenet:\\n\",\"            f_nfe_meter.update(nfe_forward)\\n\",\"            b_nfe_meter.update(nfe_backward)\\n\",\"        end = time.time()\\n\",\"\\n\",\"        if itr % batches_per_epoch == 0:\\n\",\"            with torch.no_grad():\\n\",\"                train_acc = accuracy(model, train_eval_loader)\\n\",\"                val_acc = accuracy(model, test_loader)\\n\",\"                if val_acc > best_acc:\\n\",\"                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))\\n\",\"                    best_acc = val_acc\\n\",\"                logger.info(\\n\",\"                    \\\"Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | \\\"\\n\",\"                    \\\"Train Acc {:.4f} | Test Acc {:.4f}\\\".format(\\n\",\"                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,\\n\",\"                        b_nfe_meter.avg, train_acc, val_acc\\n\",\"                    )\\n\",\"                )\\n\",\"                epoch_arr += [itr // batches_per_epoch]\\n\",\"                time_val_arr += [batch_time_meter.val]\\n\",\"                time_avg_arr += [batch_time_meter.avg]\\n\",\"                nfe_f_arr += [f_nfe_meter.avg]\\n\",\"                nfe_b_arr += [b_nfe_meter.avg]\\n\",\"                train_acc_arr += [train_acc]\\n\",\"                test_acc_arr += [val_acc]\\n\",\"                    \\n\",\"    epoch_arr = np.asarray(epoch_arr)\\n\",\"    time_val_arr = np.asarray(time_val_arr)\\n\",\"    time_avg_arr = np.asarray(time_avg_arr)\\n\",\"    nfe_f_arr = np.asarray(nfe_f_arr)\\n\",\"    nfe_b_arr = np.asarray(nfe_b_arr)\\n\",\"    train_acc_arr = np.asarray(train_acc_arr)\\n\",\"    test_acc_arr = np.asarray(test_acc_arr)\\n\",\"    \\n\",\"    np.save(os.path.join(args.save, 'epoch_arr.npy'), epoch_arr)\\n\",\"    np.save(os.path.join(args.save, 'time_val_arr.npy'), time_val_arr)\\n\",\"    np.save(os.path.join(args.save, 'time_avg_arr.npy'), time_avg_arr)\\n\",\"    np.save(os.path.join(args.save, 'nfe_f_arr.npy'), nfe_f_arr)\\n\",\"    np.save(os.path.join(args.save, 'nfe_b_arr.npy'), nfe_b_arr)\\n\",\"    np.save(os.path.join(args.save, 'train_acc_arr.npy'), train_acc_arr)\\n\",\"    np.save(os.path.join(args.save, 'test_acc_arr.npy'), test_acc_arr)\"],\"execution_count\":50,\"outputs\":[{\"output_type\":\"stream\",\"text\":[\"/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\\n\",\"/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\\n\",\"/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\\n\",\"/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\\n\",\"/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\\n\"],\"name\":\"stderr\"},{\"output_type\":\"error\",\"ename\":\"FileNotFoundError\",\"evalue\":\"ignored\",\"traceback\":[\"\\u001b[0;31m---------------------------------------------------------------------------\\u001b[0m\",\"\\u001b[0;31mFileNotFoundError\\u001b[0m                         Traceback (most recent call last)\",\"\\u001b[0;32m/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\\u001b[0m in \\u001b[0;36m<module>\\u001b[0;34m()\\u001b[0m\\n\\u001b[1;32m      2\\u001b[0m     \\u001b[0m__file__\\u001b[0m \\u001b[0;34m=\\u001b[0m \\u001b[0;34m'/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb'\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m      3\\u001b[0m     \\u001b[0mmakedirs\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0margs\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0msave\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m----> 4\\u001b[0;31m     \\u001b[0mlogger\\u001b[0m \\u001b[0;34m=\\u001b[0m \\u001b[0mget_logger\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mlogpath\\u001b[0m\\u001b[0;34m=\\u001b[0m\\u001b[0mos\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mpath\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mjoin\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0margs\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0msave\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0;34m'logs'\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0mfilepath\\u001b[0m\\u001b[0;34m=\\u001b[0m\\u001b[0mos\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mpath\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mabspath\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0m__file__\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0m\\u001b[1;32m      5\\u001b[0m     \\u001b[0mlogger\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0minfo\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0margs\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m      6\\u001b[0m \\u001b[0;34m\\u001b[0m\\u001b[0m\\n\",\"\\u001b[0;32m/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\\u001b[0m in \\u001b[0;36mget_logger\\u001b[0;34m(logpath, filepath, package_files, displaying, saving, debug)\\u001b[0m\\n\\u001b[1;32m     99\\u001b[0m         \\u001b[0mlogger\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0maddHandler\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mconsole_handler\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m    100\\u001b[0m     \\u001b[0mlogger\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0minfo\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mfilepath\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m--> 101\\u001b[0;31m     \\u001b[0;32mwith\\u001b[0m \\u001b[0mopen\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mfilepath\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0;34m\\\"r\\\"\\u001b[0m\\u001b[0;34m)\\u001b[0m \\u001b[0;32mas\\u001b[0m \\u001b[0mf\\u001b[0m\\u001b[0;34m:\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0m\\u001b[1;32m    102\\u001b[0m         \\u001b[0mlogger\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0minfo\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mf\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mread\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m    103\\u001b[0m \\u001b[0;34m\\u001b[0m\\u001b[0m\\n\",\"\\u001b[0;31mFileNotFoundError\\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb'\"]}]},{\"cell_type\":\"code\",\"metadata\":{\"colab\":{\"base_uri\":\"https://localhost:8080/\"},\"id\":\"g2k3GwQeJsNy\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617963867638,\"user_tz\":-60,\"elapsed\":420,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}},\"outputId\":\"78f13612-fb4b-4d78-a6f9-3c951d4848a1\"},\"source\":[\"a = os.path.abspath('')\\n\",\"print(type(a))\"],\"execution_count\":35,\"outputs\":[{\"output_type\":\"stream\",\"text\":[\"<class 'str'>\\n\"],\"name\":\"stdout\"}]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"_jhUzE0XKh_C\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617964280066,\"user_tz\":-60,\"elapsed\":1044,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"__file__ = '/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb'\\n\",\"makedirs(args.save)\"],\"execution_count\":52,\"outputs\":[]}]}\n","{\"nbformat\":4,\"nbformat_minor\":0,\"metadata\":{\"colab\":{\"name\":\"mnist_node.ipynb\",\"provenance\":[],\"collapsed_sections\":[],\"mount_file_id\":\"1sLUaZ_DEMKH30zwclvQKQ2FNZUI_mF3K\",\"authorship_tag\":\"ABX9TyM4RT5ovZ+PC2JziHsLNjTc\"},\"kernelspec\":{\"name\":\"python3\",\"display_name\":\"Python 3\"},\"language_info\":{\"name\":\"python\"}},\"cells\":[{\"cell_type\":\"code\",\"metadata\":{\"id\":\"sWmsXi5BIAtb\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617963280508,\"user_tz\":-60,\"elapsed\":4957,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"%%capture\\n\",\"%%bash\\n\",\"pip install torchdiffeq\"],\"execution_count\":1,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"H353RG8fIDd3\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617963632522,\"user_tz\":-60,\"elapsed\":546,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"import os\\n\",\"import argparse\\n\",\"import logging\\n\",\"import time\\n\",\"import numpy as np\\n\",\"import torch\\n\",\"import torch.nn as nn\\n\",\"from torch.utils.data import DataLoader\\n\",\"import torchvision.datasets as datasets\\n\",\"import torchvision.transforms as transforms\\n\",\"\\n\",\"\\n\",\"parser = argparse.ArgumentParser()\\n\",\"parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')\\n\",\"parser.add_argument('--tol', type=float, default=1e-3)\\n\",\"parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])\\n\",\"parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])\\n\",\"parser.add_argument('--nepochs', type=int, default=120)\\n\",\"parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])\\n\",\"parser.add_argument('--lr', type=float, default=0.1)\\n\",\"parser.add_argument('--batch_size', type=int, default=128)\\n\",\"parser.add_argument('--test_batch_size', type=int, default=1000)\\n\",\"\\n\",\"parser.add_argument('--save', type=str, default='./experiment_node1')\\n\",\"parser.add_argument('--gpu', type=int, default=0)\\n\",\"parser.add_argument('--debug', action='store_true')\\n\",\"args = parser.parse_args(args=[])\\n\",\"\\n\",\"if args.adjoint:\\n\",\"    from torchdiffeq import odeint_adjoint as odeint\\n\",\"else:\\n\",\"    from torchdiffeq import odeint\"],\"execution_count\":22,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"agpZtaKwIKnA\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617963497293,\"user_tz\":-60,\"elapsed\":412,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"def conv3x3(in_planes, out_planes, stride=1):\\n\",\"    \\\"\\\"\\\"3x3 convolution with padding\\\"\\\"\\\"\\n\",\"    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\\n\",\"\\n\",\"\\n\",\"def conv1x1(in_planes, out_planes, stride=1):\\n\",\"    \\\"\\\"\\\"1x1 convolution\\\"\\\"\\\"\\n\",\"    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\\n\",\"\\n\",\"\\n\",\"def norm(dim):\\n\",\"    return nn.GroupNorm(min(32, dim), dim)\\n\",\"\\n\",\"\\n\",\"class ResBlock(nn.Module):\\n\",\"    expansion = 1\\n\",\"\\n\",\"    def __init__(self, inplanes, planes, stride=1, downsample=None):\\n\",\"        super(ResBlock, self).__init__()\\n\",\"        self.norm1 = norm(inplanes)\\n\",\"        self.relu = nn.ReLU(inplace=True)\\n\",\"        self.downsample = downsample\\n\",\"        self.conv1 = conv3x3(inplanes, planes, stride)\\n\",\"        self.norm2 = norm(planes)\\n\",\"        self.conv2 = conv3x3(planes, planes)\\n\",\"\\n\",\"    def forward(self, x):\\n\",\"        shortcut = x\\n\",\"\\n\",\"        out = self.relu(self.norm1(x))\\n\",\"\\n\",\"        if self.downsample is not None:\\n\",\"            shortcut = self.downsample(out)\\n\",\"\\n\",\"        out = self.conv1(out)\\n\",\"        out = self.norm2(out)\\n\",\"        out = self.relu(out)\\n\",\"        out = self.conv2(out)\\n\",\"\\n\",\"        return out + shortcut\\n\",\"\\n\",\"\\n\",\"class ConcatConv2d(nn.Module):\\n\",\"\\n\",\"    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):\\n\",\"        super(ConcatConv2d, self).__init__()\\n\",\"        module = nn.ConvTranspose2d if transpose else nn.Conv2d\\n\",\"        self._layer = module(\\n\",\"            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,\\n\",\"            bias=bias\\n\",\"        )\\n\",\"\\n\",\"    def forward(self, t, x):\\n\",\"        tt = torch.ones_like(x[:, :1, :, :]) * t\\n\",\"        ttx = torch.cat([tt, x], 1)\\n\",\"        return self._layer(ttx)\"],\"execution_count\":15,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"1-rlM_3sI-1k\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617963513110,\"user_tz\":-60,\"elapsed\":974,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"class ODEfunc(nn.Module):\\n\",\"\\n\",\"    def __init__(self, dim):\\n\",\"        super(ODEfunc, self).__init__()\\n\",\"        self.norm1 = norm(dim)\\n\",\"        self.relu = nn.ReLU(inplace=True)\\n\",\"        self.conv1 = ConcatConv2d(dim, dim, 3, 1, 1)\\n\",\"        self.norm2 = norm(dim)\\n\",\"        self.conv2 = ConcatConv2d(dim, dim, 3, 1, 1)\\n\",\"        self.norm3 = norm(dim)\\n\",\"        self.nfe = 0\\n\",\"\\n\",\"    def forward(self, t, x):\\n\",\"        self.nfe += 1\\n\",\"        out = self.norm1(x)\\n\",\"        out = self.relu(out)\\n\",\"        out = self.conv1(t, out)\\n\",\"        out = self.norm2(out)\\n\",\"        out = self.relu(out)\\n\",\"        out = self.conv2(t, out)\\n\",\"        out = self.norm3(out)\\n\",\"        return out\\n\",\"\\n\",\"\\n\",\"class ODEBlock(nn.Module):\\n\",\"\\n\",\"    def __init__(self, odefunc):\\n\",\"        super(ODEBlock, self).__init__()\\n\",\"        self.odefunc = odefunc\\n\",\"        self.integration_time = torch.tensor([0, 1]).float()\\n\",\"\\n\",\"    def forward(self, x):\\n\",\"        self.integration_time = self.integration_time.type_as(x)\\n\",\"        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)\\n\",\"        return out[1]\\n\",\"\\n\",\"    @property\\n\",\"    def nfe(self):\\n\",\"        return self.odefunc.nfe\\n\",\"\\n\",\"    @nfe.setter\\n\",\"    def nfe(self, value):\\n\",\"        self.odefunc.nfe = value\"],\"execution_count\":16,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"xhFiCzNaJC0x\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617963529496,\"user_tz\":-60,\"elapsed\":991,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"class Flatten(nn.Module):\\n\",\"\\n\",\"    def __init__(self):\\n\",\"        super(Flatten, self).__init__()\\n\",\"\\n\",\"    def forward(self, x):\\n\",\"        shape = torch.prod(torch.tensor(x.shape[1:])).item()\\n\",\"        return x.view(-1, shape)\\n\",\"\\n\",\"\\n\",\"class RunningAverageMeter(object):\\n\",\"    \\\"\\\"\\\"Computes and stores the average and current value\\\"\\\"\\\"\\n\",\"\\n\",\"    def __init__(self, momentum=0.99):\\n\",\"        self.momentum = momentum\\n\",\"        self.reset()\\n\",\"\\n\",\"    def reset(self):\\n\",\"        self.val = None\\n\",\"        self.avg = 0\\n\",\"\\n\",\"    def update(self, val):\\n\",\"        if self.val is None:\\n\",\"            self.avg = val\\n\",\"        else:\\n\",\"            self.avg = self.avg * self.momentum + val * (1 - self.momentum)\\n\",\"        self.val = val\"],\"execution_count\":17,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"ltcgl1RIJG7U\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617963610116,\"user_tz\":-60,\"elapsed\":397,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"def get_mnist_loaders(data_aug=False, batch_size=128, test_batch_size=1000, perc=1.0):\\n\",\"    if data_aug:\\n\",\"        transform_train = transforms.Compose([\\n\",\"            transforms.RandomCrop(28, padding=4),\\n\",\"            transforms.ToTensor(),\\n\",\"        ])\\n\",\"    else:\\n\",\"        transform_train = transforms.Compose([\\n\",\"            transforms.ToTensor(),\\n\",\"        ])\\n\",\"\\n\",\"    transform_test = transforms.Compose([\\n\",\"        transforms.ToTensor(),\\n\",\"    ])\\n\",\"\\n\",\"    train_loader = DataLoader(\\n\",\"        datasets.MNIST(root='.data/mnist', train=True, download=True, transform=transform_train), batch_size=batch_size,\\n\",\"        shuffle=True, num_workers=2, drop_last=True\\n\",\"    )\\n\",\"\\n\",\"    train_eval_loader = DataLoader(\\n\",\"        datasets.MNIST(root='.data/mnist', train=True, download=True, transform=transform_test),\\n\",\"        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True\\n\",\"    )\\n\",\"\\n\",\"    test_loader = DataLoader(\\n\",\"        datasets.MNIST(root='.data/mnist', train=False, download=True, transform=transform_test),\\n\",\"        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True\\n\",\"    )\\n\",\"\\n\",\"    return train_loader, test_loader, train_eval_loader\\n\",\"\\n\",\"\\n\",\"def inf_generator(iterable):\\n\",\"    \\\"\\\"\\\"Allows training with DataLoaders in a single infinite loop:\\n\",\"        for i, (x, y) in enumerate(inf_generator(train_loader)):\\n\",\"    \\\"\\\"\\\"\\n\",\"    iterator = iterable.__iter__()\\n\",\"    while True:\\n\",\"        try:\\n\",\"            yield iterator.__next__()\\n\",\"        except StopIteration:\\n\",\"            iterator = iterable.__iter__()\\n\",\"\\n\",\"\\n\",\"def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):\\n\",\"    initial_learning_rate = args.lr * batch_size / batch_denom\\n\",\"\\n\",\"    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]\\n\",\"    vals = [initial_learning_rate * decay for decay in decay_rates]\\n\",\"\\n\",\"    def learning_rate_fn(itr):\\n\",\"        lt = [itr < b for b in boundaries] + [True]\\n\",\"        i = np.argmax(lt)\\n\",\"        return vals[i]\\n\",\"\\n\",\"    return learning_rate_fn\\n\",\"\\n\",\"\\n\",\"def one_hot(x, K):\\n\",\"    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)\\n\",\"\\n\",\"\\n\",\"def accuracy(model, dataset_loader):\\n\",\"    total_correct = 0\\n\",\"    for x, y in dataset_loader:\\n\",\"        x = x.to(device)\\n\",\"        y = one_hot(np.array(y.numpy()), 10)\\n\",\"\\n\",\"        target_class = np.argmax(y, axis=1)\\n\",\"        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)\\n\",\"        total_correct += np.sum(predicted_class == target_class)\\n\",\"    return total_correct / len(dataset_loader.dataset)\\n\",\"\\n\",\"\\n\",\"def count_parameters(model):\\n\",\"    return sum(p.numel() for p in model.parameters() if p.requires_grad)\\n\",\"\\n\",\"\\n\",\"def makedirs(dirname):\\n\",\"    if not os.path.exists(dirname):\\n\",\"        os.makedirs(dirname)\\n\",\"\\n\",\"\\n\",\"def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):\\n\",\"    logger = logging.getLogger()\\n\",\"    if debug:\\n\",\"        level = logging.DEBUG\\n\",\"    else:\\n\",\"        level = logging.INFO\\n\",\"    logger.setLevel(level)\\n\",\"    if saving:\\n\",\"        info_file_handler = logging.FileHandler(logpath, mode=\\\"a\\\")\\n\",\"        info_file_handler.setLevel(level)\\n\",\"        logger.addHandler(info_file_handler)\\n\",\"    if displaying:\\n\",\"        console_handler = logging.StreamHandler()\\n\",\"        console_handler.setLevel(level)\\n\",\"        logger.addHandler(console_handler)\\n\",\"    logger.info(filepath)\\n\",\"    with open(filepath, \\\"r\\\") as f:\\n\",\"        logger.info(f.read())\\n\",\"\\n\",\"    for f in package_files:\\n\",\"        logger.info(f)\\n\",\"        with open(f, \\\"r\\\") as package_f:\\n\",\"            logger.info(package_f.read())\\n\",\"\\n\",\"    return logger\\n\"],\"execution_count\":20,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"colab\":{\"base_uri\":\"https://localhost:8080/\",\"height\":470},\"id\":\"42RIwJ37JKQr\",\"executionInfo\":{\"status\":\"error\",\"timestamp\":1617964221740,\"user_tz\":-60,\"elapsed\":825,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}},\"outputId\":\"076c12d7-0015-4846-a548-eeaec45004fd\"},\"source\":[\"if __name__ == '__main__':\\n\",\"    __file__ = '/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb'\\n\",\"    makedirs(args.save)\\n\",\"    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))\\n\",\"    logger.info(args)\\n\",\"\\n\",\"    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')\\n\",\"\\n\",\"    is_odenet = args.network == 'odenet'\\n\",\"\\n\",\"    if args.downsampling_method == 'conv':\\n\",\"        downsampling_layers = [\\n\",\"            nn.Conv2d(1, 64, 3, 1),\\n\",\"            norm(64),\\n\",\"            nn.ReLU(inplace=True),\\n\",\"            nn.Conv2d(64, 64, 4, 2, 1),\\n\",\"            norm(64),\\n\",\"            nn.ReLU(inplace=True),\\n\",\"            nn.Conv2d(64, 64, 4, 2, 1),\\n\",\"        ]\\n\",\"    elif args.downsampling_method == 'res':\\n\",\"        downsampling_layers = [\\n\",\"            nn.Conv2d(1, 64, 3, 1),\\n\",\"            ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\\n\",\"            ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\\n\",\"        ]\\n\",\"\\n\",\"    feature_layers = [ODEBlock(ODEfunc(64))] if is_odenet else [ResBlock(64, 64) for _ in range(6)]\\n\",\"    fc_layers = [norm(64), nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d((1, 1)), Flatten(), nn.Linear(64, 10)]\\n\",\"\\n\",\"    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)\\n\",\"\\n\",\"    logger.info(model)\\n\",\"    logger.info('Number of parameters: {}'.format(count_parameters(model)))\\n\",\"\\n\",\"    criterion = nn.CrossEntropyLoss().to(device)\\n\",\"\\n\",\"    train_loader, test_loader, train_eval_loader = get_mnist_loaders(\\n\",\"        args.data_aug, args.batch_size, args.test_batch_size\\n\",\"    )\\n\",\"\\n\",\"    data_gen = inf_generator(train_loader)\\n\",\"    batches_per_epoch = len(train_loader)\\n\",\"\\n\",\"    lr_fn = learning_rate_with_decay(\\n\",\"        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],\\n\",\"        decay_rates=[1, 0.1, 0.01, 0.001]\\n\",\"    )\\n\",\"\\n\",\"    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)\\n\",\"\\n\",\"    best_acc = 0\\n\",\"    batch_time_meter = RunningAverageMeter()\\n\",\"    f_nfe_meter = RunningAverageMeter()\\n\",\"    b_nfe_meter = RunningAverageMeter()\\n\",\"    end = time.time()\\n\",\"    \\n\",\"    epoch_arr = []\\n\",\"    time_val_arr = []\\n\",\"    time_avg_arr = []\\n\",\"    nfe_f_arr = []\\n\",\"    nfe_b_arr = []\\n\",\"    train_acc_arr = []\\n\",\"    test_acc_arr = []\\n\",\"\\n\",\"    for itr in range(args.nepochs * batches_per_epoch):\\n\",\"\\n\",\"        for param_group in optimizer.param_groups:\\n\",\"            param_group['lr'] = lr_fn(itr)\\n\",\"\\n\",\"        optimizer.zero_grad()\\n\",\"        x, y = data_gen.__next__()\\n\",\"        x = x.to(device)\\n\",\"        y = y.to(device)\\n\",\"        logits = model(x)\\n\",\"        loss = criterion(logits, y)\\n\",\"\\n\",\"        if is_odenet:\\n\",\"            nfe_forward = feature_layers[0].nfe\\n\",\"            feature_layers[0].nfe = 0\\n\",\"\\n\",\"        loss.backward()\\n\",\"        optimizer.step()\\n\",\"\\n\",\"        if is_odenet:\\n\",\"            nfe_backward = feature_layers[0].nfe\\n\",\"            feature_layers[0].nfe = 0\\n\",\"\\n\",\"        batch_time_meter.update(time.time() - end)\\n\",\"        if is_odenet:\\n\",\"            f_nfe_meter.update(nfe_forward)\\n\",\"            b_nfe_meter.update(nfe_backward)\\n\",\"        end = time.time()\\n\",\"\\n\",\"        if itr % batches_per_epoch == 0:\\n\",\"            with torch.no_grad():\\n\",\"                train_acc = accuracy(model, train_eval_loader)\\n\",\"                val_acc = accuracy(model, test_loader)\\n\",\"                if val_acc > best_acc:\\n\",\"                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))\\n\",\"                    best_acc = val_acc\\n\",\"                logger.info(\\n\",\"                    \\\"Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | \\\"\\n\",\"                    \\\"Train Acc {:.4f} | Test Acc {:.4f}\\\".format(\\n\",\"                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,\\n\",\"                        b_nfe_meter.avg, train_acc, val_acc\\n\",\"                    )\\n\",\"                )\\n\",\"                epoch_arr += [itr // batches_per_epoch]\\n\",\"                time_val_arr += [batch_time_meter.val]\\n\",\"                time_avg_arr += [batch_time_meter.avg]\\n\",\"                nfe_f_arr += [f_nfe_meter.avg]\\n\",\"                nfe_b_arr += [b_nfe_meter.avg]\\n\",\"                train_acc_arr += [train_acc]\\n\",\"                test_acc_arr += [val_acc]\\n\",\"                    \\n\",\"    epoch_arr = np.asarray(epoch_arr)\\n\",\"    time_val_arr = np.asarray(time_val_arr)\\n\",\"    time_avg_arr = np.asarray(time_avg_arr)\\n\",\"    nfe_f_arr = np.asarray(nfe_f_arr)\\n\",\"    nfe_b_arr = np.asarray(nfe_b_arr)\\n\",\"    train_acc_arr = np.asarray(train_acc_arr)\\n\",\"    test_acc_arr = np.asarray(test_acc_arr)\\n\",\"    \\n\",\"    np.save(os.path.join(args.save, 'epoch_arr.npy'), epoch_arr)\\n\",\"    np.save(os.path.join(args.save, 'time_val_arr.npy'), time_val_arr)\\n\",\"    np.save(os.path.join(args.save, 'time_avg_arr.npy'), time_avg_arr)\\n\",\"    np.save(os.path.join(args.save, 'nfe_f_arr.npy'), nfe_f_arr)\\n\",\"    np.save(os.path.join(args.save, 'nfe_b_arr.npy'), nfe_b_arr)\\n\",\"    np.save(os.path.join(args.save, 'train_acc_arr.npy'), train_acc_arr)\\n\",\"    np.save(os.path.join(args.save, 'test_acc_arr.npy'), test_acc_arr)\"],\"execution_count\":50,\"outputs\":[{\"output_type\":\"stream\",\"text\":[\"/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\\n\",\"/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\\n\",\"/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\\n\",\"/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\\n\",\"/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\\n\"],\"name\":\"stderr\"},{\"output_type\":\"error\",\"ename\":\"FileNotFoundError\",\"evalue\":\"ignored\",\"traceback\":[\"\\u001b[0;31m---------------------------------------------------------------------------\\u001b[0m\",\"\\u001b[0;31mFileNotFoundError\\u001b[0m                         Traceback (most recent call last)\",\"\\u001b[0;32m/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\\u001b[0m in \\u001b[0;36m<module>\\u001b[0;34m()\\u001b[0m\\n\\u001b[1;32m      2\\u001b[0m     \\u001b[0m__file__\\u001b[0m \\u001b[0;34m=\\u001b[0m \\u001b[0;34m'/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb'\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m      3\\u001b[0m     \\u001b[0mmakedirs\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0margs\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0msave\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m----> 4\\u001b[0;31m     \\u001b[0mlogger\\u001b[0m \\u001b[0;34m=\\u001b[0m \\u001b[0mget_logger\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mlogpath\\u001b[0m\\u001b[0;34m=\\u001b[0m\\u001b[0mos\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mpath\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mjoin\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0margs\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0msave\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0;34m'logs'\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0mfilepath\\u001b[0m\\u001b[0;34m=\\u001b[0m\\u001b[0mos\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mpath\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mabspath\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0m__file__\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0m\\u001b[1;32m      5\\u001b[0m     \\u001b[0mlogger\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0minfo\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0margs\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m      6\\u001b[0m \\u001b[0;34m\\u001b[0m\\u001b[0m\\n\",\"\\u001b[0;32m/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\\u001b[0m in \\u001b[0;36mget_logger\\u001b[0;34m(logpath, filepath, package_files, displaying, saving, debug)\\u001b[0m\\n\\u001b[1;32m     99\\u001b[0m         \\u001b[0mlogger\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0maddHandler\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mconsole_handler\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m    100\\u001b[0m     \\u001b[0mlogger\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0minfo\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mfilepath\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m--> 101\\u001b[0;31m     \\u001b[0;32mwith\\u001b[0m \\u001b[0mopen\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mfilepath\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0;34m\\\"r\\\"\\u001b[0m\\u001b[0;34m)\\u001b[0m \\u001b[0;32mas\\u001b[0m \\u001b[0mf\\u001b[0m\\u001b[0;34m:\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0m\\u001b[1;32m    102\\u001b[0m         \\u001b[0mlogger\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0minfo\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mf\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mread\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m    103\\u001b[0m \\u001b[0;34m\\u001b[0m\\u001b[0m\\n\",\"\\u001b[0;31mFileNotFoundError\\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb'\"]}]},{\"cell_type\":\"code\",\"metadata\":{\"colab\":{\"base_uri\":\"https://localhost:8080/\"},\"id\":\"g2k3GwQeJsNy\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617963867638,\"user_tz\":-60,\"elapsed\":420,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}},\"outputId\":\"78f13612-fb4b-4d78-a6f9-3c951d4848a1\"},\"source\":[\"a = os.path.abspath('')\\n\",\"print(type(a))\"],\"execution_count\":35,\"outputs\":[{\"output_type\":\"stream\",\"text\":[\"<class 'str'>\\n\"],\"name\":\"stdout\"}]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"_jhUzE0XKh_C\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617964280066,\"user_tz\":-60,\"elapsed\":1044,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"__file__ = '/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb'\\n\",\"makedirs(args.save)\"],\"execution_count\":52,\"outputs\":[]}]}\n","{\"nbformat\":4,\"nbformat_minor\":0,\"metadata\":{\"colab\":{\"name\":\"mnist_node.ipynb\",\"provenance\":[],\"collapsed_sections\":[],\"mount_file_id\":\"1sLUaZ_DEMKH30zwclvQKQ2FNZUI_mF3K\",\"authorship_tag\":\"ABX9TyM4RT5ovZ+PC2JziHsLNjTc\"},\"kernelspec\":{\"name\":\"python3\",\"display_name\":\"Python 3\"},\"language_info\":{\"name\":\"python\"}},\"cells\":[{\"cell_type\":\"code\",\"metadata\":{\"id\":\"sWmsXi5BIAtb\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617963280508,\"user_tz\":-60,\"elapsed\":4957,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"%%capture\\n\",\"%%bash\\n\",\"pip install torchdiffeq\"],\"execution_count\":1,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"H353RG8fIDd3\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617963632522,\"user_tz\":-60,\"elapsed\":546,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"import os\\n\",\"import argparse\\n\",\"import logging\\n\",\"import time\\n\",\"import numpy as np\\n\",\"import torch\\n\",\"import torch.nn as nn\\n\",\"from torch.utils.data import DataLoader\\n\",\"import torchvision.datasets as datasets\\n\",\"import torchvision.transforms as transforms\\n\",\"\\n\",\"\\n\",\"parser = argparse.ArgumentParser()\\n\",\"parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')\\n\",\"parser.add_argument('--tol', type=float, default=1e-3)\\n\",\"parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])\\n\",\"parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])\\n\",\"parser.add_argument('--nepochs', type=int, default=120)\\n\",\"parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])\\n\",\"parser.add_argument('--lr', type=float, default=0.1)\\n\",\"parser.add_argument('--batch_size', type=int, default=128)\\n\",\"parser.add_argument('--test_batch_size', type=int, default=1000)\\n\",\"\\n\",\"parser.add_argument('--save', type=str, default='./experiment_node1')\\n\",\"parser.add_argument('--gpu', type=int, default=0)\\n\",\"parser.add_argument('--debug', action='store_true')\\n\",\"args = parser.parse_args(args=[])\\n\",\"\\n\",\"if args.adjoint:\\n\",\"    from torchdiffeq import odeint_adjoint as odeint\\n\",\"else:\\n\",\"    from torchdiffeq import odeint\"],\"execution_count\":22,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"agpZtaKwIKnA\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617963497293,\"user_tz\":-60,\"elapsed\":412,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"def conv3x3(in_planes, out_planes, stride=1):\\n\",\"    \\\"\\\"\\\"3x3 convolution with padding\\\"\\\"\\\"\\n\",\"    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\\n\",\"\\n\",\"\\n\",\"def conv1x1(in_planes, out_planes, stride=1):\\n\",\"    \\\"\\\"\\\"1x1 convolution\\\"\\\"\\\"\\n\",\"    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\\n\",\"\\n\",\"\\n\",\"def norm(dim):\\n\",\"    return nn.GroupNorm(min(32, dim), dim)\\n\",\"\\n\",\"\\n\",\"class ResBlock(nn.Module):\\n\",\"    expansion = 1\\n\",\"\\n\",\"    def __init__(self, inplanes, planes, stride=1, downsample=None):\\n\",\"        super(ResBlock, self).__init__()\\n\",\"        self.norm1 = norm(inplanes)\\n\",\"        self.relu = nn.ReLU(inplace=True)\\n\",\"        self.downsample = downsample\\n\",\"        self.conv1 = conv3x3(inplanes, planes, stride)\\n\",\"        self.norm2 = norm(planes)\\n\",\"        self.conv2 = conv3x3(planes, planes)\\n\",\"\\n\",\"    def forward(self, x):\\n\",\"        shortcut = x\\n\",\"\\n\",\"        out = self.relu(self.norm1(x))\\n\",\"\\n\",\"        if self.downsample is not None:\\n\",\"            shortcut = self.downsample(out)\\n\",\"\\n\",\"        out = self.conv1(out)\\n\",\"        out = self.norm2(out)\\n\",\"        out = self.relu(out)\\n\",\"        out = self.conv2(out)\\n\",\"\\n\",\"        return out + shortcut\\n\",\"\\n\",\"\\n\",\"class ConcatConv2d(nn.Module):\\n\",\"\\n\",\"    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):\\n\",\"        super(ConcatConv2d, self).__init__()\\n\",\"        module = nn.ConvTranspose2d if transpose else nn.Conv2d\\n\",\"        self._layer = module(\\n\",\"            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,\\n\",\"            bias=bias\\n\",\"        )\\n\",\"\\n\",\"    def forward(self, t, x):\\n\",\"        tt = torch.ones_like(x[:, :1, :, :]) * t\\n\",\"        ttx = torch.cat([tt, x], 1)\\n\",\"        return self._layer(ttx)\"],\"execution_count\":15,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"1-rlM_3sI-1k\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617963513110,\"user_tz\":-60,\"elapsed\":974,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"class ODEfunc(nn.Module):\\n\",\"\\n\",\"    def __init__(self, dim):\\n\",\"        super(ODEfunc, self).__init__()\\n\",\"        self.norm1 = norm(dim)\\n\",\"        self.relu = nn.ReLU(inplace=True)\\n\",\"        self.conv1 = ConcatConv2d(dim, dim, 3, 1, 1)\\n\",\"        self.norm2 = norm(dim)\\n\",\"        self.conv2 = ConcatConv2d(dim, dim, 3, 1, 1)\\n\",\"        self.norm3 = norm(dim)\\n\",\"        self.nfe = 0\\n\",\"\\n\",\"    def forward(self, t, x):\\n\",\"        self.nfe += 1\\n\",\"        out = self.norm1(x)\\n\",\"        out = self.relu(out)\\n\",\"        out = self.conv1(t, out)\\n\",\"        out = self.norm2(out)\\n\",\"        out = self.relu(out)\\n\",\"        out = self.conv2(t, out)\\n\",\"        out = self.norm3(out)\\n\",\"        return out\\n\",\"\\n\",\"\\n\",\"class ODEBlock(nn.Module):\\n\",\"\\n\",\"    def __init__(self, odefunc):\\n\",\"        super(ODEBlock, self).__init__()\\n\",\"        self.odefunc = odefunc\\n\",\"        self.integration_time = torch.tensor([0, 1]).float()\\n\",\"\\n\",\"    def forward(self, x):\\n\",\"        self.integration_time = self.integration_time.type_as(x)\\n\",\"        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)\\n\",\"        return out[1]\\n\",\"\\n\",\"    @property\\n\",\"    def nfe(self):\\n\",\"        return self.odefunc.nfe\\n\",\"\\n\",\"    @nfe.setter\\n\",\"    def nfe(self, value):\\n\",\"        self.odefunc.nfe = value\"],\"execution_count\":16,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"xhFiCzNaJC0x\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617963529496,\"user_tz\":-60,\"elapsed\":991,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"class Flatten(nn.Module):\\n\",\"\\n\",\"    def __init__(self):\\n\",\"        super(Flatten, self).__init__()\\n\",\"\\n\",\"    def forward(self, x):\\n\",\"        shape = torch.prod(torch.tensor(x.shape[1:])).item()\\n\",\"        return x.view(-1, shape)\\n\",\"\\n\",\"\\n\",\"class RunningAverageMeter(object):\\n\",\"    \\\"\\\"\\\"Computes and stores the average and current value\\\"\\\"\\\"\\n\",\"\\n\",\"    def __init__(self, momentum=0.99):\\n\",\"        self.momentum = momentum\\n\",\"        self.reset()\\n\",\"\\n\",\"    def reset(self):\\n\",\"        self.val = None\\n\",\"        self.avg = 0\\n\",\"\\n\",\"    def update(self, val):\\n\",\"        if self.val is None:\\n\",\"            self.avg = val\\n\",\"        else:\\n\",\"            self.avg = self.avg * self.momentum + val * (1 - self.momentum)\\n\",\"        self.val = val\"],\"execution_count\":17,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"ltcgl1RIJG7U\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617963610116,\"user_tz\":-60,\"elapsed\":397,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"def get_mnist_loaders(data_aug=False, batch_size=128, test_batch_size=1000, perc=1.0):\\n\",\"    if data_aug:\\n\",\"        transform_train = transforms.Compose([\\n\",\"            transforms.RandomCrop(28, padding=4),\\n\",\"            transforms.ToTensor(),\\n\",\"        ])\\n\",\"    else:\\n\",\"        transform_train = transforms.Compose([\\n\",\"            transforms.ToTensor(),\\n\",\"        ])\\n\",\"\\n\",\"    transform_test = transforms.Compose([\\n\",\"        transforms.ToTensor(),\\n\",\"    ])\\n\",\"\\n\",\"    train_loader = DataLoader(\\n\",\"        datasets.MNIST(root='.data/mnist', train=True, download=True, transform=transform_train), batch_size=batch_size,\\n\",\"        shuffle=True, num_workers=2, drop_last=True\\n\",\"    )\\n\",\"\\n\",\"    train_eval_loader = DataLoader(\\n\",\"        datasets.MNIST(root='.data/mnist', train=True, download=True, transform=transform_test),\\n\",\"        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True\\n\",\"    )\\n\",\"\\n\",\"    test_loader = DataLoader(\\n\",\"        datasets.MNIST(root='.data/mnist', train=False, download=True, transform=transform_test),\\n\",\"        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True\\n\",\"    )\\n\",\"\\n\",\"    return train_loader, test_loader, train_eval_loader\\n\",\"\\n\",\"\\n\",\"def inf_generator(iterable):\\n\",\"    \\\"\\\"\\\"Allows training with DataLoaders in a single infinite loop:\\n\",\"        for i, (x, y) in enumerate(inf_generator(train_loader)):\\n\",\"    \\\"\\\"\\\"\\n\",\"    iterator = iterable.__iter__()\\n\",\"    while True:\\n\",\"        try:\\n\",\"            yield iterator.__next__()\\n\",\"        except StopIteration:\\n\",\"            iterator = iterable.__iter__()\\n\",\"\\n\",\"\\n\",\"def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):\\n\",\"    initial_learning_rate = args.lr * batch_size / batch_denom\\n\",\"\\n\",\"    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]\\n\",\"    vals = [initial_learning_rate * decay for decay in decay_rates]\\n\",\"\\n\",\"    def learning_rate_fn(itr):\\n\",\"        lt = [itr < b for b in boundaries] + [True]\\n\",\"        i = np.argmax(lt)\\n\",\"        return vals[i]\\n\",\"\\n\",\"    return learning_rate_fn\\n\",\"\\n\",\"\\n\",\"def one_hot(x, K):\\n\",\"    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)\\n\",\"\\n\",\"\\n\",\"def accuracy(model, dataset_loader):\\n\",\"    total_correct = 0\\n\",\"    for x, y in dataset_loader:\\n\",\"        x = x.to(device)\\n\",\"        y = one_hot(np.array(y.numpy()), 10)\\n\",\"\\n\",\"        target_class = np.argmax(y, axis=1)\\n\",\"        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)\\n\",\"        total_correct += np.sum(predicted_class == target_class)\\n\",\"    return total_correct / len(dataset_loader.dataset)\\n\",\"\\n\",\"\\n\",\"def count_parameters(model):\\n\",\"    return sum(p.numel() for p in model.parameters() if p.requires_grad)\\n\",\"\\n\",\"\\n\",\"def makedirs(dirname):\\n\",\"    if not os.path.exists(dirname):\\n\",\"        os.makedirs(dirname)\\n\",\"\\n\",\"\\n\",\"def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):\\n\",\"    logger = logging.getLogger()\\n\",\"    if debug:\\n\",\"        level = logging.DEBUG\\n\",\"    else:\\n\",\"        level = logging.INFO\\n\",\"    logger.setLevel(level)\\n\",\"    if saving:\\n\",\"        info_file_handler = logging.FileHandler(logpath, mode=\\\"a\\\")\\n\",\"        info_file_handler.setLevel(level)\\n\",\"        logger.addHandler(info_file_handler)\\n\",\"    if displaying:\\n\",\"        console_handler = logging.StreamHandler()\\n\",\"        console_handler.setLevel(level)\\n\",\"        logger.addHandler(console_handler)\\n\",\"    logger.info(filepath)\\n\",\"    with open(filepath, \\\"r\\\") as f:\\n\",\"        logger.info(f.read())\\n\",\"\\n\",\"    for f in package_files:\\n\",\"        logger.info(f)\\n\",\"        with open(f, \\\"r\\\") as package_f:\\n\",\"            logger.info(package_f.read())\\n\",\"\\n\",\"    return logger\\n\"],\"execution_count\":20,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"colab\":{\"base_uri\":\"https://localhost:8080/\",\"height\":470},\"id\":\"42RIwJ37JKQr\",\"executionInfo\":{\"status\":\"error\",\"timestamp\":1617964221740,\"user_tz\":-60,\"elapsed\":825,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}},\"outputId\":\"076c12d7-0015-4846-a548-eeaec45004fd\"},\"source\":[\"if __name__ == '__main__':\\n\",\"    __file__ = '/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb'\\n\",\"    makedirs(args.save)\\n\",\"    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))\\n\",\"    logger.info(args)\\n\",\"\\n\",\"    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')\\n\",\"\\n\",\"    is_odenet = args.network == 'odenet'\\n\",\"\\n\",\"    if args.downsampling_method == 'conv':\\n\",\"        downsampling_layers = [\\n\",\"            nn.Conv2d(1, 64, 3, 1),\\n\",\"            norm(64),\\n\",\"            nn.ReLU(inplace=True),\\n\",\"            nn.Conv2d(64, 64, 4, 2, 1),\\n\",\"            norm(64),\\n\",\"            nn.ReLU(inplace=True),\\n\",\"            nn.Conv2d(64, 64, 4, 2, 1),\\n\",\"        ]\\n\",\"    elif args.downsampling_method == 'res':\\n\",\"        downsampling_layers = [\\n\",\"            nn.Conv2d(1, 64, 3, 1),\\n\",\"            ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\\n\",\"            ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\\n\",\"        ]\\n\",\"\\n\",\"    feature_layers = [ODEBlock(ODEfunc(64))] if is_odenet else [ResBlock(64, 64) for _ in range(6)]\\n\",\"    fc_layers = [norm(64), nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d((1, 1)), Flatten(), nn.Linear(64, 10)]\\n\",\"\\n\",\"    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)\\n\",\"\\n\",\"    logger.info(model)\\n\",\"    logger.info('Number of parameters: {}'.format(count_parameters(model)))\\n\",\"\\n\",\"    criterion = nn.CrossEntropyLoss().to(device)\\n\",\"\\n\",\"    train_loader, test_loader, train_eval_loader = get_mnist_loaders(\\n\",\"        args.data_aug, args.batch_size, args.test_batch_size\\n\",\"    )\\n\",\"\\n\",\"    data_gen = inf_generator(train_loader)\\n\",\"    batches_per_epoch = len(train_loader)\\n\",\"\\n\",\"    lr_fn = learning_rate_with_decay(\\n\",\"        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],\\n\",\"        decay_rates=[1, 0.1, 0.01, 0.001]\\n\",\"    )\\n\",\"\\n\",\"    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)\\n\",\"\\n\",\"    best_acc = 0\\n\",\"    batch_time_meter = RunningAverageMeter()\\n\",\"    f_nfe_meter = RunningAverageMeter()\\n\",\"    b_nfe_meter = RunningAverageMeter()\\n\",\"    end = time.time()\\n\",\"    \\n\",\"    epoch_arr = []\\n\",\"    time_val_arr = []\\n\",\"    time_avg_arr = []\\n\",\"    nfe_f_arr = []\\n\",\"    nfe_b_arr = []\\n\",\"    train_acc_arr = []\\n\",\"    test_acc_arr = []\\n\",\"\\n\",\"    for itr in range(args.nepochs * batches_per_epoch):\\n\",\"\\n\",\"        for param_group in optimizer.param_groups:\\n\",\"            param_group['lr'] = lr_fn(itr)\\n\",\"\\n\",\"        optimizer.zero_grad()\\n\",\"        x, y = data_gen.__next__()\\n\",\"        x = x.to(device)\\n\",\"        y = y.to(device)\\n\",\"        logits = model(x)\\n\",\"        loss = criterion(logits, y)\\n\",\"\\n\",\"        if is_odenet:\\n\",\"            nfe_forward = feature_layers[0].nfe\\n\",\"            feature_layers[0].nfe = 0\\n\",\"\\n\",\"        loss.backward()\\n\",\"        optimizer.step()\\n\",\"\\n\",\"        if is_odenet:\\n\",\"            nfe_backward = feature_layers[0].nfe\\n\",\"            feature_layers[0].nfe = 0\\n\",\"\\n\",\"        batch_time_meter.update(time.time() - end)\\n\",\"        if is_odenet:\\n\",\"            f_nfe_meter.update(nfe_forward)\\n\",\"            b_nfe_meter.update(nfe_backward)\\n\",\"        end = time.time()\\n\",\"\\n\",\"        if itr % batches_per_epoch == 0:\\n\",\"            with torch.no_grad():\\n\",\"                train_acc = accuracy(model, train_eval_loader)\\n\",\"                val_acc = accuracy(model, test_loader)\\n\",\"                if val_acc > best_acc:\\n\",\"                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))\\n\",\"                    best_acc = val_acc\\n\",\"                logger.info(\\n\",\"                    \\\"Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | \\\"\\n\",\"                    \\\"Train Acc {:.4f} | Test Acc {:.4f}\\\".format(\\n\",\"                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,\\n\",\"                        b_nfe_meter.avg, train_acc, val_acc\\n\",\"                    )\\n\",\"                )\\n\",\"                epoch_arr += [itr // batches_per_epoch]\\n\",\"                time_val_arr += [batch_time_meter.val]\\n\",\"                time_avg_arr += [batch_time_meter.avg]\\n\",\"                nfe_f_arr += [f_nfe_meter.avg]\\n\",\"                nfe_b_arr += [b_nfe_meter.avg]\\n\",\"                train_acc_arr += [train_acc]\\n\",\"                test_acc_arr += [val_acc]\\n\",\"                    \\n\",\"    epoch_arr = np.asarray(epoch_arr)\\n\",\"    time_val_arr = np.asarray(time_val_arr)\\n\",\"    time_avg_arr = np.asarray(time_avg_arr)\\n\",\"    nfe_f_arr = np.asarray(nfe_f_arr)\\n\",\"    nfe_b_arr = np.asarray(nfe_b_arr)\\n\",\"    train_acc_arr = np.asarray(train_acc_arr)\\n\",\"    test_acc_arr = np.asarray(test_acc_arr)\\n\",\"    \\n\",\"    np.save(os.path.join(args.save, 'epoch_arr.npy'), epoch_arr)\\n\",\"    np.save(os.path.join(args.save, 'time_val_arr.npy'), time_val_arr)\\n\",\"    np.save(os.path.join(args.save, 'time_avg_arr.npy'), time_avg_arr)\\n\",\"    np.save(os.path.join(args.save, 'nfe_f_arr.npy'), nfe_f_arr)\\n\",\"    np.save(os.path.join(args.save, 'nfe_b_arr.npy'), nfe_b_arr)\\n\",\"    np.save(os.path.join(args.save, 'train_acc_arr.npy'), train_acc_arr)\\n\",\"    np.save(os.path.join(args.save, 'test_acc_arr.npy'), test_acc_arr)\"],\"execution_count\":50,\"outputs\":[{\"output_type\":\"stream\",\"text\":[\"/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\\n\",\"/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\\n\",\"/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\\n\",\"/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\\n\",\"/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\\n\"],\"name\":\"stderr\"},{\"output_type\":\"error\",\"ename\":\"FileNotFoundError\",\"evalue\":\"ignored\",\"traceback\":[\"\\u001b[0;31m---------------------------------------------------------------------------\\u001b[0m\",\"\\u001b[0;31mFileNotFoundError\\u001b[0m                         Traceback (most recent call last)\",\"\\u001b[0;32m/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\\u001b[0m in \\u001b[0;36m<module>\\u001b[0;34m()\\u001b[0m\\n\\u001b[1;32m      2\\u001b[0m     \\u001b[0m__file__\\u001b[0m \\u001b[0;34m=\\u001b[0m \\u001b[0;34m'/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb'\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m      3\\u001b[0m     \\u001b[0mmakedirs\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0margs\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0msave\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m----> 4\\u001b[0;31m     \\u001b[0mlogger\\u001b[0m \\u001b[0;34m=\\u001b[0m \\u001b[0mget_logger\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mlogpath\\u001b[0m\\u001b[0;34m=\\u001b[0m\\u001b[0mos\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mpath\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mjoin\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0margs\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0msave\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0;34m'logs'\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0mfilepath\\u001b[0m\\u001b[0;34m=\\u001b[0m\\u001b[0mos\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mpath\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mabspath\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0m__file__\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0m\\u001b[1;32m      5\\u001b[0m     \\u001b[0mlogger\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0minfo\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0margs\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m      6\\u001b[0m \\u001b[0;34m\\u001b[0m\\u001b[0m\\n\",\"\\u001b[0;32m/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\\u001b[0m in \\u001b[0;36mget_logger\\u001b[0;34m(logpath, filepath, package_files, displaying, saving, debug)\\u001b[0m\\n\\u001b[1;32m     99\\u001b[0m         \\u001b[0mlogger\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0maddHandler\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mconsole_handler\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m    100\\u001b[0m     \\u001b[0mlogger\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0minfo\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mfilepath\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m--> 101\\u001b[0;31m     \\u001b[0;32mwith\\u001b[0m \\u001b[0mopen\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mfilepath\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0;34m\\\"r\\\"\\u001b[0m\\u001b[0;34m)\\u001b[0m \\u001b[0;32mas\\u001b[0m \\u001b[0mf\\u001b[0m\\u001b[0;34m:\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0m\\u001b[1;32m    102\\u001b[0m         \\u001b[0mlogger\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0minfo\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mf\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mread\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m    103\\u001b[0m \\u001b[0;34m\\u001b[0m\\u001b[0m\\n\",\"\\u001b[0;31mFileNotFoundError\\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb'\"]}]},{\"cell_type\":\"code\",\"metadata\":{\"colab\":{\"base_uri\":\"https://localhost:8080/\"},\"id\":\"g2k3GwQeJsNy\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617963867638,\"user_tz\":-60,\"elapsed\":420,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}},\"outputId\":\"78f13612-fb4b-4d78-a6f9-3c951d4848a1\"},\"source\":[\"a = os.path.abspath('')\\n\",\"print(type(a))\"],\"execution_count\":35,\"outputs\":[{\"output_type\":\"stream\",\"text\":[\"<class 'str'>\\n\"],\"name\":\"stdout\"}]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"_jhUzE0XKh_C\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617964280066,\"user_tz\":-60,\"elapsed\":1044,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"__file__ = '/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb'\\n\",\"makedirs(args.save)\"],\"execution_count\":52,\"outputs\":[]}]}\n","{\"nbformat\":4,\"nbformat_minor\":0,\"metadata\":{\"colab\":{\"name\":\"mnist_node.ipynb\",\"provenance\":[],\"collapsed_sections\":[],\"mount_file_id\":\"1sLUaZ_DEMKH30zwclvQKQ2FNZUI_mF3K\",\"authorship_tag\":\"ABX9TyM4RT5ovZ+PC2JziHsLNjTc\"},\"kernelspec\":{\"name\":\"python3\",\"display_name\":\"Python 3\"},\"language_info\":{\"name\":\"python\"}},\"cells\":[{\"cell_type\":\"code\",\"metadata\":{\"id\":\"sWmsXi5BIAtb\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617963280508,\"user_tz\":-60,\"elapsed\":4957,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"%%capture\\n\",\"%%bash\\n\",\"pip install torchdiffeq\"],\"execution_count\":1,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"H353RG8fIDd3\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617963632522,\"user_tz\":-60,\"elapsed\":546,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"import os\\n\",\"import argparse\\n\",\"import logging\\n\",\"import time\\n\",\"import numpy as np\\n\",\"import torch\\n\",\"import torch.nn as nn\\n\",\"from torch.utils.data import DataLoader\\n\",\"import torchvision.datasets as datasets\\n\",\"import torchvision.transforms as transforms\\n\",\"\\n\",\"\\n\",\"parser = argparse.ArgumentParser()\\n\",\"parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')\\n\",\"parser.add_argument('--tol', type=float, default=1e-3)\\n\",\"parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])\\n\",\"parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])\\n\",\"parser.add_argument('--nepochs', type=int, default=120)\\n\",\"parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])\\n\",\"parser.add_argument('--lr', type=float, default=0.1)\\n\",\"parser.add_argument('--batch_size', type=int, default=128)\\n\",\"parser.add_argument('--test_batch_size', type=int, default=1000)\\n\",\"\\n\",\"parser.add_argument('--save', type=str, default='./experiment_node1')\\n\",\"parser.add_argument('--gpu', type=int, default=0)\\n\",\"parser.add_argument('--debug', action='store_true')\\n\",\"args = parser.parse_args(args=[])\\n\",\"\\n\",\"if args.adjoint:\\n\",\"    from torchdiffeq import odeint_adjoint as odeint\\n\",\"else:\\n\",\"    from torchdiffeq import odeint\"],\"execution_count\":22,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"agpZtaKwIKnA\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617963497293,\"user_tz\":-60,\"elapsed\":412,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"def conv3x3(in_planes, out_planes, stride=1):\\n\",\"    \\\"\\\"\\\"3x3 convolution with padding\\\"\\\"\\\"\\n\",\"    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\\n\",\"\\n\",\"\\n\",\"def conv1x1(in_planes, out_planes, stride=1):\\n\",\"    \\\"\\\"\\\"1x1 convolution\\\"\\\"\\\"\\n\",\"    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\\n\",\"\\n\",\"\\n\",\"def norm(dim):\\n\",\"    return nn.GroupNorm(min(32, dim), dim)\\n\",\"\\n\",\"\\n\",\"class ResBlock(nn.Module):\\n\",\"    expansion = 1\\n\",\"\\n\",\"    def __init__(self, inplanes, planes, stride=1, downsample=None):\\n\",\"        super(ResBlock, self).__init__()\\n\",\"        self.norm1 = norm(inplanes)\\n\",\"        self.relu = nn.ReLU(inplace=True)\\n\",\"        self.downsample = downsample\\n\",\"        self.conv1 = conv3x3(inplanes, planes, stride)\\n\",\"        self.norm2 = norm(planes)\\n\",\"        self.conv2 = conv3x3(planes, planes)\\n\",\"\\n\",\"    def forward(self, x):\\n\",\"        shortcut = x\\n\",\"\\n\",\"        out = self.relu(self.norm1(x))\\n\",\"\\n\",\"        if self.downsample is not None:\\n\",\"            shortcut = self.downsample(out)\\n\",\"\\n\",\"        out = self.conv1(out)\\n\",\"        out = self.norm2(out)\\n\",\"        out = self.relu(out)\\n\",\"        out = self.conv2(out)\\n\",\"\\n\",\"        return out + shortcut\\n\",\"\\n\",\"\\n\",\"class ConcatConv2d(nn.Module):\\n\",\"\\n\",\"    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):\\n\",\"        super(ConcatConv2d, self).__init__()\\n\",\"        module = nn.ConvTranspose2d if transpose else nn.Conv2d\\n\",\"        self._layer = module(\\n\",\"            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,\\n\",\"            bias=bias\\n\",\"        )\\n\",\"\\n\",\"    def forward(self, t, x):\\n\",\"        tt = torch.ones_like(x[:, :1, :, :]) * t\\n\",\"        ttx = torch.cat([tt, x], 1)\\n\",\"        return self._layer(ttx)\"],\"execution_count\":15,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"1-rlM_3sI-1k\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617963513110,\"user_tz\":-60,\"elapsed\":974,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"class ODEfunc(nn.Module):\\n\",\"\\n\",\"    def __init__(self, dim):\\n\",\"        super(ODEfunc, self).__init__()\\n\",\"        self.norm1 = norm(dim)\\n\",\"        self.relu = nn.ReLU(inplace=True)\\n\",\"        self.conv1 = ConcatConv2d(dim, dim, 3, 1, 1)\\n\",\"        self.norm2 = norm(dim)\\n\",\"        self.conv2 = ConcatConv2d(dim, dim, 3, 1, 1)\\n\",\"        self.norm3 = norm(dim)\\n\",\"        self.nfe = 0\\n\",\"\\n\",\"    def forward(self, t, x):\\n\",\"        self.nfe += 1\\n\",\"        out = self.norm1(x)\\n\",\"        out = self.relu(out)\\n\",\"        out = self.conv1(t, out)\\n\",\"        out = self.norm2(out)\\n\",\"        out = self.relu(out)\\n\",\"        out = self.conv2(t, out)\\n\",\"        out = self.norm3(out)\\n\",\"        return out\\n\",\"\\n\",\"\\n\",\"class ODEBlock(nn.Module):\\n\",\"\\n\",\"    def __init__(self, odefunc):\\n\",\"        super(ODEBlock, self).__init__()\\n\",\"        self.odefunc = odefunc\\n\",\"        self.integration_time = torch.tensor([0, 1]).float()\\n\",\"\\n\",\"    def forward(self, x):\\n\",\"        self.integration_time = self.integration_time.type_as(x)\\n\",\"        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)\\n\",\"        return out[1]\\n\",\"\\n\",\"    @property\\n\",\"    def nfe(self):\\n\",\"        return self.odefunc.nfe\\n\",\"\\n\",\"    @nfe.setter\\n\",\"    def nfe(self, value):\\n\",\"        self.odefunc.nfe = value\"],\"execution_count\":16,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"xhFiCzNaJC0x\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617963529496,\"user_tz\":-60,\"elapsed\":991,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"class Flatten(nn.Module):\\n\",\"\\n\",\"    def __init__(self):\\n\",\"        super(Flatten, self).__init__()\\n\",\"\\n\",\"    def forward(self, x):\\n\",\"        shape = torch.prod(torch.tensor(x.shape[1:])).item()\\n\",\"        return x.view(-1, shape)\\n\",\"\\n\",\"\\n\",\"class RunningAverageMeter(object):\\n\",\"    \\\"\\\"\\\"Computes and stores the average and current value\\\"\\\"\\\"\\n\",\"\\n\",\"    def __init__(self, momentum=0.99):\\n\",\"        self.momentum = momentum\\n\",\"        self.reset()\\n\",\"\\n\",\"    def reset(self):\\n\",\"        self.val = None\\n\",\"        self.avg = 0\\n\",\"\\n\",\"    def update(self, val):\\n\",\"        if self.val is None:\\n\",\"            self.avg = val\\n\",\"        else:\\n\",\"            self.avg = self.avg * self.momentum + val * (1 - self.momentum)\\n\",\"        self.val = val\"],\"execution_count\":17,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"ltcgl1RIJG7U\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617963610116,\"user_tz\":-60,\"elapsed\":397,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"def get_mnist_loaders(data_aug=False, batch_size=128, test_batch_size=1000, perc=1.0):\\n\",\"    if data_aug:\\n\",\"        transform_train = transforms.Compose([\\n\",\"            transforms.RandomCrop(28, padding=4),\\n\",\"            transforms.ToTensor(),\\n\",\"        ])\\n\",\"    else:\\n\",\"        transform_train = transforms.Compose([\\n\",\"            transforms.ToTensor(),\\n\",\"        ])\\n\",\"\\n\",\"    transform_test = transforms.Compose([\\n\",\"        transforms.ToTensor(),\\n\",\"    ])\\n\",\"\\n\",\"    train_loader = DataLoader(\\n\",\"        datasets.MNIST(root='.data/mnist', train=True, download=True, transform=transform_train), batch_size=batch_size,\\n\",\"        shuffle=True, num_workers=2, drop_last=True\\n\",\"    )\\n\",\"\\n\",\"    train_eval_loader = DataLoader(\\n\",\"        datasets.MNIST(root='.data/mnist', train=True, download=True, transform=transform_test),\\n\",\"        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True\\n\",\"    )\\n\",\"\\n\",\"    test_loader = DataLoader(\\n\",\"        datasets.MNIST(root='.data/mnist', train=False, download=True, transform=transform_test),\\n\",\"        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True\\n\",\"    )\\n\",\"\\n\",\"    return train_loader, test_loader, train_eval_loader\\n\",\"\\n\",\"\\n\",\"def inf_generator(iterable):\\n\",\"    \\\"\\\"\\\"Allows training with DataLoaders in a single infinite loop:\\n\",\"        for i, (x, y) in enumerate(inf_generator(train_loader)):\\n\",\"    \\\"\\\"\\\"\\n\",\"    iterator = iterable.__iter__()\\n\",\"    while True:\\n\",\"        try:\\n\",\"            yield iterator.__next__()\\n\",\"        except StopIteration:\\n\",\"            iterator = iterable.__iter__()\\n\",\"\\n\",\"\\n\",\"def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):\\n\",\"    initial_learning_rate = args.lr * batch_size / batch_denom\\n\",\"\\n\",\"    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]\\n\",\"    vals = [initial_learning_rate * decay for decay in decay_rates]\\n\",\"\\n\",\"    def learning_rate_fn(itr):\\n\",\"        lt = [itr < b for b in boundaries] + [True]\\n\",\"        i = np.argmax(lt)\\n\",\"        return vals[i]\\n\",\"\\n\",\"    return learning_rate_fn\\n\",\"\\n\",\"\\n\",\"def one_hot(x, K):\\n\",\"    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)\\n\",\"\\n\",\"\\n\",\"def accuracy(model, dataset_loader):\\n\",\"    total_correct = 0\\n\",\"    for x, y in dataset_loader:\\n\",\"        x = x.to(device)\\n\",\"        y = one_hot(np.array(y.numpy()), 10)\\n\",\"\\n\",\"        target_class = np.argmax(y, axis=1)\\n\",\"        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)\\n\",\"        total_correct += np.sum(predicted_class == target_class)\\n\",\"    return total_correct / len(dataset_loader.dataset)\\n\",\"\\n\",\"\\n\",\"def count_parameters(model):\\n\",\"    return sum(p.numel() for p in model.parameters() if p.requires_grad)\\n\",\"\\n\",\"\\n\",\"def makedirs(dirname):\\n\",\"    if not os.path.exists(dirname):\\n\",\"        os.makedirs(dirname)\\n\",\"\\n\",\"\\n\",\"def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):\\n\",\"    logger = logging.getLogger()\\n\",\"    if debug:\\n\",\"        level = logging.DEBUG\\n\",\"    else:\\n\",\"        level = logging.INFO\\n\",\"    logger.setLevel(level)\\n\",\"    if saving:\\n\",\"        info_file_handler = logging.FileHandler(logpath, mode=\\\"a\\\")\\n\",\"        info_file_handler.setLevel(level)\\n\",\"        logger.addHandler(info_file_handler)\\n\",\"    if displaying:\\n\",\"        console_handler = logging.StreamHandler()\\n\",\"        console_handler.setLevel(level)\\n\",\"        logger.addHandler(console_handler)\\n\",\"    logger.info(filepath)\\n\",\"    with open(filepath, \\\"r\\\") as f:\\n\",\"        logger.info(f.read())\\n\",\"\\n\",\"    for f in package_files:\\n\",\"        logger.info(f)\\n\",\"        with open(f, \\\"r\\\") as package_f:\\n\",\"            logger.info(package_f.read())\\n\",\"\\n\",\"    return logger\\n\"],\"execution_count\":20,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"colab\":{\"base_uri\":\"https://localhost:8080/\",\"height\":470},\"id\":\"42RIwJ37JKQr\",\"executionInfo\":{\"status\":\"error\",\"timestamp\":1617964221740,\"user_tz\":-60,\"elapsed\":825,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}},\"outputId\":\"076c12d7-0015-4846-a548-eeaec45004fd\"},\"source\":[\"if __name__ == '__main__':\\n\",\"    __file__ = '/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb'\\n\",\"    makedirs(args.save)\\n\",\"    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))\\n\",\"    logger.info(args)\\n\",\"\\n\",\"    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')\\n\",\"\\n\",\"    is_odenet = args.network == 'odenet'\\n\",\"\\n\",\"    if args.downsampling_method == 'conv':\\n\",\"        downsampling_layers = [\\n\",\"            nn.Conv2d(1, 64, 3, 1),\\n\",\"            norm(64),\\n\",\"            nn.ReLU(inplace=True),\\n\",\"            nn.Conv2d(64, 64, 4, 2, 1),\\n\",\"            norm(64),\\n\",\"            nn.ReLU(inplace=True),\\n\",\"            nn.Conv2d(64, 64, 4, 2, 1),\\n\",\"        ]\\n\",\"    elif args.downsampling_method == 'res':\\n\",\"        downsampling_layers = [\\n\",\"            nn.Conv2d(1, 64, 3, 1),\\n\",\"            ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\\n\",\"            ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\\n\",\"        ]\\n\",\"\\n\",\"    feature_layers = [ODEBlock(ODEfunc(64))] if is_odenet else [ResBlock(64, 64) for _ in range(6)]\\n\",\"    fc_layers = [norm(64), nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d((1, 1)), Flatten(), nn.Linear(64, 10)]\\n\",\"\\n\",\"    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)\\n\",\"\\n\",\"    logger.info(model)\\n\",\"    logger.info('Number of parameters: {}'.format(count_parameters(model)))\\n\",\"\\n\",\"    criterion = nn.CrossEntropyLoss().to(device)\\n\",\"\\n\",\"    train_loader, test_loader, train_eval_loader = get_mnist_loaders(\\n\",\"        args.data_aug, args.batch_size, args.test_batch_size\\n\",\"    )\\n\",\"\\n\",\"    data_gen = inf_generator(train_loader)\\n\",\"    batches_per_epoch = len(train_loader)\\n\",\"\\n\",\"    lr_fn = learning_rate_with_decay(\\n\",\"        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],\\n\",\"        decay_rates=[1, 0.1, 0.01, 0.001]\\n\",\"    )\\n\",\"\\n\",\"    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)\\n\",\"\\n\",\"    best_acc = 0\\n\",\"    batch_time_meter = RunningAverageMeter()\\n\",\"    f_nfe_meter = RunningAverageMeter()\\n\",\"    b_nfe_meter = RunningAverageMeter()\\n\",\"    end = time.time()\\n\",\"    \\n\",\"    epoch_arr = []\\n\",\"    time_val_arr = []\\n\",\"    time_avg_arr = []\\n\",\"    nfe_f_arr = []\\n\",\"    nfe_b_arr = []\\n\",\"    train_acc_arr = []\\n\",\"    test_acc_arr = []\\n\",\"\\n\",\"    for itr in range(args.nepochs * batches_per_epoch):\\n\",\"\\n\",\"        for param_group in optimizer.param_groups:\\n\",\"            param_group['lr'] = lr_fn(itr)\\n\",\"\\n\",\"        optimizer.zero_grad()\\n\",\"        x, y = data_gen.__next__()\\n\",\"        x = x.to(device)\\n\",\"        y = y.to(device)\\n\",\"        logits = model(x)\\n\",\"        loss = criterion(logits, y)\\n\",\"\\n\",\"        if is_odenet:\\n\",\"            nfe_forward = feature_layers[0].nfe\\n\",\"            feature_layers[0].nfe = 0\\n\",\"\\n\",\"        loss.backward()\\n\",\"        optimizer.step()\\n\",\"\\n\",\"        if is_odenet:\\n\",\"            nfe_backward = feature_layers[0].nfe\\n\",\"            feature_layers[0].nfe = 0\\n\",\"\\n\",\"        batch_time_meter.update(time.time() - end)\\n\",\"        if is_odenet:\\n\",\"            f_nfe_meter.update(nfe_forward)\\n\",\"            b_nfe_meter.update(nfe_backward)\\n\",\"        end = time.time()\\n\",\"\\n\",\"        if itr % batches_per_epoch == 0:\\n\",\"            with torch.no_grad():\\n\",\"                train_acc = accuracy(model, train_eval_loader)\\n\",\"                val_acc = accuracy(model, test_loader)\\n\",\"                if val_acc > best_acc:\\n\",\"                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))\\n\",\"                    best_acc = val_acc\\n\",\"                logger.info(\\n\",\"                    \\\"Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | \\\"\\n\",\"                    \\\"Train Acc {:.4f} | Test Acc {:.4f}\\\".format(\\n\",\"                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,\\n\",\"                        b_nfe_meter.avg, train_acc, val_acc\\n\",\"                    )\\n\",\"                )\\n\",\"                epoch_arr += [itr // batches_per_epoch]\\n\",\"                time_val_arr += [batch_time_meter.val]\\n\",\"                time_avg_arr += [batch_time_meter.avg]\\n\",\"                nfe_f_arr += [f_nfe_meter.avg]\\n\",\"                nfe_b_arr += [b_nfe_meter.avg]\\n\",\"                train_acc_arr += [train_acc]\\n\",\"                test_acc_arr += [val_acc]\\n\",\"                    \\n\",\"    epoch_arr = np.asarray(epoch_arr)\\n\",\"    time_val_arr = np.asarray(time_val_arr)\\n\",\"    time_avg_arr = np.asarray(time_avg_arr)\\n\",\"    nfe_f_arr = np.asarray(nfe_f_arr)\\n\",\"    nfe_b_arr = np.asarray(nfe_b_arr)\\n\",\"    train_acc_arr = np.asarray(train_acc_arr)\\n\",\"    test_acc_arr = np.asarray(test_acc_arr)\\n\",\"    \\n\",\"    np.save(os.path.join(args.save, 'epoch_arr.npy'), epoch_arr)\\n\",\"    np.save(os.path.join(args.save, 'time_val_arr.npy'), time_val_arr)\\n\",\"    np.save(os.path.join(args.save, 'time_avg_arr.npy'), time_avg_arr)\\n\",\"    np.save(os.path.join(args.save, 'nfe_f_arr.npy'), nfe_f_arr)\\n\",\"    np.save(os.path.join(args.save, 'nfe_b_arr.npy'), nfe_b_arr)\\n\",\"    np.save(os.path.join(args.save, 'train_acc_arr.npy'), train_acc_arr)\\n\",\"    np.save(os.path.join(args.save, 'test_acc_arr.npy'), test_acc_arr)\"],\"execution_count\":50,\"outputs\":[{\"output_type\":\"stream\",\"text\":[\"/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\\n\",\"/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\\n\",\"/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\\n\",\"/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\\n\",\"/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\\n\"],\"name\":\"stderr\"},{\"output_type\":\"error\",\"ename\":\"FileNotFoundError\",\"evalue\":\"ignored\",\"traceback\":[\"\\u001b[0;31m---------------------------------------------------------------------------\\u001b[0m\",\"\\u001b[0;31mFileNotFoundError\\u001b[0m                         Traceback (most recent call last)\",\"\\u001b[0;32m/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\\u001b[0m in \\u001b[0;36m<module>\\u001b[0;34m()\\u001b[0m\\n\\u001b[1;32m      2\\u001b[0m     \\u001b[0m__file__\\u001b[0m \\u001b[0;34m=\\u001b[0m \\u001b[0;34m'/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb'\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m      3\\u001b[0m     \\u001b[0mmakedirs\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0margs\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0msave\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m----> 4\\u001b[0;31m     \\u001b[0mlogger\\u001b[0m \\u001b[0;34m=\\u001b[0m \\u001b[0mget_logger\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mlogpath\\u001b[0m\\u001b[0;34m=\\u001b[0m\\u001b[0mos\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mpath\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mjoin\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0margs\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0msave\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0;34m'logs'\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0mfilepath\\u001b[0m\\u001b[0;34m=\\u001b[0m\\u001b[0mos\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mpath\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mabspath\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0m__file__\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0m\\u001b[1;32m      5\\u001b[0m     \\u001b[0mlogger\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0minfo\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0margs\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m      6\\u001b[0m \\u001b[0;34m\\u001b[0m\\u001b[0m\\n\",\"\\u001b[0;32m/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\\u001b[0m in \\u001b[0;36mget_logger\\u001b[0;34m(logpath, filepath, package_files, displaying, saving, debug)\\u001b[0m\\n\\u001b[1;32m     99\\u001b[0m         \\u001b[0mlogger\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0maddHandler\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mconsole_handler\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m    100\\u001b[0m     \\u001b[0mlogger\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0minfo\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mfilepath\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m--> 101\\u001b[0;31m     \\u001b[0;32mwith\\u001b[0m \\u001b[0mopen\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mfilepath\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0;34m\\\"r\\\"\\u001b[0m\\u001b[0;34m)\\u001b[0m \\u001b[0;32mas\\u001b[0m \\u001b[0mf\\u001b[0m\\u001b[0;34m:\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0m\\u001b[1;32m    102\\u001b[0m         \\u001b[0mlogger\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0minfo\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mf\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mread\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m    103\\u001b[0m \\u001b[0;34m\\u001b[0m\\u001b[0m\\n\",\"\\u001b[0;31mFileNotFoundError\\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb'\"]}]},{\"cell_type\":\"code\",\"metadata\":{\"colab\":{\"base_uri\":\"https://localhost:8080/\"},\"id\":\"g2k3GwQeJsNy\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617963867638,\"user_tz\":-60,\"elapsed\":420,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}},\"outputId\":\"78f13612-fb4b-4d78-a6f9-3c951d4848a1\"},\"source\":[\"a = os.path.abspath('')\\n\",\"print(type(a))\"],\"execution_count\":35,\"outputs\":[{\"output_type\":\"stream\",\"text\":[\"<class 'str'>\\n\"],\"name\":\"stdout\"}]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"_jhUzE0XKh_C\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617964280066,\"user_tz\":-60,\"elapsed\":1044,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"__file__ = '/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb'\\n\",\"makedirs(args.save)\"],\"execution_count\":52,\"outputs\":[]}]}\n","{\"nbformat\":4,\"nbformat_minor\":0,\"metadata\":{\"colab\":{\"name\":\"mnist_node.ipynb\",\"provenance\":[],\"collapsed_sections\":[],\"mount_file_id\":\"1sLUaZ_DEMKH30zwclvQKQ2FNZUI_mF3K\",\"authorship_tag\":\"ABX9TyM4RT5ovZ+PC2JziHsLNjTc\"},\"kernelspec\":{\"name\":\"python3\",\"display_name\":\"Python 3\"},\"language_info\":{\"name\":\"python\"}},\"cells\":[{\"cell_type\":\"code\",\"metadata\":{\"id\":\"sWmsXi5BIAtb\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617963280508,\"user_tz\":-60,\"elapsed\":4957,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"%%capture\\n\",\"%%bash\\n\",\"pip install torchdiffeq\"],\"execution_count\":1,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"H353RG8fIDd3\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617963632522,\"user_tz\":-60,\"elapsed\":546,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"import os\\n\",\"import argparse\\n\",\"import logging\\n\",\"import time\\n\",\"import numpy as np\\n\",\"import torch\\n\",\"import torch.nn as nn\\n\",\"from torch.utils.data import DataLoader\\n\",\"import torchvision.datasets as datasets\\n\",\"import torchvision.transforms as transforms\\n\",\"\\n\",\"\\n\",\"parser = argparse.ArgumentParser()\\n\",\"parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')\\n\",\"parser.add_argument('--tol', type=float, default=1e-3)\\n\",\"parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])\\n\",\"parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])\\n\",\"parser.add_argument('--nepochs', type=int, default=120)\\n\",\"parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])\\n\",\"parser.add_argument('--lr', type=float, default=0.1)\\n\",\"parser.add_argument('--batch_size', type=int, default=128)\\n\",\"parser.add_argument('--test_batch_size', type=int, default=1000)\\n\",\"\\n\",\"parser.add_argument('--save', type=str, default='./experiment_node1')\\n\",\"parser.add_argument('--gpu', type=int, default=0)\\n\",\"parser.add_argument('--debug', action='store_true')\\n\",\"args = parser.parse_args(args=[])\\n\",\"\\n\",\"if args.adjoint:\\n\",\"    from torchdiffeq import odeint_adjoint as odeint\\n\",\"else:\\n\",\"    from torchdiffeq import odeint\"],\"execution_count\":22,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"agpZtaKwIKnA\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617963497293,\"user_tz\":-60,\"elapsed\":412,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"def conv3x3(in_planes, out_planes, stride=1):\\n\",\"    \\\"\\\"\\\"3x3 convolution with padding\\\"\\\"\\\"\\n\",\"    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\\n\",\"\\n\",\"\\n\",\"def conv1x1(in_planes, out_planes, stride=1):\\n\",\"    \\\"\\\"\\\"1x1 convolution\\\"\\\"\\\"\\n\",\"    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\\n\",\"\\n\",\"\\n\",\"def norm(dim):\\n\",\"    return nn.GroupNorm(min(32, dim), dim)\\n\",\"\\n\",\"\\n\",\"class ResBlock(nn.Module):\\n\",\"    expansion = 1\\n\",\"\\n\",\"    def __init__(self, inplanes, planes, stride=1, downsample=None):\\n\",\"        super(ResBlock, self).__init__()\\n\",\"        self.norm1 = norm(inplanes)\\n\",\"        self.relu = nn.ReLU(inplace=True)\\n\",\"        self.downsample = downsample\\n\",\"        self.conv1 = conv3x3(inplanes, planes, stride)\\n\",\"        self.norm2 = norm(planes)\\n\",\"        self.conv2 = conv3x3(planes, planes)\\n\",\"\\n\",\"    def forward(self, x):\\n\",\"        shortcut = x\\n\",\"\\n\",\"        out = self.relu(self.norm1(x))\\n\",\"\\n\",\"        if self.downsample is not None:\\n\",\"            shortcut = self.downsample(out)\\n\",\"\\n\",\"        out = self.conv1(out)\\n\",\"        out = self.norm2(out)\\n\",\"        out = self.relu(out)\\n\",\"        out = self.conv2(out)\\n\",\"\\n\",\"        return out + shortcut\\n\",\"\\n\",\"\\n\",\"class ConcatConv2d(nn.Module):\\n\",\"\\n\",\"    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):\\n\",\"        super(ConcatConv2d, self).__init__()\\n\",\"        module = nn.ConvTranspose2d if transpose else nn.Conv2d\\n\",\"        self._layer = module(\\n\",\"            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,\\n\",\"            bias=bias\\n\",\"        )\\n\",\"\\n\",\"    def forward(self, t, x):\\n\",\"        tt = torch.ones_like(x[:, :1, :, :]) * t\\n\",\"        ttx = torch.cat([tt, x], 1)\\n\",\"        return self._layer(ttx)\"],\"execution_count\":15,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"1-rlM_3sI-1k\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617963513110,\"user_tz\":-60,\"elapsed\":974,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"class ODEfunc(nn.Module):\\n\",\"\\n\",\"    def __init__(self, dim):\\n\",\"        super(ODEfunc, self).__init__()\\n\",\"        self.norm1 = norm(dim)\\n\",\"        self.relu = nn.ReLU(inplace=True)\\n\",\"        self.conv1 = ConcatConv2d(dim, dim, 3, 1, 1)\\n\",\"        self.norm2 = norm(dim)\\n\",\"        self.conv2 = ConcatConv2d(dim, dim, 3, 1, 1)\\n\",\"        self.norm3 = norm(dim)\\n\",\"        self.nfe = 0\\n\",\"\\n\",\"    def forward(self, t, x):\\n\",\"        self.nfe += 1\\n\",\"        out = self.norm1(x)\\n\",\"        out = self.relu(out)\\n\",\"        out = self.conv1(t, out)\\n\",\"        out = self.norm2(out)\\n\",\"        out = self.relu(out)\\n\",\"        out = self.conv2(t, out)\\n\",\"        out = self.norm3(out)\\n\",\"        return out\\n\",\"\\n\",\"\\n\",\"class ODEBlock(nn.Module):\\n\",\"\\n\",\"    def __init__(self, odefunc):\\n\",\"        super(ODEBlock, self).__init__()\\n\",\"        self.odefunc = odefunc\\n\",\"        self.integration_time = torch.tensor([0, 1]).float()\\n\",\"\\n\",\"    def forward(self, x):\\n\",\"        self.integration_time = self.integration_time.type_as(x)\\n\",\"        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)\\n\",\"        return out[1]\\n\",\"\\n\",\"    @property\\n\",\"    def nfe(self):\\n\",\"        return self.odefunc.nfe\\n\",\"\\n\",\"    @nfe.setter\\n\",\"    def nfe(self, value):\\n\",\"        self.odefunc.nfe = value\"],\"execution_count\":16,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"xhFiCzNaJC0x\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617963529496,\"user_tz\":-60,\"elapsed\":991,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"class Flatten(nn.Module):\\n\",\"\\n\",\"    def __init__(self):\\n\",\"        super(Flatten, self).__init__()\\n\",\"\\n\",\"    def forward(self, x):\\n\",\"        shape = torch.prod(torch.tensor(x.shape[1:])).item()\\n\",\"        return x.view(-1, shape)\\n\",\"\\n\",\"\\n\",\"class RunningAverageMeter(object):\\n\",\"    \\\"\\\"\\\"Computes and stores the average and current value\\\"\\\"\\\"\\n\",\"\\n\",\"    def __init__(self, momentum=0.99):\\n\",\"        self.momentum = momentum\\n\",\"        self.reset()\\n\",\"\\n\",\"    def reset(self):\\n\",\"        self.val = None\\n\",\"        self.avg = 0\\n\",\"\\n\",\"    def update(self, val):\\n\",\"        if self.val is None:\\n\",\"            self.avg = val\\n\",\"        else:\\n\",\"            self.avg = self.avg * self.momentum + val * (1 - self.momentum)\\n\",\"        self.val = val\"],\"execution_count\":17,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"ltcgl1RIJG7U\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617963610116,\"user_tz\":-60,\"elapsed\":397,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"def get_mnist_loaders(data_aug=False, batch_size=128, test_batch_size=1000, perc=1.0):\\n\",\"    if data_aug:\\n\",\"        transform_train = transforms.Compose([\\n\",\"            transforms.RandomCrop(28, padding=4),\\n\",\"            transforms.ToTensor(),\\n\",\"        ])\\n\",\"    else:\\n\",\"        transform_train = transforms.Compose([\\n\",\"            transforms.ToTensor(),\\n\",\"        ])\\n\",\"\\n\",\"    transform_test = transforms.Compose([\\n\",\"        transforms.ToTensor(),\\n\",\"    ])\\n\",\"\\n\",\"    train_loader = DataLoader(\\n\",\"        datasets.MNIST(root='.data/mnist', train=True, download=True, transform=transform_train), batch_size=batch_size,\\n\",\"        shuffle=True, num_workers=2, drop_last=True\\n\",\"    )\\n\",\"\\n\",\"    train_eval_loader = DataLoader(\\n\",\"        datasets.MNIST(root='.data/mnist', train=True, download=True, transform=transform_test),\\n\",\"        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True\\n\",\"    )\\n\",\"\\n\",\"    test_loader = DataLoader(\\n\",\"        datasets.MNIST(root='.data/mnist', train=False, download=True, transform=transform_test),\\n\",\"        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True\\n\",\"    )\\n\",\"\\n\",\"    return train_loader, test_loader, train_eval_loader\\n\",\"\\n\",\"\\n\",\"def inf_generator(iterable):\\n\",\"    \\\"\\\"\\\"Allows training with DataLoaders in a single infinite loop:\\n\",\"        for i, (x, y) in enumerate(inf_generator(train_loader)):\\n\",\"    \\\"\\\"\\\"\\n\",\"    iterator = iterable.__iter__()\\n\",\"    while True:\\n\",\"        try:\\n\",\"            yield iterator.__next__()\\n\",\"        except StopIteration:\\n\",\"            iterator = iterable.__iter__()\\n\",\"\\n\",\"\\n\",\"def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):\\n\",\"    initial_learning_rate = args.lr * batch_size / batch_denom\\n\",\"\\n\",\"    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]\\n\",\"    vals = [initial_learning_rate * decay for decay in decay_rates]\\n\",\"\\n\",\"    def learning_rate_fn(itr):\\n\",\"        lt = [itr < b for b in boundaries] + [True]\\n\",\"        i = np.argmax(lt)\\n\",\"        return vals[i]\\n\",\"\\n\",\"    return learning_rate_fn\\n\",\"\\n\",\"\\n\",\"def one_hot(x, K):\\n\",\"    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)\\n\",\"\\n\",\"\\n\",\"def accuracy(model, dataset_loader):\\n\",\"    total_correct = 0\\n\",\"    for x, y in dataset_loader:\\n\",\"        x = x.to(device)\\n\",\"        y = one_hot(np.array(y.numpy()), 10)\\n\",\"\\n\",\"        target_class = np.argmax(y, axis=1)\\n\",\"        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)\\n\",\"        total_correct += np.sum(predicted_class == target_class)\\n\",\"    return total_correct / len(dataset_loader.dataset)\\n\",\"\\n\",\"\\n\",\"def count_parameters(model):\\n\",\"    return sum(p.numel() for p in model.parameters() if p.requires_grad)\\n\",\"\\n\",\"\\n\",\"def makedirs(dirname):\\n\",\"    if not os.path.exists(dirname):\\n\",\"        os.makedirs(dirname)\\n\",\"\\n\",\"\\n\",\"def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):\\n\",\"    logger = logging.getLogger()\\n\",\"    if debug:\\n\",\"        level = logging.DEBUG\\n\",\"    else:\\n\",\"        level = logging.INFO\\n\",\"    logger.setLevel(level)\\n\",\"    if saving:\\n\",\"        info_file_handler = logging.FileHandler(logpath, mode=\\\"a\\\")\\n\",\"        info_file_handler.setLevel(level)\\n\",\"        logger.addHandler(info_file_handler)\\n\",\"    if displaying:\\n\",\"        console_handler = logging.StreamHandler()\\n\",\"        console_handler.setLevel(level)\\n\",\"        logger.addHandler(console_handler)\\n\",\"    logger.info(filepath)\\n\",\"    with open(filepath, \\\"r\\\") as f:\\n\",\"        logger.info(f.read())\\n\",\"\\n\",\"    for f in package_files:\\n\",\"        logger.info(f)\\n\",\"        with open(f, \\\"r\\\") as package_f:\\n\",\"            logger.info(package_f.read())\\n\",\"\\n\",\"    return logger\\n\"],\"execution_count\":20,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"colab\":{\"base_uri\":\"https://localhost:8080/\",\"height\":470},\"id\":\"42RIwJ37JKQr\",\"executionInfo\":{\"status\":\"error\",\"timestamp\":1617964221740,\"user_tz\":-60,\"elapsed\":825,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}},\"outputId\":\"076c12d7-0015-4846-a548-eeaec45004fd\"},\"source\":[\"if __name__ == '__main__':\\n\",\"    __file__ = '/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb'\\n\",\"    makedirs(args.save)\\n\",\"    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))\\n\",\"    logger.info(args)\\n\",\"\\n\",\"    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')\\n\",\"\\n\",\"    is_odenet = args.network == 'odenet'\\n\",\"\\n\",\"    if args.downsampling_method == 'conv':\\n\",\"        downsampling_layers = [\\n\",\"            nn.Conv2d(1, 64, 3, 1),\\n\",\"            norm(64),\\n\",\"            nn.ReLU(inplace=True),\\n\",\"            nn.Conv2d(64, 64, 4, 2, 1),\\n\",\"            norm(64),\\n\",\"            nn.ReLU(inplace=True),\\n\",\"            nn.Conv2d(64, 64, 4, 2, 1),\\n\",\"        ]\\n\",\"    elif args.downsampling_method == 'res':\\n\",\"        downsampling_layers = [\\n\",\"            nn.Conv2d(1, 64, 3, 1),\\n\",\"            ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\\n\",\"            ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\\n\",\"        ]\\n\",\"\\n\",\"    feature_layers = [ODEBlock(ODEfunc(64))] if is_odenet else [ResBlock(64, 64) for _ in range(6)]\\n\",\"    fc_layers = [norm(64), nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d((1, 1)), Flatten(), nn.Linear(64, 10)]\\n\",\"\\n\",\"    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)\\n\",\"\\n\",\"    logger.info(model)\\n\",\"    logger.info('Number of parameters: {}'.format(count_parameters(model)))\\n\",\"\\n\",\"    criterion = nn.CrossEntropyLoss().to(device)\\n\",\"\\n\",\"    train_loader, test_loader, train_eval_loader = get_mnist_loaders(\\n\",\"        args.data_aug, args.batch_size, args.test_batch_size\\n\",\"    )\\n\",\"\\n\",\"    data_gen = inf_generator(train_loader)\\n\",\"    batches_per_epoch = len(train_loader)\\n\",\"\\n\",\"    lr_fn = learning_rate_with_decay(\\n\",\"        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],\\n\",\"        decay_rates=[1, 0.1, 0.01, 0.001]\\n\",\"    )\\n\",\"\\n\",\"    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)\\n\",\"\\n\",\"    best_acc = 0\\n\",\"    batch_time_meter = RunningAverageMeter()\\n\",\"    f_nfe_meter = RunningAverageMeter()\\n\",\"    b_nfe_meter = RunningAverageMeter()\\n\",\"    end = time.time()\\n\",\"    \\n\",\"    epoch_arr = []\\n\",\"    time_val_arr = []\\n\",\"    time_avg_arr = []\\n\",\"    nfe_f_arr = []\\n\",\"    nfe_b_arr = []\\n\",\"    train_acc_arr = []\\n\",\"    test_acc_arr = []\\n\",\"\\n\",\"    for itr in range(args.nepochs * batches_per_epoch):\\n\",\"\\n\",\"        for param_group in optimizer.param_groups:\\n\",\"            param_group['lr'] = lr_fn(itr)\\n\",\"\\n\",\"        optimizer.zero_grad()\\n\",\"        x, y = data_gen.__next__()\\n\",\"        x = x.to(device)\\n\",\"        y = y.to(device)\\n\",\"        logits = model(x)\\n\",\"        loss = criterion(logits, y)\\n\",\"\\n\",\"        if is_odenet:\\n\",\"            nfe_forward = feature_layers[0].nfe\\n\",\"            feature_layers[0].nfe = 0\\n\",\"\\n\",\"        loss.backward()\\n\",\"        optimizer.step()\\n\",\"\\n\",\"        if is_odenet:\\n\",\"            nfe_backward = feature_layers[0].nfe\\n\",\"            feature_layers[0].nfe = 0\\n\",\"\\n\",\"        batch_time_meter.update(time.time() - end)\\n\",\"        if is_odenet:\\n\",\"            f_nfe_meter.update(nfe_forward)\\n\",\"            b_nfe_meter.update(nfe_backward)\\n\",\"        end = time.time()\\n\",\"\\n\",\"        if itr % batches_per_epoch == 0:\\n\",\"            with torch.no_grad():\\n\",\"                train_acc = accuracy(model, train_eval_loader)\\n\",\"                val_acc = accuracy(model, test_loader)\\n\",\"                if val_acc > best_acc:\\n\",\"                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))\\n\",\"                    best_acc = val_acc\\n\",\"                logger.info(\\n\",\"                    \\\"Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | \\\"\\n\",\"                    \\\"Train Acc {:.4f} | Test Acc {:.4f}\\\".format(\\n\",\"                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,\\n\",\"                        b_nfe_meter.avg, train_acc, val_acc\\n\",\"                    )\\n\",\"                )\\n\",\"                epoch_arr += [itr // batches_per_epoch]\\n\",\"                time_val_arr += [batch_time_meter.val]\\n\",\"                time_avg_arr += [batch_time_meter.avg]\\n\",\"                nfe_f_arr += [f_nfe_meter.avg]\\n\",\"                nfe_b_arr += [b_nfe_meter.avg]\\n\",\"                train_acc_arr += [train_acc]\\n\",\"                test_acc_arr += [val_acc]\\n\",\"                    \\n\",\"    epoch_arr = np.asarray(epoch_arr)\\n\",\"    time_val_arr = np.asarray(time_val_arr)\\n\",\"    time_avg_arr = np.asarray(time_avg_arr)\\n\",\"    nfe_f_arr = np.asarray(nfe_f_arr)\\n\",\"    nfe_b_arr = np.asarray(nfe_b_arr)\\n\",\"    train_acc_arr = np.asarray(train_acc_arr)\\n\",\"    test_acc_arr = np.asarray(test_acc_arr)\\n\",\"    \\n\",\"    np.save(os.path.join(args.save, 'epoch_arr.npy'), epoch_arr)\\n\",\"    np.save(os.path.join(args.save, 'time_val_arr.npy'), time_val_arr)\\n\",\"    np.save(os.path.join(args.save, 'time_avg_arr.npy'), time_avg_arr)\\n\",\"    np.save(os.path.join(args.save, 'nfe_f_arr.npy'), nfe_f_arr)\\n\",\"    np.save(os.path.join(args.save, 'nfe_b_arr.npy'), nfe_b_arr)\\n\",\"    np.save(os.path.join(args.save, 'train_acc_arr.npy'), train_acc_arr)\\n\",\"    np.save(os.path.join(args.save, 'test_acc_arr.npy'), test_acc_arr)\"],\"execution_count\":50,\"outputs\":[{\"output_type\":\"stream\",\"text\":[\"/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\\n\",\"/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\\n\",\"/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\\n\",\"/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\\n\",\"/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\\n\"],\"name\":\"stderr\"},{\"output_type\":\"error\",\"ename\":\"FileNotFoundError\",\"evalue\":\"ignored\",\"traceback\":[\"\\u001b[0;31m---------------------------------------------------------------------------\\u001b[0m\",\"\\u001b[0;31mFileNotFoundError\\u001b[0m                         Traceback (most recent call last)\",\"\\u001b[0;32m/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\\u001b[0m in \\u001b[0;36m<module>\\u001b[0;34m()\\u001b[0m\\n\\u001b[1;32m      2\\u001b[0m     \\u001b[0m__file__\\u001b[0m \\u001b[0;34m=\\u001b[0m \\u001b[0;34m'/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb'\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m      3\\u001b[0m     \\u001b[0mmakedirs\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0margs\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0msave\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m----> 4\\u001b[0;31m     \\u001b[0mlogger\\u001b[0m \\u001b[0;34m=\\u001b[0m \\u001b[0mget_logger\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mlogpath\\u001b[0m\\u001b[0;34m=\\u001b[0m\\u001b[0mos\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mpath\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mjoin\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0margs\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0msave\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0;34m'logs'\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0mfilepath\\u001b[0m\\u001b[0;34m=\\u001b[0m\\u001b[0mos\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mpath\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mabspath\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0m__file__\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0m\\u001b[1;32m      5\\u001b[0m     \\u001b[0mlogger\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0minfo\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0margs\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m      6\\u001b[0m \\u001b[0;34m\\u001b[0m\\u001b[0m\\n\",\"\\u001b[0;32m/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\\u001b[0m in \\u001b[0;36mget_logger\\u001b[0;34m(logpath, filepath, package_files, displaying, saving, debug)\\u001b[0m\\n\\u001b[1;32m     99\\u001b[0m         \\u001b[0mlogger\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0maddHandler\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mconsole_handler\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m    100\\u001b[0m     \\u001b[0mlogger\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0minfo\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mfilepath\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m--> 101\\u001b[0;31m     \\u001b[0;32mwith\\u001b[0m \\u001b[0mopen\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mfilepath\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0;34m\\\"r\\\"\\u001b[0m\\u001b[0;34m)\\u001b[0m \\u001b[0;32mas\\u001b[0m \\u001b[0mf\\u001b[0m\\u001b[0;34m:\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0m\\u001b[1;32m    102\\u001b[0m         \\u001b[0mlogger\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0minfo\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mf\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mread\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m    103\\u001b[0m \\u001b[0;34m\\u001b[0m\\u001b[0m\\n\",\"\\u001b[0;31mFileNotFoundError\\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb'\"]}]},{\"cell_type\":\"code\",\"metadata\":{\"colab\":{\"base_uri\":\"https://localhost:8080/\"},\"id\":\"g2k3GwQeJsNy\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617963867638,\"user_tz\":-60,\"elapsed\":420,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}},\"outputId\":\"78f13612-fb4b-4d78-a6f9-3c951d4848a1\"},\"source\":[\"a = os.path.abspath('')\\n\",\"print(type(a))\"],\"execution_count\":35,\"outputs\":[{\"output_type\":\"stream\",\"text\":[\"<class 'str'>\\n\"],\"name\":\"stdout\"}]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"_jhUzE0XKh_C\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617964280066,\"user_tz\":-60,\"elapsed\":1044,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"__file__ = '/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb'\\n\",\"makedirs(args.save)\"],\"execution_count\":52,\"outputs\":[]}]}\n","{\"nbformat\":4,\"nbformat_minor\":0,\"metadata\":{\"colab\":{\"name\":\"mnist_node.ipynb\",\"provenance\":[],\"collapsed_sections\":[],\"mount_file_id\":\"1sLUaZ_DEMKH30zwclvQKQ2FNZUI_mF3K\",\"authorship_tag\":\"ABX9TyM4RT5ovZ+PC2JziHsLNjTc\"},\"kernelspec\":{\"name\":\"python3\",\"display_name\":\"Python 3\"},\"language_info\":{\"name\":\"python\"}},\"cells\":[{\"cell_type\":\"code\",\"metadata\":{\"id\":\"sWmsXi5BIAtb\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617963280508,\"user_tz\":-60,\"elapsed\":4957,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"%%capture\\n\",\"%%bash\\n\",\"pip install torchdiffeq\"],\"execution_count\":1,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"H353RG8fIDd3\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617963632522,\"user_tz\":-60,\"elapsed\":546,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"import os\\n\",\"import argparse\\n\",\"import logging\\n\",\"import time\\n\",\"import numpy as np\\n\",\"import torch\\n\",\"import torch.nn as nn\\n\",\"from torch.utils.data import DataLoader\\n\",\"import torchvision.datasets as datasets\\n\",\"import torchvision.transforms as transforms\\n\",\"\\n\",\"\\n\",\"parser = argparse.ArgumentParser()\\n\",\"parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')\\n\",\"parser.add_argument('--tol', type=float, default=1e-3)\\n\",\"parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])\\n\",\"parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])\\n\",\"parser.add_argument('--nepochs', type=int, default=120)\\n\",\"parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])\\n\",\"parser.add_argument('--lr', type=float, default=0.1)\\n\",\"parser.add_argument('--batch_size', type=int, default=128)\\n\",\"parser.add_argument('--test_batch_size', type=int, default=1000)\\n\",\"\\n\",\"parser.add_argument('--save', type=str, default='./experiment_node1')\\n\",\"parser.add_argument('--gpu', type=int, default=0)\\n\",\"parser.add_argument('--debug', action='store_true')\\n\",\"args = parser.parse_args(args=[])\\n\",\"\\n\",\"if args.adjoint:\\n\",\"    from torchdiffeq import odeint_adjoint as odeint\\n\",\"else:\\n\",\"    from torchdiffeq import odeint\"],\"execution_count\":22,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"agpZtaKwIKnA\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617963497293,\"user_tz\":-60,\"elapsed\":412,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"def conv3x3(in_planes, out_planes, stride=1):\\n\",\"    \\\"\\\"\\\"3x3 convolution with padding\\\"\\\"\\\"\\n\",\"    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\\n\",\"\\n\",\"\\n\",\"def conv1x1(in_planes, out_planes, stride=1):\\n\",\"    \\\"\\\"\\\"1x1 convolution\\\"\\\"\\\"\\n\",\"    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\\n\",\"\\n\",\"\\n\",\"def norm(dim):\\n\",\"    return nn.GroupNorm(min(32, dim), dim)\\n\",\"\\n\",\"\\n\",\"class ResBlock(nn.Module):\\n\",\"    expansion = 1\\n\",\"\\n\",\"    def __init__(self, inplanes, planes, stride=1, downsample=None):\\n\",\"        super(ResBlock, self).__init__()\\n\",\"        self.norm1 = norm(inplanes)\\n\",\"        self.relu = nn.ReLU(inplace=True)\\n\",\"        self.downsample = downsample\\n\",\"        self.conv1 = conv3x3(inplanes, planes, stride)\\n\",\"        self.norm2 = norm(planes)\\n\",\"        self.conv2 = conv3x3(planes, planes)\\n\",\"\\n\",\"    def forward(self, x):\\n\",\"        shortcut = x\\n\",\"\\n\",\"        out = self.relu(self.norm1(x))\\n\",\"\\n\",\"        if self.downsample is not None:\\n\",\"            shortcut = self.downsample(out)\\n\",\"\\n\",\"        out = self.conv1(out)\\n\",\"        out = self.norm2(out)\\n\",\"        out = self.relu(out)\\n\",\"        out = self.conv2(out)\\n\",\"\\n\",\"        return out + shortcut\\n\",\"\\n\",\"\\n\",\"class ConcatConv2d(nn.Module):\\n\",\"\\n\",\"    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):\\n\",\"        super(ConcatConv2d, self).__init__()\\n\",\"        module = nn.ConvTranspose2d if transpose else nn.Conv2d\\n\",\"        self._layer = module(\\n\",\"            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,\\n\",\"            bias=bias\\n\",\"        )\\n\",\"\\n\",\"    def forward(self, t, x):\\n\",\"        tt = torch.ones_like(x[:, :1, :, :]) * t\\n\",\"        ttx = torch.cat([tt, x], 1)\\n\",\"        return self._layer(ttx)\"],\"execution_count\":15,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"1-rlM_3sI-1k\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617963513110,\"user_tz\":-60,\"elapsed\":974,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"class ODEfunc(nn.Module):\\n\",\"\\n\",\"    def __init__(self, dim):\\n\",\"        super(ODEfunc, self).__init__()\\n\",\"        self.norm1 = norm(dim)\\n\",\"        self.relu = nn.ReLU(inplace=True)\\n\",\"        self.conv1 = ConcatConv2d(dim, dim, 3, 1, 1)\\n\",\"        self.norm2 = norm(dim)\\n\",\"        self.conv2 = ConcatConv2d(dim, dim, 3, 1, 1)\\n\",\"        self.norm3 = norm(dim)\\n\",\"        self.nfe = 0\\n\",\"\\n\",\"    def forward(self, t, x):\\n\",\"        self.nfe += 1\\n\",\"        out = self.norm1(x)\\n\",\"        out = self.relu(out)\\n\",\"        out = self.conv1(t, out)\\n\",\"        out = self.norm2(out)\\n\",\"        out = self.relu(out)\\n\",\"        out = self.conv2(t, out)\\n\",\"        out = self.norm3(out)\\n\",\"        return out\\n\",\"\\n\",\"\\n\",\"class ODEBlock(nn.Module):\\n\",\"\\n\",\"    def __init__(self, odefunc):\\n\",\"        super(ODEBlock, self).__init__()\\n\",\"        self.odefunc = odefunc\\n\",\"        self.integration_time = torch.tensor([0, 1]).float()\\n\",\"\\n\",\"    def forward(self, x):\\n\",\"        self.integration_time = self.integration_time.type_as(x)\\n\",\"        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)\\n\",\"        return out[1]\\n\",\"\\n\",\"    @property\\n\",\"    def nfe(self):\\n\",\"        return self.odefunc.nfe\\n\",\"\\n\",\"    @nfe.setter\\n\",\"    def nfe(self, value):\\n\",\"        self.odefunc.nfe = value\"],\"execution_count\":16,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"xhFiCzNaJC0x\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617963529496,\"user_tz\":-60,\"elapsed\":991,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"class Flatten(nn.Module):\\n\",\"\\n\",\"    def __init__(self):\\n\",\"        super(Flatten, self).__init__()\\n\",\"\\n\",\"    def forward(self, x):\\n\",\"        shape = torch.prod(torch.tensor(x.shape[1:])).item()\\n\",\"        return x.view(-1, shape)\\n\",\"\\n\",\"\\n\",\"class RunningAverageMeter(object):\\n\",\"    \\\"\\\"\\\"Computes and stores the average and current value\\\"\\\"\\\"\\n\",\"\\n\",\"    def __init__(self, momentum=0.99):\\n\",\"        self.momentum = momentum\\n\",\"        self.reset()\\n\",\"\\n\",\"    def reset(self):\\n\",\"        self.val = None\\n\",\"        self.avg = 0\\n\",\"\\n\",\"    def update(self, val):\\n\",\"        if self.val is None:\\n\",\"            self.avg = val\\n\",\"        else:\\n\",\"            self.avg = self.avg * self.momentum + val * (1 - self.momentum)\\n\",\"        self.val = val\"],\"execution_count\":17,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"ltcgl1RIJG7U\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617963610116,\"user_tz\":-60,\"elapsed\":397,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"def get_mnist_loaders(data_aug=False, batch_size=128, test_batch_size=1000, perc=1.0):\\n\",\"    if data_aug:\\n\",\"        transform_train = transforms.Compose([\\n\",\"            transforms.RandomCrop(28, padding=4),\\n\",\"            transforms.ToTensor(),\\n\",\"        ])\\n\",\"    else:\\n\",\"        transform_train = transforms.Compose([\\n\",\"            transforms.ToTensor(),\\n\",\"        ])\\n\",\"\\n\",\"    transform_test = transforms.Compose([\\n\",\"        transforms.ToTensor(),\\n\",\"    ])\\n\",\"\\n\",\"    train_loader = DataLoader(\\n\",\"        datasets.MNIST(root='.data/mnist', train=True, download=True, transform=transform_train), batch_size=batch_size,\\n\",\"        shuffle=True, num_workers=2, drop_last=True\\n\",\"    )\\n\",\"\\n\",\"    train_eval_loader = DataLoader(\\n\",\"        datasets.MNIST(root='.data/mnist', train=True, download=True, transform=transform_test),\\n\",\"        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True\\n\",\"    )\\n\",\"\\n\",\"    test_loader = DataLoader(\\n\",\"        datasets.MNIST(root='.data/mnist', train=False, download=True, transform=transform_test),\\n\",\"        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True\\n\",\"    )\\n\",\"\\n\",\"    return train_loader, test_loader, train_eval_loader\\n\",\"\\n\",\"\\n\",\"def inf_generator(iterable):\\n\",\"    \\\"\\\"\\\"Allows training with DataLoaders in a single infinite loop:\\n\",\"        for i, (x, y) in enumerate(inf_generator(train_loader)):\\n\",\"    \\\"\\\"\\\"\\n\",\"    iterator = iterable.__iter__()\\n\",\"    while True:\\n\",\"        try:\\n\",\"            yield iterator.__next__()\\n\",\"        except StopIteration:\\n\",\"            iterator = iterable.__iter__()\\n\",\"\\n\",\"\\n\",\"def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):\\n\",\"    initial_learning_rate = args.lr * batch_size / batch_denom\\n\",\"\\n\",\"    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]\\n\",\"    vals = [initial_learning_rate * decay for decay in decay_rates]\\n\",\"\\n\",\"    def learning_rate_fn(itr):\\n\",\"        lt = [itr < b for b in boundaries] + [True]\\n\",\"        i = np.argmax(lt)\\n\",\"        return vals[i]\\n\",\"\\n\",\"    return learning_rate_fn\\n\",\"\\n\",\"\\n\",\"def one_hot(x, K):\\n\",\"    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)\\n\",\"\\n\",\"\\n\",\"def accuracy(model, dataset_loader):\\n\",\"    total_correct = 0\\n\",\"    for x, y in dataset_loader:\\n\",\"        x = x.to(device)\\n\",\"        y = one_hot(np.array(y.numpy()), 10)\\n\",\"\\n\",\"        target_class = np.argmax(y, axis=1)\\n\",\"        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)\\n\",\"        total_correct += np.sum(predicted_class == target_class)\\n\",\"    return total_correct / len(dataset_loader.dataset)\\n\",\"\\n\",\"\\n\",\"def count_parameters(model):\\n\",\"    return sum(p.numel() for p in model.parameters() if p.requires_grad)\\n\",\"\\n\",\"\\n\",\"def makedirs(dirname):\\n\",\"    if not os.path.exists(dirname):\\n\",\"        os.makedirs(dirname)\\n\",\"\\n\",\"\\n\",\"def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):\\n\",\"    logger = logging.getLogger()\\n\",\"    if debug:\\n\",\"        level = logging.DEBUG\\n\",\"    else:\\n\",\"        level = logging.INFO\\n\",\"    logger.setLevel(level)\\n\",\"    if saving:\\n\",\"        info_file_handler = logging.FileHandler(logpath, mode=\\\"a\\\")\\n\",\"        info_file_handler.setLevel(level)\\n\",\"        logger.addHandler(info_file_handler)\\n\",\"    if displaying:\\n\",\"        console_handler = logging.StreamHandler()\\n\",\"        console_handler.setLevel(level)\\n\",\"        logger.addHandler(console_handler)\\n\",\"    logger.info(filepath)\\n\",\"    with open(filepath, \\\"r\\\") as f:\\n\",\"        logger.info(f.read())\\n\",\"\\n\",\"    for f in package_files:\\n\",\"        logger.info(f)\\n\",\"        with open(f, \\\"r\\\") as package_f:\\n\",\"            logger.info(package_f.read())\\n\",\"\\n\",\"    return logger\\n\"],\"execution_count\":20,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"colab\":{\"base_uri\":\"https://localhost:8080/\",\"height\":470},\"id\":\"42RIwJ37JKQr\",\"executionInfo\":{\"status\":\"error\",\"timestamp\":1617964221740,\"user_tz\":-60,\"elapsed\":825,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}},\"outputId\":\"076c12d7-0015-4846-a548-eeaec45004fd\"},\"source\":[\"if __name__ == '__main__':\\n\",\"    __file__ = '/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb'\\n\",\"    makedirs(args.save)\\n\",\"    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))\\n\",\"    logger.info(args)\\n\",\"\\n\",\"    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')\\n\",\"\\n\",\"    is_odenet = args.network == 'odenet'\\n\",\"\\n\",\"    if args.downsampling_method == 'conv':\\n\",\"        downsampling_layers = [\\n\",\"            nn.Conv2d(1, 64, 3, 1),\\n\",\"            norm(64),\\n\",\"            nn.ReLU(inplace=True),\\n\",\"            nn.Conv2d(64, 64, 4, 2, 1),\\n\",\"            norm(64),\\n\",\"            nn.ReLU(inplace=True),\\n\",\"            nn.Conv2d(64, 64, 4, 2, 1),\\n\",\"        ]\\n\",\"    elif args.downsampling_method == 'res':\\n\",\"        downsampling_layers = [\\n\",\"            nn.Conv2d(1, 64, 3, 1),\\n\",\"            ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\\n\",\"            ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\\n\",\"        ]\\n\",\"\\n\",\"    feature_layers = [ODEBlock(ODEfunc(64))] if is_odenet else [ResBlock(64, 64) for _ in range(6)]\\n\",\"    fc_layers = [norm(64), nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d((1, 1)), Flatten(), nn.Linear(64, 10)]\\n\",\"\\n\",\"    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)\\n\",\"\\n\",\"    logger.info(model)\\n\",\"    logger.info('Number of parameters: {}'.format(count_parameters(model)))\\n\",\"\\n\",\"    criterion = nn.CrossEntropyLoss().to(device)\\n\",\"\\n\",\"    train_loader, test_loader, train_eval_loader = get_mnist_loaders(\\n\",\"        args.data_aug, args.batch_size, args.test_batch_size\\n\",\"    )\\n\",\"\\n\",\"    data_gen = inf_generator(train_loader)\\n\",\"    batches_per_epoch = len(train_loader)\\n\",\"\\n\",\"    lr_fn = learning_rate_with_decay(\\n\",\"        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],\\n\",\"        decay_rates=[1, 0.1, 0.01, 0.001]\\n\",\"    )\\n\",\"\\n\",\"    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)\\n\",\"\\n\",\"    best_acc = 0\\n\",\"    batch_time_meter = RunningAverageMeter()\\n\",\"    f_nfe_meter = RunningAverageMeter()\\n\",\"    b_nfe_meter = RunningAverageMeter()\\n\",\"    end = time.time()\\n\",\"    \\n\",\"    epoch_arr = []\\n\",\"    time_val_arr = []\\n\",\"    time_avg_arr = []\\n\",\"    nfe_f_arr = []\\n\",\"    nfe_b_arr = []\\n\",\"    train_acc_arr = []\\n\",\"    test_acc_arr = []\\n\",\"\\n\",\"    for itr in range(args.nepochs * batches_per_epoch):\\n\",\"\\n\",\"        for param_group in optimizer.param_groups:\\n\",\"            param_group['lr'] = lr_fn(itr)\\n\",\"\\n\",\"        optimizer.zero_grad()\\n\",\"        x, y = data_gen.__next__()\\n\",\"        x = x.to(device)\\n\",\"        y = y.to(device)\\n\",\"        logits = model(x)\\n\",\"        loss = criterion(logits, y)\\n\",\"\\n\",\"        if is_odenet:\\n\",\"            nfe_forward = feature_layers[0].nfe\\n\",\"            feature_layers[0].nfe = 0\\n\",\"\\n\",\"        loss.backward()\\n\",\"        optimizer.step()\\n\",\"\\n\",\"        if is_odenet:\\n\",\"            nfe_backward = feature_layers[0].nfe\\n\",\"            feature_layers[0].nfe = 0\\n\",\"\\n\",\"        batch_time_meter.update(time.time() - end)\\n\",\"        if is_odenet:\\n\",\"            f_nfe_meter.update(nfe_forward)\\n\",\"            b_nfe_meter.update(nfe_backward)\\n\",\"        end = time.time()\\n\",\"\\n\",\"        if itr % batches_per_epoch == 0:\\n\",\"            with torch.no_grad():\\n\",\"                train_acc = accuracy(model, train_eval_loader)\\n\",\"                val_acc = accuracy(model, test_loader)\\n\",\"                if val_acc > best_acc:\\n\",\"                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))\\n\",\"                    best_acc = val_acc\\n\",\"                logger.info(\\n\",\"                    \\\"Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | \\\"\\n\",\"                    \\\"Train Acc {:.4f} | Test Acc {:.4f}\\\".format(\\n\",\"                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,\\n\",\"                        b_nfe_meter.avg, train_acc, val_acc\\n\",\"                    )\\n\",\"                )\\n\",\"                epoch_arr += [itr // batches_per_epoch]\\n\",\"                time_val_arr += [batch_time_meter.val]\\n\",\"                time_avg_arr += [batch_time_meter.avg]\\n\",\"                nfe_f_arr += [f_nfe_meter.avg]\\n\",\"                nfe_b_arr += [b_nfe_meter.avg]\\n\",\"                train_acc_arr += [train_acc]\\n\",\"                test_acc_arr += [val_acc]\\n\",\"                    \\n\",\"    epoch_arr = np.asarray(epoch_arr)\\n\",\"    time_val_arr = np.asarray(time_val_arr)\\n\",\"    time_avg_arr = np.asarray(time_avg_arr)\\n\",\"    nfe_f_arr = np.asarray(nfe_f_arr)\\n\",\"    nfe_b_arr = np.asarray(nfe_b_arr)\\n\",\"    train_acc_arr = np.asarray(train_acc_arr)\\n\",\"    test_acc_arr = np.asarray(test_acc_arr)\\n\",\"    \\n\",\"    np.save(os.path.join(args.save, 'epoch_arr.npy'), epoch_arr)\\n\",\"    np.save(os.path.join(args.save, 'time_val_arr.npy'), time_val_arr)\\n\",\"    np.save(os.path.join(args.save, 'time_avg_arr.npy'), time_avg_arr)\\n\",\"    np.save(os.path.join(args.save, 'nfe_f_arr.npy'), nfe_f_arr)\\n\",\"    np.save(os.path.join(args.save, 'nfe_b_arr.npy'), nfe_b_arr)\\n\",\"    np.save(os.path.join(args.save, 'train_acc_arr.npy'), train_acc_arr)\\n\",\"    np.save(os.path.join(args.save, 'test_acc_arr.npy'), test_acc_arr)\"],\"execution_count\":50,\"outputs\":[{\"output_type\":\"stream\",\"text\":[\"/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\\n\",\"/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\\n\",\"/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\\n\",\"/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\\n\",\"/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\\n\"],\"name\":\"stderr\"},{\"output_type\":\"error\",\"ename\":\"FileNotFoundError\",\"evalue\":\"ignored\",\"traceback\":[\"\\u001b[0;31m---------------------------------------------------------------------------\\u001b[0m\",\"\\u001b[0;31mFileNotFoundError\\u001b[0m                         Traceback (most recent call last)\",\"\\u001b[0;32m/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\\u001b[0m in \\u001b[0;36m<module>\\u001b[0;34m()\\u001b[0m\\n\\u001b[1;32m      2\\u001b[0m     \\u001b[0m__file__\\u001b[0m \\u001b[0;34m=\\u001b[0m \\u001b[0;34m'/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb'\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m      3\\u001b[0m     \\u001b[0mmakedirs\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0margs\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0msave\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m----> 4\\u001b[0;31m     \\u001b[0mlogger\\u001b[0m \\u001b[0;34m=\\u001b[0m \\u001b[0mget_logger\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mlogpath\\u001b[0m\\u001b[0;34m=\\u001b[0m\\u001b[0mos\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mpath\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mjoin\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0margs\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0msave\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0;34m'logs'\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0mfilepath\\u001b[0m\\u001b[0;34m=\\u001b[0m\\u001b[0mos\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mpath\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mabspath\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0m__file__\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0m\\u001b[1;32m      5\\u001b[0m     \\u001b[0mlogger\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0minfo\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0margs\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m      6\\u001b[0m \\u001b[0;34m\\u001b[0m\\u001b[0m\\n\",\"\\u001b[0;32m/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\\u001b[0m in \\u001b[0;36mget_logger\\u001b[0;34m(logpath, filepath, package_files, displaying, saving, debug)\\u001b[0m\\n\\u001b[1;32m     99\\u001b[0m         \\u001b[0mlogger\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0maddHandler\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mconsole_handler\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m    100\\u001b[0m     \\u001b[0mlogger\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0minfo\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mfilepath\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m--> 101\\u001b[0;31m     \\u001b[0;32mwith\\u001b[0m \\u001b[0mopen\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mfilepath\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0;34m\\\"r\\\"\\u001b[0m\\u001b[0;34m)\\u001b[0m \\u001b[0;32mas\\u001b[0m \\u001b[0mf\\u001b[0m\\u001b[0;34m:\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0m\\u001b[1;32m    102\\u001b[0m         \\u001b[0mlogger\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0minfo\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mf\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mread\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m    103\\u001b[0m \\u001b[0;34m\\u001b[0m\\u001b[0m\\n\",\"\\u001b[0;31mFileNotFoundError\\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb'\"]}]},{\"cell_type\":\"code\",\"metadata\":{\"colab\":{\"base_uri\":\"https://localhost:8080/\"},\"id\":\"g2k3GwQeJsNy\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617963867638,\"user_tz\":-60,\"elapsed\":420,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}},\"outputId\":\"78f13612-fb4b-4d78-a6f9-3c951d4848a1\"},\"source\":[\"a = os.path.abspath('')\\n\",\"print(type(a))\"],\"execution_count\":35,\"outputs\":[{\"output_type\":\"stream\",\"text\":[\"<class 'str'>\\n\"],\"name\":\"stdout\"}]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"_jhUzE0XKh_C\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617964280066,\"user_tz\":-60,\"elapsed\":1044,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"__file__ = '/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb'\\n\",\"makedirs(args.save)\"],\"execution_count\":52,\"outputs\":[]}]}\n","Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.1, nepochs=120, network='odenet', save='./experiment_node1', test_batch_size=1000, tol=0.001)\n","Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.1, nepochs=120, network='odenet', save='./experiment_node1', test_batch_size=1000, tol=0.001)\n","Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.1, nepochs=120, network='odenet', save='./experiment_node1', test_batch_size=1000, tol=0.001)\n","Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.1, nepochs=120, network='odenet', save='./experiment_node1', test_batch_size=1000, tol=0.001)\n","Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.1, nepochs=120, network='odenet', save='./experiment_node1', test_batch_size=1000, tol=0.001)\n","Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.1, nepochs=120, network='odenet', save='./experiment_node1', test_batch_size=1000, tol=0.001)\n","Sequential(\n","  (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1))\n","  (1): GroupNorm(32, 64, eps=1e-05, affine=True)\n","  (2): ReLU(inplace=True)\n","  (3): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","  (4): GroupNorm(32, 64, eps=1e-05, affine=True)\n","  (5): ReLU(inplace=True)\n","  (6): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","  (7): ODEBlock(\n","    (odefunc): ODEfunc(\n","      (norm1): GroupNorm(32, 64, eps=1e-05, affine=True)\n","      (relu): ReLU(inplace=True)\n","      (conv1): ConcatConv2d(\n","        (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","      (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)\n","      (conv2): ConcatConv2d(\n","        (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","      (norm3): GroupNorm(32, 64, eps=1e-05, affine=True)\n","    )\n","  )\n","  (8): GroupNorm(32, 64, eps=1e-05, affine=True)\n","  (9): ReLU(inplace=True)\n","  (10): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (11): Flatten()\n","  (12): Linear(in_features=64, out_features=10, bias=True)\n",")\n","Sequential(\n","  (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1))\n","  (1): GroupNorm(32, 64, eps=1e-05, affine=True)\n","  (2): ReLU(inplace=True)\n","  (3): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","  (4): GroupNorm(32, 64, eps=1e-05, affine=True)\n","  (5): ReLU(inplace=True)\n","  (6): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","  (7): ODEBlock(\n","    (odefunc): ODEfunc(\n","      (norm1): GroupNorm(32, 64, eps=1e-05, affine=True)\n","      (relu): ReLU(inplace=True)\n","      (conv1): ConcatConv2d(\n","        (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","      (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)\n","      (conv2): ConcatConv2d(\n","        (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","      (norm3): GroupNorm(32, 64, eps=1e-05, affine=True)\n","    )\n","  )\n","  (8): GroupNorm(32, 64, eps=1e-05, affine=True)\n","  (9): ReLU(inplace=True)\n","  (10): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (11): Flatten()\n","  (12): Linear(in_features=64, out_features=10, bias=True)\n",")\n","Sequential(\n","  (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1))\n","  (1): GroupNorm(32, 64, eps=1e-05, affine=True)\n","  (2): ReLU(inplace=True)\n","  (3): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","  (4): GroupNorm(32, 64, eps=1e-05, affine=True)\n","  (5): ReLU(inplace=True)\n","  (6): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","  (7): ODEBlock(\n","    (odefunc): ODEfunc(\n","      (norm1): GroupNorm(32, 64, eps=1e-05, affine=True)\n","      (relu): ReLU(inplace=True)\n","      (conv1): ConcatConv2d(\n","        (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","      (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)\n","      (conv2): ConcatConv2d(\n","        (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","      (norm3): GroupNorm(32, 64, eps=1e-05, affine=True)\n","    )\n","  )\n","  (8): GroupNorm(32, 64, eps=1e-05, affine=True)\n","  (9): ReLU(inplace=True)\n","  (10): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (11): Flatten()\n","  (12): Linear(in_features=64, out_features=10, bias=True)\n",")\n","Sequential(\n","  (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1))\n","  (1): GroupNorm(32, 64, eps=1e-05, affine=True)\n","  (2): ReLU(inplace=True)\n","  (3): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","  (4): GroupNorm(32, 64, eps=1e-05, affine=True)\n","  (5): ReLU(inplace=True)\n","  (6): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","  (7): ODEBlock(\n","    (odefunc): ODEfunc(\n","      (norm1): GroupNorm(32, 64, eps=1e-05, affine=True)\n","      (relu): ReLU(inplace=True)\n","      (conv1): ConcatConv2d(\n","        (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","      (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)\n","      (conv2): ConcatConv2d(\n","        (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","      (norm3): GroupNorm(32, 64, eps=1e-05, affine=True)\n","    )\n","  )\n","  (8): GroupNorm(32, 64, eps=1e-05, affine=True)\n","  (9): ReLU(inplace=True)\n","  (10): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (11): Flatten()\n","  (12): Linear(in_features=64, out_features=10, bias=True)\n",")\n","Sequential(\n","  (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1))\n","  (1): GroupNorm(32, 64, eps=1e-05, affine=True)\n","  (2): ReLU(inplace=True)\n","  (3): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","  (4): GroupNorm(32, 64, eps=1e-05, affine=True)\n","  (5): ReLU(inplace=True)\n","  (6): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","  (7): ODEBlock(\n","    (odefunc): ODEfunc(\n","      (norm1): GroupNorm(32, 64, eps=1e-05, affine=True)\n","      (relu): ReLU(inplace=True)\n","      (conv1): ConcatConv2d(\n","        (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","      (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)\n","      (conv2): ConcatConv2d(\n","        (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","      (norm3): GroupNorm(32, 64, eps=1e-05, affine=True)\n","    )\n","  )\n","  (8): GroupNorm(32, 64, eps=1e-05, affine=True)\n","  (9): ReLU(inplace=True)\n","  (10): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (11): Flatten()\n","  (12): Linear(in_features=64, out_features=10, bias=True)\n",")\n","Sequential(\n","  (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1))\n","  (1): GroupNorm(32, 64, eps=1e-05, affine=True)\n","  (2): ReLU(inplace=True)\n","  (3): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","  (4): GroupNorm(32, 64, eps=1e-05, affine=True)\n","  (5): ReLU(inplace=True)\n","  (6): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","  (7): ODEBlock(\n","    (odefunc): ODEfunc(\n","      (norm1): GroupNorm(32, 64, eps=1e-05, affine=True)\n","      (relu): ReLU(inplace=True)\n","      (conv1): ConcatConv2d(\n","        (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","      (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)\n","      (conv2): ConcatConv2d(\n","        (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","      (norm3): GroupNorm(32, 64, eps=1e-05, affine=True)\n","    )\n","  )\n","  (8): GroupNorm(32, 64, eps=1e-05, affine=True)\n","  (9): ReLU(inplace=True)\n","  (10): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (11): Flatten()\n","  (12): Linear(in_features=64, out_features=10, bias=True)\n",")\n","Number of parameters: 208266\n","Number of parameters: 208266\n","Number of parameters: 208266\n","Number of parameters: 208266\n","Number of parameters: 208266\n","Number of parameters: 208266\n"],"name":"stderr"},{"output_type":"stream","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to .data/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"950091007f4b44d9a71db065b0adb84d","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=9912422.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     train_loader, test_loader, train_eval_loader = get_mnist_loaders(\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_aug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     )\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\u001b[0m in \u001b[0;36mget_mnist_loaders\u001b[0;34m(data_aug, batch_size, test_batch_size, perc)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     train_loader = DataLoader(\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'.data/mnist'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    156\u001b[0m                         \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_root\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_folder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m                         \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m                         \u001b[0mmd5\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmd5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m                     )\n\u001b[1;32m    160\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mURLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_and_extract_archive\u001b[0;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     \u001b[0mdownload_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0marchive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_url\u001b[0;34m(url, root, filename, md5, max_redirect_hops)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Downloading '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0m_urlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mURLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'https'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36m_urlretrieve\u001b[0;34m(url, filename, chunk_size)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"User-Agent\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUSER_AGENT\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"User-Agent\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUSER_AGENT\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g2k3GwQeJsNy","executionInfo":{"status":"ok","timestamp":1617963867638,"user_tz":-60,"elapsed":420,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}},"outputId":"78f13612-fb4b-4d78-a6f9-3c951d4848a1"},"source":["a = os.path.abspath('')\n","print(type(a))"],"execution_count":35,"outputs":[{"output_type":"stream","text":["<class 'str'>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_jhUzE0XKh_C","executionInfo":{"status":"ok","timestamp":1617964280066,"user_tz":-60,"elapsed":1044,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["__file__ = '/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb'\n","makedirs(args.save)"],"execution_count":52,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ner61Yv1L9ZC","executionInfo":{"status":"ok","timestamp":1617964311135,"user_tz":-60,"elapsed":462,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}},"outputId":"e99b8201-546c-478b-b3c1-5fe19c32a2bb"},"source":["%%bash\n","cd '/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/'\n","ls"],"execution_count":55,"outputs":[{"output_type":"stream","text":["mnist_node.ipynb\n"],"name":"stdout"}]}]}
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.1, nepochs=120, network='odenet', save='/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/experiment_node1', test_batch_size=1000, tol=0.001)
Sequential(
  (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1))
  (1): GroupNorm(32, 64, eps=1e-05, affine=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): GroupNorm(32, 64, eps=1e-05, affine=True)
  (5): ReLU(inplace=True)
  (6): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (norm1): GroupNorm(32, 64, eps=1e-05, affine=True)
      (relu): ReLU(inplace=True)
      (conv1): ConcatConv2d(
        (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)
      (conv2): ConcatConv2d(
        (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (norm3): GroupNorm(32, 64, eps=1e-05, affine=True)
    )
  )
  (8): GroupNorm(32, 64, eps=1e-05, affine=True)
  (9): ReLU(inplace=True)
  (10): AdaptiveAvgPool2d(output_size=(1, 1))
  (11): Flatten()
  (12): Linear(in_features=64, out_features=10, bias=True)
)
Number of parameters: 208266
/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb
/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mnist_node.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1sLUaZ_DEMKH30zwclvQKQ2FNZUI_mF3K","authorship_tag":"ABX9TyOONKimm0nld99hTwlQ4+bc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"sWmsXi5BIAtb","executionInfo":{"status":"ok","timestamp":1617963280508,"user_tz":-60,"elapsed":4957,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["%%capture\n","%%bash\n","pip install torchdiffeq"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"H353RG8fIDd3","executionInfo":{"status":"ok","timestamp":1617964406284,"user_tz":-60,"elapsed":427,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["import os\n","import argparse\n","import logging\n","import time\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","\n","\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')\n","parser.add_argument('--tol', type=float, default=1e-3)\n","parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])\n","parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])\n","parser.add_argument('--nepochs', type=int, default=120)\n","parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])\n","parser.add_argument('--lr', type=float, default=0.1)\n","parser.add_argument('--batch_size', type=int, default=128)\n","parser.add_argument('--test_batch_size', type=int, default=1000)\n","\n","parser.add_argument('--save', type=str, default='./experiment_node1')\n","parser.add_argument('--gpu', type=int, default=0)\n","parser.add_argument('--debug', action='store_true')\n","args = parser.parse_args(args=[])\n","\n","args.save = '/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/experiment_node1'\n","\n","if args.adjoint:\n","    from torchdiffeq import odeint_adjoint as odeint\n","else:\n","    from torchdiffeq import odeint"],"execution_count":59,"outputs":[]},{"cell_type":"code","metadata":{"id":"agpZtaKwIKnA","executionInfo":{"status":"ok","timestamp":1617963497293,"user_tz":-60,"elapsed":412,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["def conv3x3(in_planes, out_planes, stride=1):\n","    \"\"\"3x3 convolution with padding\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","\n","def conv1x1(in_planes, out_planes, stride=1):\n","    \"\"\"1x1 convolution\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n","\n","\n","def norm(dim):\n","    return nn.GroupNorm(min(32, dim), dim)\n","\n","\n","class ResBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None):\n","        super(ResBlock, self).__init__()\n","        self.norm1 = norm(inplanes)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.downsample = downsample\n","        self.conv1 = conv3x3(inplanes, planes, stride)\n","        self.norm2 = norm(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","\n","    def forward(self, x):\n","        shortcut = x\n","\n","        out = self.relu(self.norm1(x))\n","\n","        if self.downsample is not None:\n","            shortcut = self.downsample(out)\n","\n","        out = self.conv1(out)\n","        out = self.norm2(out)\n","        out = self.relu(out)\n","        out = self.conv2(out)\n","\n","        return out + shortcut\n","\n","\n","class ConcatConv2d(nn.Module):\n","\n","    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):\n","        super(ConcatConv2d, self).__init__()\n","        module = nn.ConvTranspose2d if transpose else nn.Conv2d\n","        self._layer = module(\n","            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,\n","            bias=bias\n","        )\n","\n","    def forward(self, t, x):\n","        tt = torch.ones_like(x[:, :1, :, :]) * t\n","        ttx = torch.cat([tt, x], 1)\n","        return self._layer(ttx)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"1-rlM_3sI-1k","executionInfo":{"status":"ok","timestamp":1617963513110,"user_tz":-60,"elapsed":974,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["class ODEfunc(nn.Module):\n","\n","    def __init__(self, dim):\n","        super(ODEfunc, self).__init__()\n","        self.norm1 = norm(dim)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv1 = ConcatConv2d(dim, dim, 3, 1, 1)\n","        self.norm2 = norm(dim)\n","        self.conv2 = ConcatConv2d(dim, dim, 3, 1, 1)\n","        self.norm3 = norm(dim)\n","        self.nfe = 0\n","\n","    def forward(self, t, x):\n","        self.nfe += 1\n","        out = self.norm1(x)\n","        out = self.relu(out)\n","        out = self.conv1(t, out)\n","        out = self.norm2(out)\n","        out = self.relu(out)\n","        out = self.conv2(t, out)\n","        out = self.norm3(out)\n","        return out\n","\n","\n","class ODEBlock(nn.Module):\n","\n","    def __init__(self, odefunc):\n","        super(ODEBlock, self).__init__()\n","        self.odefunc = odefunc\n","        self.integration_time = torch.tensor([0, 1]).float()\n","\n","    def forward(self, x):\n","        self.integration_time = self.integration_time.type_as(x)\n","        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)\n","        return out[1]\n","\n","    @property\n","    def nfe(self):\n","        return self.odefunc.nfe\n","\n","    @nfe.setter\n","    def nfe(self, value):\n","        self.odefunc.nfe = value"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"xhFiCzNaJC0x","executionInfo":{"status":"ok","timestamp":1617963529496,"user_tz":-60,"elapsed":991,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["class Flatten(nn.Module):\n","\n","    def __init__(self):\n","        super(Flatten, self).__init__()\n","\n","    def forward(self, x):\n","        shape = torch.prod(torch.tensor(x.shape[1:])).item()\n","        return x.view(-1, shape)\n","\n","\n","class RunningAverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","\n","    def __init__(self, momentum=0.99):\n","        self.momentum = momentum\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = None\n","        self.avg = 0\n","\n","    def update(self, val):\n","        if self.val is None:\n","            self.avg = val\n","        else:\n","            self.avg = self.avg * self.momentum + val * (1 - self.momentum)\n","        self.val = val"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"ltcgl1RIJG7U","executionInfo":{"status":"ok","timestamp":1617963610116,"user_tz":-60,"elapsed":397,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["def get_mnist_loaders(data_aug=False, batch_size=128, test_batch_size=1000, perc=1.0):\n","    if data_aug:\n","        transform_train = transforms.Compose([\n","            transforms.RandomCrop(28, padding=4),\n","            transforms.ToTensor(),\n","        ])\n","    else:\n","        transform_train = transforms.Compose([\n","            transforms.ToTensor(),\n","        ])\n","\n","    transform_test = transforms.Compose([\n","        transforms.ToTensor(),\n","    ])\n","\n","    train_loader = DataLoader(\n","        datasets.MNIST(root='.data/mnist', train=True, download=True, transform=transform_train), batch_size=batch_size,\n","        shuffle=True, num_workers=2, drop_last=True\n","    )\n","\n","    train_eval_loader = DataLoader(\n","        datasets.MNIST(root='.data/mnist', train=True, download=True, transform=transform_test),\n","        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True\n","    )\n","\n","    test_loader = DataLoader(\n","        datasets.MNIST(root='.data/mnist', train=False, download=True, transform=transform_test),\n","        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True\n","    )\n","\n","    return train_loader, test_loader, train_eval_loader\n","\n","\n","def inf_generator(iterable):\n","    \"\"\"Allows training with DataLoaders in a single infinite loop:\n","        for i, (x, y) in enumerate(inf_generator(train_loader)):\n","    \"\"\"\n","    iterator = iterable.__iter__()\n","    while True:\n","        try:\n","            yield iterator.__next__()\n","        except StopIteration:\n","            iterator = iterable.__iter__()\n","\n","\n","def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):\n","    initial_learning_rate = args.lr * batch_size / batch_denom\n","\n","    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]\n","    vals = [initial_learning_rate * decay for decay in decay_rates]\n","\n","    def learning_rate_fn(itr):\n","        lt = [itr < b for b in boundaries] + [True]\n","        i = np.argmax(lt)\n","        return vals[i]\n","\n","    return learning_rate_fn\n","\n","\n","def one_hot(x, K):\n","    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)\n","\n","\n","def accuracy(model, dataset_loader):\n","    total_correct = 0\n","    for x, y in dataset_loader:\n","        x = x.to(device)\n","        y = one_hot(np.array(y.numpy()), 10)\n","\n","        target_class = np.argmax(y, axis=1)\n","        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)\n","        total_correct += np.sum(predicted_class == target_class)\n","    return total_correct / len(dataset_loader.dataset)\n","\n","\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","\n","def makedirs(dirname):\n","    if not os.path.exists(dirname):\n","        os.makedirs(dirname)\n","\n","\n","def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):\n","    logger = logging.getLogger()\n","    if debug:\n","        level = logging.DEBUG\n","    else:\n","        level = logging.INFO\n","    logger.setLevel(level)\n","    if saving:\n","        info_file_handler = logging.FileHandler(logpath, mode=\"a\")\n","        info_file_handler.setLevel(level)\n","        logger.addHandler(info_file_handler)\n","    if displaying:\n","        console_handler = logging.StreamHandler()\n","        console_handler.setLevel(level)\n","        logger.addHandler(console_handler)\n","    logger.info(filepath)\n","    with open(filepath, \"r\") as f:\n","        logger.info(f.read())\n","\n","    for f in package_files:\n","        logger.info(f)\n","        with open(f, \"r\") as package_f:\n","            logger.info(package_f.read())\n","\n","    return logger\n"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"42RIwJ37JKQr"},"source":["if __name__ == '__main__':\n","    __file__ = '/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb'\n","    makedirs(args.save)\n","    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))\n","    logger.info(args)\n","\n","    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')\n","\n","    is_odenet = args.network == 'odenet'\n","\n","    if args.downsampling_method == 'conv':\n","        downsampling_layers = [\n","            nn.Conv2d(1, 64, 3, 1),\n","            norm(64),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(64, 64, 4, 2, 1),\n","            norm(64),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(64, 64, 4, 2, 1),\n","        ]\n","    elif args.downsampling_method == 'res':\n","        downsampling_layers = [\n","            nn.Conv2d(1, 64, 3, 1),\n","            ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\n","            ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\n","        ]\n","\n","    feature_layers = [ODEBlock(ODEfunc(64))] if is_odenet else [ResBlock(64, 64) for _ in range(6)]\n","    fc_layers = [norm(64), nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d((1, 1)), Flatten(), nn.Linear(64, 10)]\n","\n","    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)\n","\n","    logger.info(model)\n","    logger.info('Number of parameters: {}'.format(count_parameters(model)))\n","\n","    criterion = nn.CrossEntropyLoss().to(device)\n","\n","    train_loader, test_loader, train_eval_loader = get_mnist_loaders(\n","        args.data_aug, args.batch_size, args.test_batch_size\n","    )\n","\n","    data_gen = inf_generator(train_loader)\n","    batches_per_epoch = len(train_loader)\n","\n","    lr_fn = learning_rate_with_decay(\n","        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],\n","        decay_rates=[1, 0.1, 0.01, 0.001]\n","    )\n","\n","    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)\n","\n","    best_acc = 0\n","    batch_time_meter = RunningAverageMeter()\n","    f_nfe_meter = RunningAverageMeter()\n","    b_nfe_meter = RunningAverageMeter()\n","    end = time.time()\n","    \n","    epoch_arr = []\n","    time_val_arr = []\n","    time_avg_arr = []\n","    nfe_f_arr = []\n","    nfe_b_arr = []\n","    train_acc_arr = []\n","    test_acc_arr = []\n","\n","    for itr in range(args.nepochs * batches_per_epoch):\n","\n","        for param_group in optimizer.param_groups:\n","            param_group['lr'] = lr_fn(itr)\n","\n","        optimizer.zero_grad()\n","        x, y = data_gen.__next__()\n","        x = x.to(device)\n","        y = y.to(device)\n","        logits = model(x)\n","        loss = criterion(logits, y)\n","\n","        \n","\n","        if is_odenet:\n","            nfe_forward = feature_layers[0].nfe\n","            feature_layers[0].nfe = 0\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        if is_odenet:\n","            nfe_backward = feature_layers[0].nfe\n","            feature_layers[0].nfe = 0\n","\n","        batch_time_meter.update(time.time() - end)\n","        if is_odenet:\n","            f_nfe_meter.update(nfe_forward)\n","            b_nfe_meter.update(nfe_backward)\n","        end = time.time()\n","\n","        if itr % batches_per_epoch == 0:\n","            with torch.no_grad():\n","                train_acc = accuracy(model, train_eval_loader)\n","                val_acc = accuracy(model, test_loader)\n","                if val_acc > best_acc:\n","                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))\n","                    best_acc = val_acc\n","                logger.info(\n","                    \"Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | \"\n","                    \"Train Acc {:.4f} | Test Acc {:.4f}\".format(\n","                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,\n","                        b_nfe_meter.avg, train_acc, val_acc\n","                    )\n","                )\n","                epoch_arr += [itr // batches_per_epoch]\n","                time_val_arr += [batch_time_meter.val]\n","                time_avg_arr += [batch_time_meter.avg]\n","                nfe_f_arr += [f_nfe_meter.avg]\n","                nfe_b_arr += [b_nfe_meter.avg]\n","                train_acc_arr += [train_acc]\n","                test_acc_arr += [val_acc]\n","                    \n","    epoch_arr = np.asarray(epoch_arr)\n","    time_val_arr = np.asarray(time_val_arr)\n","    time_avg_arr = np.asarray(time_avg_arr)\n","    nfe_f_arr = np.asarray(nfe_f_arr)\n","    nfe_b_arr = np.asarray(nfe_b_arr)\n","    train_acc_arr = np.asarray(train_acc_arr)\n","    test_acc_arr = np.asarray(test_acc_arr)\n","    \n","    np.save(os.path.join(args.save, 'epoch_arr.npy'), epoch_arr)\n","    np.save(os.path.join(args.save, 'time_val_arr.npy'), time_val_arr)\n","    np.save(os.path.join(args.save, 'time_avg_arr.npy'), time_avg_arr)\n","    np.save(os.path.join(args.save, 'nfe_f_arr.npy'), nfe_f_arr)\n","    np.save(os.path.join(args.save, 'nfe_b_arr.npy'), nfe_b_arr)\n","    np.save(os.path.join(args.save, 'train_acc_arr.npy'), train_acc_arr)\n","    np.save(os.path.join(args.save, 'test_acc_arr.npy'), test_acc_arr)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ner61Yv1L9ZC","executionInfo":{"status":"ok","timestamp":1617964389098,"user_tz":-60,"elapsed":1452,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["%%bash\n","rm -r /content/experiment_node1"],"execution_count":58,"outputs":[]},{"cell_type":"code","metadata":{"id":"gl0nftCJOgzw"},"source":["\"\"\"\n","--------------------------------------------------------------------------------------------------------------------------------------------\n","The code for Hessian analysis.\n","--------------------------------------------------------------------------------------------------------------------------------------------\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m75L8B4eOjXq","executionInfo":{"status":"ok","timestamp":1617964978011,"user_tz":-60,"elapsed":441,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["def get_manual_hessian(grads, parameters, show_iters=True):\n","  \"\"\"\n","  Calculation of the Hessian using nested for loops.\n","  Inputs:   - grads:        tuple of gradient tensors. Created using something \n","                            like grads = torch.autograd.grad(loss, parameters, create_graph=True).\n","            - parameters:   List of parameter objects. Created using something \n","                            like parameters = optimizer.param_groups[0]['params'].\n","            - show_iters:   True or False, depending on if the iteration number is to be shown during training. \n","                            Note that the iteration updates are not provided every row, but instead periodically \n","                            (roughly according to the number of parameters in the system).\n","  \"\"\"\n","  start = time.time()        \n","\n","  n_params = 0\n","  for param in parameters:\n","    n_params += torch.numel(param)\n","  grads2 = torch.zeros(n_params,n_params)            #Create an matrix of zeros thas has the same shape as the Hessian.\n","\n","  y_counter = 0                             #y_direction refers to row number in the Hessian.\n","\n","  for grad in grads:\n","      grad = torch.reshape(grad, [-1])                                  #Rearrange the gradient information into a vector.        \n","\n","      for j, g in enumerate(grad):\n","        x_counter = 0                                                   #x_direction refers to column number in the Hessian.\n","\n","        for l, param in enumerate(parameters):\n","          g2 = torch.autograd.grad(g, param, retain_graph=True)[0]      #Calculate the gradient of an element of the gradient wrt one layer's parameters.\n","          g2 = torch.reshape(g2, [-1])                                  #Reshape this into a vector.\n","          len = g2.shape[0]                       \n","          grads2[j+y_counter, x_counter:x_counter+len] = g2             #Indexing ensures that the second order derivatives are placed in the correct positions.\n","          x_counter += len\n","\n","      grads2 = grads2.to(device)\n","      y_counter += grad.shape[0]\n","\n","      if show_iters:\n","        print(\"Gradients calculated for row number \" + str(y_counter) + \".\")\n","  \n","  print('Time used was ', time.time() - start)\n","\n","  return grads2"],"execution_count":65,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":426},"id":"G8w3mIvwOlz5","executionInfo":{"status":"error","timestamp":1617965363914,"user_tz":-60,"elapsed":699,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}},"outputId":"d0dcab47-382d-40d7-f612-eb3119ceb615"},"source":["train_loader, test_loader, train_eval_loader = get_mnist_loaders(\n","        args.data_aug, args.batch_size, args.test_batch_size)\n","\n","optimizer = torch.optim.SGD(model.parameters(), lr=1)\n","optimizer.zero_grad()\n","\n","criterion = nn.CrossEntropyLoss().to(device)\n","for param_group in optimizer.param_groups:\n","            param_group['lr'] = lr_fn(itr)\n","\n","optimizer.zero_grad()\n","x, y = data_gen.__next__()\n","x = x.to(device)\n","y = y.to(device)\n","logits = model(x)\n","loss = criterion(logits, y)\n","\n","target = torch.zeros(len(zN))\n","for i in range(len(zN)):\n","  if zN[i]==0:\n","    target[i] = 0\n","  else:\n","    target[i] = 1\n","target = target.long()\n","\n","loss = criterion(pred_y, target)\n","grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n","parameters = optimizer.param_groups[0]['params']\n","\n","print('Obtaining manual hessian...')\n","manual_hessian = get_manual_hessian(grads, parameters)           #get manual hessian."],"execution_count":67,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# unserialize the data after having released the lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/reductions.py\u001b[0m in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrebuild_storage_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m     \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mdetach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;34m'''Get the fd.  This should only be called once.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0m_resource_sharer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mget_connection\u001b[0;34m(ident)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mident\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mClient\u001b[0;34m(address, family, authkey)\u001b[0m\n\u001b[1;32m    491\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSocketClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mSocketClient\u001b[0;34m(address)\u001b[0m\n\u001b[1;32m    619\u001b[0m         \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetblocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m         \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mConnection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\u001b[0m in \u001b[0;36minf_generator\u001b[0;34m(iterable)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1183\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1146\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1149\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    997\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m                 \u001b[0mpids_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DataLoader worker (pid(s) {}) exited unexpectedly'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpids_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmpty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 448, 449) exited unexpectedly"]}]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mnist_node.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1sLUaZ_DEMKH30zwclvQKQ2FNZUI_mF3K","authorship_tag":"ABX9TyOONKimm0nld99hTwlQ4+bc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"sWmsXi5BIAtb","executionInfo":{"status":"ok","timestamp":1617963280508,"user_tz":-60,"elapsed":4957,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["%%capture\n","%%bash\n","pip install torchdiffeq"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"H353RG8fIDd3","executionInfo":{"status":"ok","timestamp":1617964406284,"user_tz":-60,"elapsed":427,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["import os\n","import argparse\n","import logging\n","import time\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","\n","\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')\n","parser.add_argument('--tol', type=float, default=1e-3)\n","parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])\n","parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])\n","parser.add_argument('--nepochs', type=int, default=120)\n","parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])\n","parser.add_argument('--lr', type=float, default=0.1)\n","parser.add_argument('--batch_size', type=int, default=128)\n","parser.add_argument('--test_batch_size', type=int, default=1000)\n","\n","parser.add_argument('--save', type=str, default='./experiment_node1')\n","parser.add_argument('--gpu', type=int, default=0)\n","parser.add_argument('--debug', action='store_true')\n","args = parser.parse_args(args=[])\n","\n","args.save = '/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/experiment_node1'\n","\n","if args.adjoint:\n","    from torchdiffeq import odeint_adjoint as odeint\n","else:\n","    from torchdiffeq import odeint"],"execution_count":59,"outputs":[]},{"cell_type":"code","metadata":{"id":"agpZtaKwIKnA","executionInfo":{"status":"ok","timestamp":1617963497293,"user_tz":-60,"elapsed":412,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["def conv3x3(in_planes, out_planes, stride=1):\n","    \"\"\"3x3 convolution with padding\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","\n","def conv1x1(in_planes, out_planes, stride=1):\n","    \"\"\"1x1 convolution\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n","\n","\n","def norm(dim):\n","    return nn.GroupNorm(min(32, dim), dim)\n","\n","\n","class ResBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None):\n","        super(ResBlock, self).__init__()\n","        self.norm1 = norm(inplanes)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.downsample = downsample\n","        self.conv1 = conv3x3(inplanes, planes, stride)\n","        self.norm2 = norm(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","\n","    def forward(self, x):\n","        shortcut = x\n","\n","        out = self.relu(self.norm1(x))\n","\n","        if self.downsample is not None:\n","            shortcut = self.downsample(out)\n","\n","        out = self.conv1(out)\n","        out = self.norm2(out)\n","        out = self.relu(out)\n","        out = self.conv2(out)\n","\n","        return out + shortcut\n","\n","\n","class ConcatConv2d(nn.Module):\n","\n","    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):\n","        super(ConcatConv2d, self).__init__()\n","        module = nn.ConvTranspose2d if transpose else nn.Conv2d\n","        self._layer = module(\n","            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,\n","            bias=bias\n","        )\n","\n","    def forward(self, t, x):\n","        tt = torch.ones_like(x[:, :1, :, :]) * t\n","        ttx = torch.cat([tt, x], 1)\n","        return self._layer(ttx)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"1-rlM_3sI-1k","executionInfo":{"status":"ok","timestamp":1617963513110,"user_tz":-60,"elapsed":974,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["class ODEfunc(nn.Module):\n","\n","    def __init__(self, dim):\n","        super(ODEfunc, self).__init__()\n","        self.norm1 = norm(dim)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv1 = ConcatConv2d(dim, dim, 3, 1, 1)\n","        self.norm2 = norm(dim)\n","        self.conv2 = ConcatConv2d(dim, dim, 3, 1, 1)\n","        self.norm3 = norm(dim)\n","        self.nfe = 0\n","\n","    def forward(self, t, x):\n","        self.nfe += 1\n","        out = self.norm1(x)\n","        out = self.relu(out)\n","        out = self.conv1(t, out)\n","        out = self.norm2(out)\n","        out = self.relu(out)\n","        out = self.conv2(t, out)\n","        out = self.norm3(out)\n","        return out\n","\n","\n","class ODEBlock(nn.Module):\n","\n","    def __init__(self, odefunc):\n","        super(ODEBlock, self).__init__()\n","        self.odefunc = odefunc\n","        self.integration_time = torch.tensor([0, 1]).float()\n","\n","    def forward(self, x):\n","        self.integration_time = self.integration_time.type_as(x)\n","        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)\n","        return out[1]\n","\n","    @property\n","    def nfe(self):\n","        return self.odefunc.nfe\n","\n","    @nfe.setter\n","    def nfe(self, value):\n","        self.odefunc.nfe = value"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"xhFiCzNaJC0x","executionInfo":{"status":"ok","timestamp":1617963529496,"user_tz":-60,"elapsed":991,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["class Flatten(nn.Module):\n","\n","    def __init__(self):\n","        super(Flatten, self).__init__()\n","\n","    def forward(self, x):\n","        shape = torch.prod(torch.tensor(x.shape[1:])).item()\n","        return x.view(-1, shape)\n","\n","\n","class RunningAverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","\n","    def __init__(self, momentum=0.99):\n","        self.momentum = momentum\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = None\n","        self.avg = 0\n","\n","    def update(self, val):\n","        if self.val is None:\n","            self.avg = val\n","        else:\n","            self.avg = self.avg * self.momentum + val * (1 - self.momentum)\n","        self.val = val"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"ltcgl1RIJG7U","executionInfo":{"status":"ok","timestamp":1617963610116,"user_tz":-60,"elapsed":397,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["def get_mnist_loaders(data_aug=False, batch_size=128, test_batch_size=1000, perc=1.0):\n","    if data_aug:\n","        transform_train = transforms.Compose([\n","            transforms.RandomCrop(28, padding=4),\n","            transforms.ToTensor(),\n","        ])\n","    else:\n","        transform_train = transforms.Compose([\n","            transforms.ToTensor(),\n","        ])\n","\n","    transform_test = transforms.Compose([\n","        transforms.ToTensor(),\n","    ])\n","\n","    train_loader = DataLoader(\n","        datasets.MNIST(root='.data/mnist', train=True, download=True, transform=transform_train), batch_size=batch_size,\n","        shuffle=True, num_workers=2, drop_last=True\n","    )\n","\n","    train_eval_loader = DataLoader(\n","        datasets.MNIST(root='.data/mnist', train=True, download=True, transform=transform_test),\n","        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True\n","    )\n","\n","    test_loader = DataLoader(\n","        datasets.MNIST(root='.data/mnist', train=False, download=True, transform=transform_test),\n","        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True\n","    )\n","\n","    return train_loader, test_loader, train_eval_loader\n","\n","\n","def inf_generator(iterable):\n","    \"\"\"Allows training with DataLoaders in a single infinite loop:\n","        for i, (x, y) in enumerate(inf_generator(train_loader)):\n","    \"\"\"\n","    iterator = iterable.__iter__()\n","    while True:\n","        try:\n","            yield iterator.__next__()\n","        except StopIteration:\n","            iterator = iterable.__iter__()\n","\n","\n","def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):\n","    initial_learning_rate = args.lr * batch_size / batch_denom\n","\n","    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]\n","    vals = [initial_learning_rate * decay for decay in decay_rates]\n","\n","    def learning_rate_fn(itr):\n","        lt = [itr < b for b in boundaries] + [True]\n","        i = np.argmax(lt)\n","        return vals[i]\n","\n","    return learning_rate_fn\n","\n","\n","def one_hot(x, K):\n","    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)\n","\n","\n","def accuracy(model, dataset_loader):\n","    total_correct = 0\n","    for x, y in dataset_loader:\n","        x = x.to(device)\n","        y = one_hot(np.array(y.numpy()), 10)\n","\n","        target_class = np.argmax(y, axis=1)\n","        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)\n","        total_correct += np.sum(predicted_class == target_class)\n","    return total_correct / len(dataset_loader.dataset)\n","\n","\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","\n","def makedirs(dirname):\n","    if not os.path.exists(dirname):\n","        os.makedirs(dirname)\n","\n","\n","def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):\n","    logger = logging.getLogger()\n","    if debug:\n","        level = logging.DEBUG\n","    else:\n","        level = logging.INFO\n","    logger.setLevel(level)\n","    if saving:\n","        info_file_handler = logging.FileHandler(logpath, mode=\"a\")\n","        info_file_handler.setLevel(level)\n","        logger.addHandler(info_file_handler)\n","    if displaying:\n","        console_handler = logging.StreamHandler()\n","        console_handler.setLevel(level)\n","        logger.addHandler(console_handler)\n","    logger.info(filepath)\n","    with open(filepath, \"r\") as f:\n","        logger.info(f.read())\n","\n","    for f in package_files:\n","        logger.info(f)\n","        with open(f, \"r\") as package_f:\n","            logger.info(package_f.read())\n","\n","    return logger\n"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"42RIwJ37JKQr"},"source":["if __name__ == '__main__':\n","    __file__ = '/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb'\n","    makedirs(args.save)\n","    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))\n","    logger.info(args)\n","\n","    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')\n","\n","    is_odenet = args.network == 'odenet'\n","\n","    if args.downsampling_method == 'conv':\n","        downsampling_layers = [\n","            nn.Conv2d(1, 64, 3, 1),\n","            norm(64),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(64, 64, 4, 2, 1),\n","            norm(64),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(64, 64, 4, 2, 1),\n","        ]\n","    elif args.downsampling_method == 'res':\n","        downsampling_layers = [\n","            nn.Conv2d(1, 64, 3, 1),\n","            ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\n","            ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\n","        ]\n","\n","    feature_layers = [ODEBlock(ODEfunc(64))] if is_odenet else [ResBlock(64, 64) for _ in range(6)]\n","    fc_layers = [norm(64), nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d((1, 1)), Flatten(), nn.Linear(64, 10)]\n","\n","    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)\n","\n","    logger.info(model)\n","    logger.info('Number of parameters: {}'.format(count_parameters(model)))\n","\n","    criterion = nn.CrossEntropyLoss().to(device)\n","\n","    train_loader, test_loader, train_eval_loader = get_mnist_loaders(\n","        args.data_aug, args.batch_size, args.test_batch_size\n","    )\n","\n","    data_gen = inf_generator(train_loader)\n","    batches_per_epoch = len(train_loader)\n","\n","    lr_fn = learning_rate_with_decay(\n","        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],\n","        decay_rates=[1, 0.1, 0.01, 0.001]\n","    )\n","\n","    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)\n","\n","    best_acc = 0\n","    batch_time_meter = RunningAverageMeter()\n","    f_nfe_meter = RunningAverageMeter()\n","    b_nfe_meter = RunningAverageMeter()\n","    end = time.time()\n","    \n","    epoch_arr = []\n","    time_val_arr = []\n","    time_avg_arr = []\n","    nfe_f_arr = []\n","    nfe_b_arr = []\n","    train_acc_arr = []\n","    test_acc_arr = []\n","\n","    for itr in range(args.nepochs * batches_per_epoch):\n","\n","        for param_group in optimizer.param_groups:\n","            param_group['lr'] = lr_fn(itr)\n","\n","        optimizer.zero_grad()\n","        x, y = data_gen.__next__()\n","        x = x.to(device)\n","        y = y.to(device)\n","        logits = model(x)\n","        loss = criterion(logits, y)\n","\n","        \n","\n","        if is_odenet:\n","            nfe_forward = feature_layers[0].nfe\n","            feature_layers[0].nfe = 0\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        if is_odenet:\n","            nfe_backward = feature_layers[0].nfe\n","            feature_layers[0].nfe = 0\n","\n","        batch_time_meter.update(time.time() - end)\n","        if is_odenet:\n","            f_nfe_meter.update(nfe_forward)\n","            b_nfe_meter.update(nfe_backward)\n","        end = time.time()\n","\n","        if itr % batches_per_epoch == 0:\n","            with torch.no_grad():\n","                train_acc = accuracy(model, train_eval_loader)\n","                val_acc = accuracy(model, test_loader)\n","                if val_acc > best_acc:\n","                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))\n","                    best_acc = val_acc\n","                logger.info(\n","                    \"Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | \"\n","                    \"Train Acc {:.4f} | Test Acc {:.4f}\".format(\n","                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,\n","                        b_nfe_meter.avg, train_acc, val_acc\n","                    )\n","                )\n","                epoch_arr += [itr // batches_per_epoch]\n","                time_val_arr += [batch_time_meter.val]\n","                time_avg_arr += [batch_time_meter.avg]\n","                nfe_f_arr += [f_nfe_meter.avg]\n","                nfe_b_arr += [b_nfe_meter.avg]\n","                train_acc_arr += [train_acc]\n","                test_acc_arr += [val_acc]\n","                    \n","    epoch_arr = np.asarray(epoch_arr)\n","    time_val_arr = np.asarray(time_val_arr)\n","    time_avg_arr = np.asarray(time_avg_arr)\n","    nfe_f_arr = np.asarray(nfe_f_arr)\n","    nfe_b_arr = np.asarray(nfe_b_arr)\n","    train_acc_arr = np.asarray(train_acc_arr)\n","    test_acc_arr = np.asarray(test_acc_arr)\n","    \n","    np.save(os.path.join(args.save, 'epoch_arr.npy'), epoch_arr)\n","    np.save(os.path.join(args.save, 'time_val_arr.npy'), time_val_arr)\n","    np.save(os.path.join(args.save, 'time_avg_arr.npy'), time_avg_arr)\n","    np.save(os.path.join(args.save, 'nfe_f_arr.npy'), nfe_f_arr)\n","    np.save(os.path.join(args.save, 'nfe_b_arr.npy'), nfe_b_arr)\n","    np.save(os.path.join(args.save, 'train_acc_arr.npy'), train_acc_arr)\n","    np.save(os.path.join(args.save, 'test_acc_arr.npy'), test_acc_arr)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ner61Yv1L9ZC","executionInfo":{"status":"ok","timestamp":1617964389098,"user_tz":-60,"elapsed":1452,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["%%bash\n","rm -r /content/experiment_node1"],"execution_count":58,"outputs":[]},{"cell_type":"code","metadata":{"id":"gl0nftCJOgzw"},"source":["\"\"\"\n","--------------------------------------------------------------------------------------------------------------------------------------------\n","The code for Hessian analysis.\n","--------------------------------------------------------------------------------------------------------------------------------------------\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m75L8B4eOjXq","executionInfo":{"status":"ok","timestamp":1617964978011,"user_tz":-60,"elapsed":441,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["def get_manual_hessian(grads, parameters, show_iters=True):\n","  \"\"\"\n","  Calculation of the Hessian using nested for loops.\n","  Inputs:   - grads:        tuple of gradient tensors. Created using something \n","                            like grads = torch.autograd.grad(loss, parameters, create_graph=True).\n","            - parameters:   List of parameter objects. Created using something \n","                            like parameters = optimizer.param_groups[0]['params'].\n","            - show_iters:   True or False, depending on if the iteration number is to be shown during training. \n","                            Note that the iteration updates are not provided every row, but instead periodically \n","                            (roughly according to the number of parameters in the system).\n","  \"\"\"\n","  start = time.time()        \n","\n","  n_params = 0\n","  for param in parameters:\n","    n_params += torch.numel(param)\n","  grads2 = torch.zeros(n_params,n_params)            #Create an matrix of zeros thas has the same shape as the Hessian.\n","\n","  y_counter = 0                             #y_direction refers to row number in the Hessian.\n","\n","  for grad in grads:\n","      grad = torch.reshape(grad, [-1])                                  #Rearrange the gradient information into a vector.        \n","\n","      for j, g in enumerate(grad):\n","        x_counter = 0                                                   #x_direction refers to column number in the Hessian.\n","\n","        for l, param in enumerate(parameters):\n","          g2 = torch.autograd.grad(g, param, retain_graph=True)[0]      #Calculate the gradient of an element of the gradient wrt one layer's parameters.\n","          g2 = torch.reshape(g2, [-1])                                  #Reshape this into a vector.\n","          len = g2.shape[0]                       \n","          grads2[j+y_counter, x_counter:x_counter+len] = g2             #Indexing ensures that the second order derivatives are placed in the correct positions.\n","          x_counter += len\n","\n","      grads2 = grads2.to(device)\n","      y_counter += grad.shape[0]\n","\n","      if show_iters:\n","        print(\"Gradients calculated for row number \" + str(y_counter) + \".\")\n","  \n","  print('Time used was ', time.time() - start)\n","\n","  return grads2"],"execution_count":65,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":426},"id":"G8w3mIvwOlz5","executionInfo":{"status":"error","timestamp":1617965363914,"user_tz":-60,"elapsed":699,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}},"outputId":"d0dcab47-382d-40d7-f612-eb3119ceb615"},"source":["train_loader, test_loader, train_eval_loader = get_mnist_loaders(\n","        args.data_aug, args.batch_size, args.test_batch_size)\n","\n","optimizer = torch.optim.SGD(model.parameters(), lr=1)\n","optimizer.zero_grad()\n","\n","criterion = nn.CrossEntropyLoss().to(device)\n","for param_group in optimizer.param_groups:\n","            param_group['lr'] = lr_fn(itr)\n","\n","optimizer.zero_grad()\n","x, y = data_gen.__next__()\n","x = x.to(device)\n","y = y.to(device)\n","logits = model(x)\n","loss = criterion(logits, y)\n","\n","target = torch.zeros(len(zN))\n","for i in range(len(zN)):\n","  if zN[i]==0:\n","    target[i] = 0\n","  else:\n","    target[i] = 1\n","target = target.long()\n","\n","loss = criterion(pred_y, target)\n","grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n","parameters = optimizer.param_groups[0]['params']\n","\n","print('Obtaining manual hessian...')\n","manual_hessian = get_manual_hessian(grads, parameters)           #get manual hessian."],"execution_count":67,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# unserialize the data after having released the lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/reductions.py\u001b[0m in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrebuild_storage_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m     \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mdetach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;34m'''Get the fd.  This should only be called once.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0m_resource_sharer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mget_connection\u001b[0;34m(ident)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mident\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mClient\u001b[0;34m(address, family, authkey)\u001b[0m\n\u001b[1;32m    491\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSocketClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mSocketClient\u001b[0;34m(address)\u001b[0m\n\u001b[1;32m    619\u001b[0m         \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetblocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m         \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mConnection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\u001b[0m in \u001b[0;36minf_generator\u001b[0;34m(iterable)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1183\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1146\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1149\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    997\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m                 \u001b[0mpids_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DataLoader worker (pid(s) {}) exited unexpectedly'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpids_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmpty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 448, 449) exited unexpectedly"]}]}]}
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.1, nepochs=120, network='odenet', save='/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/experiment_node1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.1, nepochs=120, network='odenet', save='/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/experiment_node1', test_batch_size=1000, tol=0.001)
Sequential(
  (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1))
  (1): GroupNorm(32, 64, eps=1e-05, affine=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): GroupNorm(32, 64, eps=1e-05, affine=True)
  (5): ReLU(inplace=True)
  (6): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (norm1): GroupNorm(32, 64, eps=1e-05, affine=True)
      (relu): ReLU(inplace=True)
      (conv1): ConcatConv2d(
        (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)
      (conv2): ConcatConv2d(
        (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (norm3): GroupNorm(32, 64, eps=1e-05, affine=True)
    )
  )
  (8): GroupNorm(32, 64, eps=1e-05, affine=True)
  (9): ReLU(inplace=True)
  (10): AdaptiveAvgPool2d(output_size=(1, 1))
  (11): Flatten()
  (12): Linear(in_features=64, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1))
  (1): GroupNorm(32, 64, eps=1e-05, affine=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): GroupNorm(32, 64, eps=1e-05, affine=True)
  (5): ReLU(inplace=True)
  (6): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (norm1): GroupNorm(32, 64, eps=1e-05, affine=True)
      (relu): ReLU(inplace=True)
      (conv1): ConcatConv2d(
        (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)
      (conv2): ConcatConv2d(
        (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (norm3): GroupNorm(32, 64, eps=1e-05, affine=True)
    )
  )
  (8): GroupNorm(32, 64, eps=1e-05, affine=True)
  (9): ReLU(inplace=True)
  (10): AdaptiveAvgPool2d(output_size=(1, 1))
  (11): Flatten()
  (12): Linear(in_features=64, out_features=10, bias=True)
)
Number of parameters: 208266
Number of parameters: 208266
/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mnist_node.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1sLUaZ_DEMKH30zwclvQKQ2FNZUI_mF3K","authorship_tag":"ABX9TyPy4w1yrlFYQDGrN0tP3duT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"sWmsXi5BIAtb"},"source":["%%capture\n","%%bash\n","pip install torchdiffeq"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H353RG8fIDd3"},"source":["import os\n","import argparse\n","import logging\n","import time\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","\n","\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')\n","parser.add_argument('--tol', type=float, default=1e-3)\n","parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])\n","parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])\n","parser.add_argument('--nepochs', type=int, default=120)\n","parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])\n","parser.add_argument('--lr', type=float, default=0.1)\n","parser.add_argument('--batch_size', type=int, default=128)\n","parser.add_argument('--test_batch_size', type=int, default=1000)\n","\n","parser.add_argument('--save', type=str, default='./experiment_node1')\n","parser.add_argument('--gpu', type=int, default=0)\n","parser.add_argument('--debug', action='store_true')\n","args = parser.parse_args(args=[])\n","\n","args.save = '/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/experiment_node1'\n","\n","if args.adjoint:\n","    from torchdiffeq import odeint_adjoint as odeint\n","else:\n","    from torchdiffeq import odeint"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"agpZtaKwIKnA"},"source":["def conv3x3(in_planes, out_planes, stride=1):\n","    \"\"\"3x3 convolution with padding\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","\n","def conv1x1(in_planes, out_planes, stride=1):\n","    \"\"\"1x1 convolution\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n","\n","\n","def norm(dim):\n","    return nn.GroupNorm(min(32, dim), dim)\n","\n","\n","class ResBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None):\n","        super(ResBlock, self).__init__()\n","        self.norm1 = norm(inplanes)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.downsample = downsample\n","        self.conv1 = conv3x3(inplanes, planes, stride)\n","        self.norm2 = norm(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","\n","    def forward(self, x):\n","        shortcut = x\n","\n","        out = self.relu(self.norm1(x))\n","\n","        if self.downsample is not None:\n","            shortcut = self.downsample(out)\n","\n","        out = self.conv1(out)\n","        out = self.norm2(out)\n","        out = self.relu(out)\n","        out = self.conv2(out)\n","\n","        return out + shortcut\n","\n","\n","class ConcatConv2d(nn.Module):\n","\n","    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):\n","        super(ConcatConv2d, self).__init__()\n","        module = nn.ConvTranspose2d if transpose else nn.Conv2d\n","        self._layer = module(\n","            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,\n","            bias=bias\n","        )\n","\n","    def forward(self, t, x):\n","        tt = torch.ones_like(x[:, :1, :, :]) * t\n","        ttx = torch.cat([tt, x], 1)\n","        return self._layer(ttx)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1-rlM_3sI-1k"},"source":["class ODEfunc(nn.Module):\n","\n","    def __init__(self, dim):\n","        super(ODEfunc, self).__init__()\n","        self.norm1 = norm(dim)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv1 = ConcatConv2d(dim, dim, 3, 1, 1)\n","        self.norm2 = norm(dim)\n","        self.conv2 = ConcatConv2d(dim, dim, 3, 1, 1)\n","        self.norm3 = norm(dim)\n","        self.nfe = 0\n","\n","    def forward(self, t, x):\n","        self.nfe += 1\n","        out = self.norm1(x)\n","        out = self.relu(out)\n","        out = self.conv1(t, out)\n","        out = self.norm2(out)\n","        out = self.relu(out)\n","        out = self.conv2(t, out)\n","        out = self.norm3(out)\n","        return out\n","\n","\n","class ODEBlock(nn.Module):\n","\n","    def __init__(self, odefunc):\n","        super(ODEBlock, self).__init__()\n","        self.odefunc = odefunc\n","        self.integration_time = torch.tensor([0, 1]).float()\n","\n","    def forward(self, x):\n","        self.integration_time = self.integration_time.type_as(x)\n","        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)\n","        return out[1]\n","\n","    @property\n","    def nfe(self):\n","        return self.odefunc.nfe\n","\n","    @nfe.setter\n","    def nfe(self, value):\n","        self.odefunc.nfe = value"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xhFiCzNaJC0x"},"source":["class Flatten(nn.Module):\n","\n","    def __init__(self):\n","        super(Flatten, self).__init__()\n","\n","    def forward(self, x):\n","        shape = torch.prod(torch.tensor(x.shape[1:])).item()\n","        return x.view(-1, shape)\n","\n","\n","class RunningAverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","\n","    def __init__(self, momentum=0.99):\n","        self.momentum = momentum\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = None\n","        self.avg = 0\n","\n","    def update(self, val):\n","        if self.val is None:\n","            self.avg = val\n","        else:\n","            self.avg = self.avg * self.momentum + val * (1 - self.momentum)\n","        self.val = val"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ltcgl1RIJG7U","executionInfo":{"status":"ok","timestamp":1617963610116,"user_tz":-60,"elapsed":397,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["def get_mnist_loaders(data_aug=False, batch_size=128, test_batch_size=1000, perc=1.0):\n","    if data_aug:\n","        transform_train = transforms.Compose([\n","            transforms.RandomCrop(28, padding=4),\n","            transforms.ToTensor(),\n","        ])\n","    else:\n","        transform_train = transforms.Compose([\n","            transforms.ToTensor(),\n","        ])\n","\n","    transform_test = transforms.Compose([\n","        transforms.ToTensor(),\n","    ])\n","\n","    train_loader = DataLoader(\n","        datasets.MNIST(root='.data/mnist', train=True, download=True, transform=transform_train), batch_size=batch_size,\n","        shuffle=True, num_workers=2, drop_last=True\n","    )\n","\n","    train_eval_loader = DataLoader(\n","        datasets.MNIST(root='.data/mnist', train=True, download=True, transform=transform_test),\n","        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True\n","    )\n","\n","    test_loader = DataLoader(\n","        datasets.MNIST(root='.data/mnist', train=False, download=True, transform=transform_test),\n","        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True\n","    )\n","\n","    return train_loader, test_loader, train_eval_loader\n","\n","\n","def inf_generator(iterable):\n","    \"\"\"Allows training with DataLoaders in a single infinite loop:\n","        for i, (x, y) in enumerate(inf_generator(train_loader)):\n","    \"\"\"\n","    iterator = iterable.__iter__()\n","    while True:\n","        try:\n","            yield iterator.__next__()\n","        except StopIteration:\n","            iterator = iterable.__iter__()\n","\n","\n","def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):\n","    initial_learning_rate = args.lr * batch_size / batch_denom\n","\n","    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]\n","    vals = [initial_learning_rate * decay for decay in decay_rates]\n","\n","    def learning_rate_fn(itr):\n","        lt = [itr < b for b in boundaries] + [True]\n","        i = np.argmax(lt)\n","        return vals[i]\n","\n","    return learning_rate_fn\n","\n","\n","def one_hot(x, K):\n","    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)\n","\n","\n","def accuracy(model, dataset_loader):\n","    total_correct = 0\n","    for x, y in dataset_loader:\n","        x = x.to(device)\n","        y = one_hot(np.array(y.numpy()), 10)\n","\n","        target_class = np.argmax(y, axis=1)\n","        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)\n","        total_correct += np.sum(predicted_class == target_class)\n","    return total_correct / len(dataset_loader.dataset)\n","\n","\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","\n","def makedirs(dirname):\n","    if not os.path.exists(dirname):\n","        os.makedirs(dirname)\n","\n","\n","def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):\n","    logger = logging.getLogger()\n","    if debug:\n","        level = logging.DEBUG\n","    else:\n","        level = logging.INFO\n","    logger.setLevel(level)\n","    if saving:\n","        info_file_handler = logging.FileHandler(logpath, mode=\"a\")\n","        info_file_handler.setLevel(level)\n","        logger.addHandler(info_file_handler)\n","    if displaying:\n","        console_handler = logging.StreamHandler()\n","        console_handler.setLevel(level)\n","        logger.addHandler(console_handler)\n","    logger.info(filepath)\n","    with open(filepath, \"r\") as f:\n","        logger.info(f.read())\n","\n","    for f in package_files:\n","        logger.info(f)\n","        with open(f, \"r\") as package_f:\n","            logger.info(package_f.read())\n","\n","    return logger\n"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":232},"id":"42RIwJ37JKQr","executionInfo":{"status":"error","timestamp":1617965459245,"user_tz":-60,"elapsed":1211,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}},"outputId":"6af21cd7-8912-4810-96ce-a23024a394f6"},"source":["if __name__ == '__main__':\n","    __file__ = '/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb'\n","    makedirs(args.save)\n","    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))\n","    logger.info(args)\n","\n","    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')\n","\n","    is_odenet = args.network == 'odenet'\n","\n","    if args.downsampling_method == 'conv':\n","        downsampling_layers = [\n","            nn.Conv2d(1, 64, 3, 1),\n","            norm(64),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(64, 64, 4, 2, 1),\n","            norm(64),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(64, 64, 4, 2, 1),\n","        ]\n","    elif args.downsampling_method == 'res':\n","        downsampling_layers = [\n","            nn.Conv2d(1, 64, 3, 1),\n","            ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\n","            ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\n","        ]\n","\n","    feature_layers = [ODEBlock(ODEfunc(64))] if is_odenet else [ResBlock(64, 64) for _ in range(6)]\n","    fc_layers = [norm(64), nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d((1, 1)), Flatten(), nn.Linear(64, 10)]\n","\n","    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)\n","\n","    logger.info(model)\n","    logger.info('Number of parameters: {}'.format(count_parameters(model)))\n","\n","    criterion = nn.CrossEntropyLoss().to(device)\n","\n","    train_loader, test_loader, train_eval_loader = get_mnist_loaders(\n","        args.data_aug, args.batch_size, args.test_batch_size\n","    )\n","\n","    data_gen = inf_generator(train_loader)\n","    batches_per_epoch = len(train_loader)\n","\n","    lr_fn = learning_rate_with_decay(\n","        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],\n","        decay_rates=[1, 0.1, 0.01, 0.001]\n","    )\n","\n","    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)\n","\n","    best_acc = 0\n","    batch_time_meter = RunningAverageMeter()\n","    f_nfe_meter = RunningAverageMeter()\n","    b_nfe_meter = RunningAverageMeter()\n","    end = time.time()\n","    \n","    epoch_arr = []\n","    time_val_arr = []\n","    time_avg_arr = []\n","    nfe_f_arr = []\n","    nfe_b_arr = []\n","    train_acc_arr = []\n","    test_acc_arr = []\n","\n","    for itr in range(args.nepochs * batches_per_epoch):\n","\n","        for param_group in optimizer.param_groups:\n","            param_group['lr'] = lr_fn(itr)\n","\n","        optimizer.zero_grad()\n","        x, y = data_gen.__next__()\n","        x = x.to(device)\n","        y = y.to(device)\n","        logits = model(x)\n","        loss = criterion(logits, y)\n","\n","        grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n","        parameters = optimizer.param_groups[0]['params']\n","\n","        print('Obtaining manual hessian...')\n","        manual_hessian = get_manual_hessian(grads, parameters)           #get manual hessian.\n","\n","        if is_odenet:\n","            nfe_forward = feature_layers[0].nfe\n","            feature_layers[0].nfe = 0\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        if is_odenet:\n","            nfe_backward = feature_layers[0].nfe\n","            feature_layers[0].nfe = 0\n","\n","        batch_time_meter.update(time.time() - end)\n","        if is_odenet:\n","            f_nfe_meter.update(nfe_forward)\n","            b_nfe_meter.update(nfe_backward)\n","        end = time.time()\n","\n","        if itr % batches_per_epoch == 0:\n","            with torch.no_grad():\n","                train_acc = accuracy(model, train_eval_loader)\n","                val_acc = accuracy(model, test_loader)\n","                if val_acc > best_acc:\n","                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))\n","                    best_acc = val_acc\n","                logger.info(\n","                    \"Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | \"\n","                    \"Train Acc {:.4f} | Test Acc {:.4f}\".format(\n","                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,\n","                        b_nfe_meter.avg, train_acc, val_acc\n","                    )\n","                )\n","                epoch_arr += [itr // batches_per_epoch]\n","                time_val_arr += [batch_time_meter.val]\n","                time_avg_arr += [batch_time_meter.avg]\n","                nfe_f_arr += [f_nfe_meter.avg]\n","                nfe_b_arr += [b_nfe_meter.avg]\n","                train_acc_arr += [train_acc]\n","                test_acc_arr += [val_acc]\n","                    \n","    epoch_arr = np.asarray(epoch_arr)\n","    time_val_arr = np.asarray(time_val_arr)\n","    time_avg_arr = np.asarray(time_avg_arr)\n","    nfe_f_arr = np.asarray(nfe_f_arr)\n","    nfe_b_arr = np.asarray(nfe_b_arr)\n","    train_acc_arr = np.asarray(train_acc_arr)\n","    test_acc_arr = np.asarray(test_acc_arr)\n","    \n","    np.save(os.path.join(args.save, 'epoch_arr.npy'), epoch_arr)\n","    np.save(os.path.join(args.save, 'time_val_arr.npy'), time_val_arr)\n","    np.save(os.path.join(args.save, 'time_avg_arr.npy'), time_avg_arr)\n","    np.save(os.path.join(args.save, 'nfe_f_arr.npy'), nfe_f_arr)\n","    np.save(os.path.join(args.save, 'nfe_b_arr.npy'), nfe_b_arr)\n","    np.save(os.path.join(args.save, 'train_acc_arr.npy'), train_acc_arr)\n","    np.save(os.path.join(args.save, 'test_acc_arr.npy'), test_acc_arr)"],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0m__file__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'logs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'makedirs' is not defined"]}]},{"cell_type":"code","metadata":{"id":"Ner61Yv1L9ZC","executionInfo":{"status":"ok","timestamp":1617964389098,"user_tz":-60,"elapsed":1452,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["%%bash\n","rm -r /content/experiment_node1"],"execution_count":58,"outputs":[]},{"cell_type":"code","metadata":{"id":"gl0nftCJOgzw"},"source":["\"\"\"\n","--------------------------------------------------------------------------------------------------------------------------------------------\n","The code for Hessian analysis.\n","--------------------------------------------------------------------------------------------------------------------------------------------\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m75L8B4eOjXq","executionInfo":{"status":"ok","timestamp":1617964978011,"user_tz":-60,"elapsed":441,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["def get_manual_hessian(grads, parameters, show_iters=True):\n","  \"\"\"\n","  Calculation of the Hessian using nested for loops.\n","  Inputs:   - grads:        tuple of gradient tensors. Created using something \n","                            like grads = torch.autograd.grad(loss, parameters, create_graph=True).\n","            - parameters:   List of parameter objects. Created using something \n","                            like parameters = optimizer.param_groups[0]['params'].\n","            - show_iters:   True or False, depending on if the iteration number is to be shown during training. \n","                            Note that the iteration updates are not provided every row, but instead periodically \n","                            (roughly according to the number of parameters in the system).\n","  \"\"\"\n","  start = time.time()        \n","\n","  n_params = 0\n","  for param in parameters:\n","    n_params += torch.numel(param)\n","  grads2 = torch.zeros(n_params,n_params)            #Create an matrix of zeros thas has the same shape as the Hessian.\n","\n","  y_counter = 0                             #y_direction refers to row number in the Hessian.\n","\n","  for grad in grads:\n","      grad = torch.reshape(grad, [-1])                                  #Rearrange the gradient information into a vector.        \n","\n","      for j, g in enumerate(grad):\n","        x_counter = 0                                                   #x_direction refers to column number in the Hessian.\n","\n","        for l, param in enumerate(parameters):\n","          g2 = torch.autograd.grad(g, param, retain_graph=True)[0]      #Calculate the gradient of an element of the gradient wrt one layer's parameters.\n","          g2 = torch.reshape(g2, [-1])                                  #Reshape this into a vector.\n","          len = g2.shape[0]                       \n","          grads2[j+y_counter, x_counter:x_counter+len] = g2             #Indexing ensures that the second order derivatives are placed in the correct positions.\n","          x_counter += len\n","\n","      grads2 = grads2.to(device)\n","      y_counter += grad.shape[0]\n","\n","      if show_iters:\n","        print(\"Gradients calculated for row number \" + str(y_counter) + \".\")\n","  \n","  print('Time used was ', time.time() - start)\n","\n","  return grads2"],"execution_count":65,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":426},"id":"G8w3mIvwOlz5","executionInfo":{"status":"error","timestamp":1617965363914,"user_tz":-60,"elapsed":699,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}},"outputId":"d0dcab47-382d-40d7-f612-eb3119ceb615"},"source":["train_loader, test_loader, train_eval_loader = get_mnist_loaders(\n","        args.data_aug, args.batch_size, args.test_batch_size)\n","\n","optimizer = torch.optim.SGD(model.parameters(), lr=1)\n","optimizer.zero_grad()\n","\n","criterion = nn.CrossEntropyLoss().to(device)\n","for param_group in optimizer.param_groups:\n","            param_group['lr'] = lr_fn(itr)\n","\n","optimizer.zero_grad()\n","x, y = data_gen.__next__()\n","x = x.to(device)\n","y = y.to(device)\n","logits = model(x)\n","loss = criterion(logits, y)\n","\n","target = torch.zeros(len(zN))\n","for i in range(len(zN)):\n","  if zN[i]==0:\n","    target[i] = 0\n","  else:\n","    target[i] = 1\n","target = target.long()\n","\n","loss = criterion(pred_y, target)\n","grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n","parameters = optimizer.param_groups[0]['params']\n","\n","print('Obtaining manual hessian...')\n","manual_hessian = get_manual_hessian(grads, parameters)           #get manual hessian."],"execution_count":67,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# unserialize the data after having released the lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/reductions.py\u001b[0m in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrebuild_storage_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m     \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mdetach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;34m'''Get the fd.  This should only be called once.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0m_resource_sharer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mget_connection\u001b[0;34m(ident)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mident\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mClient\u001b[0;34m(address, family, authkey)\u001b[0m\n\u001b[1;32m    491\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSocketClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mSocketClient\u001b[0;34m(address)\u001b[0m\n\u001b[1;32m    619\u001b[0m         \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetblocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m         \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mConnection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\u001b[0m in \u001b[0;36minf_generator\u001b[0;34m(iterable)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1183\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1146\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1149\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    997\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m                 \u001b[0mpids_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DataLoader worker (pid(s) {}) exited unexpectedly'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpids_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmpty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 448, 449) exited unexpectedly"]}]}]}
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.1, nepochs=120, network='odenet', save='/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/experiment_node1', test_batch_size=1000, tol=0.001)
Sequential(
  (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1))
  (1): GroupNorm(32, 64, eps=1e-05, affine=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): GroupNorm(32, 64, eps=1e-05, affine=True)
  (5): ReLU(inplace=True)
  (6): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (norm1): GroupNorm(32, 64, eps=1e-05, affine=True)
      (relu): ReLU(inplace=True)
      (conv1): ConcatConv2d(
        (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)
      (conv2): ConcatConv2d(
        (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (norm3): GroupNorm(32, 64, eps=1e-05, affine=True)
    )
  )
  (8): GroupNorm(32, 64, eps=1e-05, affine=True)
  (9): ReLU(inplace=True)
  (10): AdaptiveAvgPool2d(output_size=(1, 1))
  (11): Flatten()
  (12): Linear(in_features=64, out_features=10, bias=True)
)
Number of parameters: 208266
/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb
/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mnist_node.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1sLUaZ_DEMKH30zwclvQKQ2FNZUI_mF3K","authorship_tag":"ABX9TyPy4w1yrlFYQDGrN0tP3duT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"sWmsXi5BIAtb"},"source":["%%capture\n","%%bash\n","pip install torchdiffeq"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H353RG8fIDd3"},"source":["import os\n","import argparse\n","import logging\n","import time\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","\n","\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')\n","parser.add_argument('--tol', type=float, default=1e-3)\n","parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])\n","parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])\n","parser.add_argument('--nepochs', type=int, default=120)\n","parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])\n","parser.add_argument('--lr', type=float, default=0.1)\n","parser.add_argument('--batch_size', type=int, default=128)\n","parser.add_argument('--test_batch_size', type=int, default=1000)\n","\n","parser.add_argument('--save', type=str, default='./experiment_node1')\n","parser.add_argument('--gpu', type=int, default=0)\n","parser.add_argument('--debug', action='store_true')\n","args = parser.parse_args(args=[])\n","\n","args.save = '/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/experiment_node1'\n","\n","if args.adjoint:\n","    from torchdiffeq import odeint_adjoint as odeint\n","else:\n","    from torchdiffeq import odeint"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"agpZtaKwIKnA"},"source":["def conv3x3(in_planes, out_planes, stride=1):\n","    \"\"\"3x3 convolution with padding\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","\n","def conv1x1(in_planes, out_planes, stride=1):\n","    \"\"\"1x1 convolution\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n","\n","\n","def norm(dim):\n","    return nn.GroupNorm(min(32, dim), dim)\n","\n","\n","class ResBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None):\n","        super(ResBlock, self).__init__()\n","        self.norm1 = norm(inplanes)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.downsample = downsample\n","        self.conv1 = conv3x3(inplanes, planes, stride)\n","        self.norm2 = norm(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","\n","    def forward(self, x):\n","        shortcut = x\n","\n","        out = self.relu(self.norm1(x))\n","\n","        if self.downsample is not None:\n","            shortcut = self.downsample(out)\n","\n","        out = self.conv1(out)\n","        out = self.norm2(out)\n","        out = self.relu(out)\n","        out = self.conv2(out)\n","\n","        return out + shortcut\n","\n","\n","class ConcatConv2d(nn.Module):\n","\n","    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):\n","        super(ConcatConv2d, self).__init__()\n","        module = nn.ConvTranspose2d if transpose else nn.Conv2d\n","        self._layer = module(\n","            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,\n","            bias=bias\n","        )\n","\n","    def forward(self, t, x):\n","        tt = torch.ones_like(x[:, :1, :, :]) * t\n","        ttx = torch.cat([tt, x], 1)\n","        return self._layer(ttx)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1-rlM_3sI-1k"},"source":["class ODEfunc(nn.Module):\n","\n","    def __init__(self, dim):\n","        super(ODEfunc, self).__init__()\n","        self.norm1 = norm(dim)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv1 = ConcatConv2d(dim, dim, 3, 1, 1)\n","        self.norm2 = norm(dim)\n","        self.conv2 = ConcatConv2d(dim, dim, 3, 1, 1)\n","        self.norm3 = norm(dim)\n","        self.nfe = 0\n","\n","    def forward(self, t, x):\n","        self.nfe += 1\n","        out = self.norm1(x)\n","        out = self.relu(out)\n","        out = self.conv1(t, out)\n","        out = self.norm2(out)\n","        out = self.relu(out)\n","        out = self.conv2(t, out)\n","        out = self.norm3(out)\n","        return out\n","\n","\n","class ODEBlock(nn.Module):\n","\n","    def __init__(self, odefunc):\n","        super(ODEBlock, self).__init__()\n","        self.odefunc = odefunc\n","        self.integration_time = torch.tensor([0, 1]).float()\n","\n","    def forward(self, x):\n","        self.integration_time = self.integration_time.type_as(x)\n","        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)\n","        return out[1]\n","\n","    @property\n","    def nfe(self):\n","        return self.odefunc.nfe\n","\n","    @nfe.setter\n","    def nfe(self, value):\n","        self.odefunc.nfe = value"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xhFiCzNaJC0x"},"source":["class Flatten(nn.Module):\n","\n","    def __init__(self):\n","        super(Flatten, self).__init__()\n","\n","    def forward(self, x):\n","        shape = torch.prod(torch.tensor(x.shape[1:])).item()\n","        return x.view(-1, shape)\n","\n","\n","class RunningAverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","\n","    def __init__(self, momentum=0.99):\n","        self.momentum = momentum\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = None\n","        self.avg = 0\n","\n","    def update(self, val):\n","        if self.val is None:\n","            self.avg = val\n","        else:\n","            self.avg = self.avg * self.momentum + val * (1 - self.momentum)\n","        self.val = val"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ltcgl1RIJG7U","executionInfo":{"status":"ok","timestamp":1617963610116,"user_tz":-60,"elapsed":397,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["def get_mnist_loaders(data_aug=False, batch_size=128, test_batch_size=1000, perc=1.0):\n","    if data_aug:\n","        transform_train = transforms.Compose([\n","            transforms.RandomCrop(28, padding=4),\n","            transforms.ToTensor(),\n","        ])\n","    else:\n","        transform_train = transforms.Compose([\n","            transforms.ToTensor(),\n","        ])\n","\n","    transform_test = transforms.Compose([\n","        transforms.ToTensor(),\n","    ])\n","\n","    train_loader = DataLoader(\n","        datasets.MNIST(root='.data/mnist', train=True, download=True, transform=transform_train), batch_size=batch_size,\n","        shuffle=True, num_workers=2, drop_last=True\n","    )\n","\n","    train_eval_loader = DataLoader(\n","        datasets.MNIST(root='.data/mnist', train=True, download=True, transform=transform_test),\n","        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True\n","    )\n","\n","    test_loader = DataLoader(\n","        datasets.MNIST(root='.data/mnist', train=False, download=True, transform=transform_test),\n","        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True\n","    )\n","\n","    return train_loader, test_loader, train_eval_loader\n","\n","\n","def inf_generator(iterable):\n","    \"\"\"Allows training with DataLoaders in a single infinite loop:\n","        for i, (x, y) in enumerate(inf_generator(train_loader)):\n","    \"\"\"\n","    iterator = iterable.__iter__()\n","    while True:\n","        try:\n","            yield iterator.__next__()\n","        except StopIteration:\n","            iterator = iterable.__iter__()\n","\n","\n","def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):\n","    initial_learning_rate = args.lr * batch_size / batch_denom\n","\n","    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]\n","    vals = [initial_learning_rate * decay for decay in decay_rates]\n","\n","    def learning_rate_fn(itr):\n","        lt = [itr < b for b in boundaries] + [True]\n","        i = np.argmax(lt)\n","        return vals[i]\n","\n","    return learning_rate_fn\n","\n","\n","def one_hot(x, K):\n","    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)\n","\n","\n","def accuracy(model, dataset_loader):\n","    total_correct = 0\n","    for x, y in dataset_loader:\n","        x = x.to(device)\n","        y = one_hot(np.array(y.numpy()), 10)\n","\n","        target_class = np.argmax(y, axis=1)\n","        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)\n","        total_correct += np.sum(predicted_class == target_class)\n","    return total_correct / len(dataset_loader.dataset)\n","\n","\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","\n","def makedirs(dirname):\n","    if not os.path.exists(dirname):\n","        os.makedirs(dirname)\n","\n","\n","def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):\n","    logger = logging.getLogger()\n","    if debug:\n","        level = logging.DEBUG\n","    else:\n","        level = logging.INFO\n","    logger.setLevel(level)\n","    if saving:\n","        info_file_handler = logging.FileHandler(logpath, mode=\"a\")\n","        info_file_handler.setLevel(level)\n","        logger.addHandler(info_file_handler)\n","    if displaying:\n","        console_handler = logging.StreamHandler()\n","        console_handler.setLevel(level)\n","        logger.addHandler(console_handler)\n","    logger.info(filepath)\n","    with open(filepath, \"r\") as f:\n","        logger.info(f.read())\n","\n","    for f in package_files:\n","        logger.info(f)\n","        with open(f, \"r\") as package_f:\n","            logger.info(package_f.read())\n","\n","    return logger\n"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":232},"id":"42RIwJ37JKQr","executionInfo":{"status":"error","timestamp":1617965459245,"user_tz":-60,"elapsed":1211,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}},"outputId":"6af21cd7-8912-4810-96ce-a23024a394f6"},"source":["if __name__ == '__main__':\n","    __file__ = '/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb'\n","    makedirs(args.save)\n","    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))\n","    logger.info(args)\n","\n","    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')\n","\n","    is_odenet = args.network == 'odenet'\n","\n","    if args.downsampling_method == 'conv':\n","        downsampling_layers = [\n","            nn.Conv2d(1, 64, 3, 1),\n","            norm(64),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(64, 64, 4, 2, 1),\n","            norm(64),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(64, 64, 4, 2, 1),\n","        ]\n","    elif args.downsampling_method == 'res':\n","        downsampling_layers = [\n","            nn.Conv2d(1, 64, 3, 1),\n","            ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\n","            ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\n","        ]\n","\n","    feature_layers = [ODEBlock(ODEfunc(64))] if is_odenet else [ResBlock(64, 64) for _ in range(6)]\n","    fc_layers = [norm(64), nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d((1, 1)), Flatten(), nn.Linear(64, 10)]\n","\n","    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)\n","\n","    logger.info(model)\n","    logger.info('Number of parameters: {}'.format(count_parameters(model)))\n","\n","    criterion = nn.CrossEntropyLoss().to(device)\n","\n","    train_loader, test_loader, train_eval_loader = get_mnist_loaders(\n","        args.data_aug, args.batch_size, args.test_batch_size\n","    )\n","\n","    data_gen = inf_generator(train_loader)\n","    batches_per_epoch = len(train_loader)\n","\n","    lr_fn = learning_rate_with_decay(\n","        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],\n","        decay_rates=[1, 0.1, 0.01, 0.001]\n","    )\n","\n","    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)\n","\n","    best_acc = 0\n","    batch_time_meter = RunningAverageMeter()\n","    f_nfe_meter = RunningAverageMeter()\n","    b_nfe_meter = RunningAverageMeter()\n","    end = time.time()\n","    \n","    epoch_arr = []\n","    time_val_arr = []\n","    time_avg_arr = []\n","    nfe_f_arr = []\n","    nfe_b_arr = []\n","    train_acc_arr = []\n","    test_acc_arr = []\n","\n","    for itr in range(args.nepochs * batches_per_epoch):\n","\n","        for param_group in optimizer.param_groups:\n","            param_group['lr'] = lr_fn(itr)\n","\n","        optimizer.zero_grad()\n","        x, y = data_gen.__next__()\n","        x = x.to(device)\n","        y = y.to(device)\n","        logits = model(x)\n","        loss = criterion(logits, y)\n","\n","        grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n","        parameters = optimizer.param_groups[0]['params']\n","\n","        print('Obtaining manual hessian...')\n","        manual_hessian = get_manual_hessian(grads, parameters)           #get manual hessian.\n","\n","        if is_odenet:\n","            nfe_forward = feature_layers[0].nfe\n","            feature_layers[0].nfe = 0\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        if is_odenet:\n","            nfe_backward = feature_layers[0].nfe\n","            feature_layers[0].nfe = 0\n","\n","        batch_time_meter.update(time.time() - end)\n","        if is_odenet:\n","            f_nfe_meter.update(nfe_forward)\n","            b_nfe_meter.update(nfe_backward)\n","        end = time.time()\n","\n","        if itr % batches_per_epoch == 0:\n","            with torch.no_grad():\n","                train_acc = accuracy(model, train_eval_loader)\n","                val_acc = accuracy(model, test_loader)\n","                if val_acc > best_acc:\n","                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))\n","                    best_acc = val_acc\n","                logger.info(\n","                    \"Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | \"\n","                    \"Train Acc {:.4f} | Test Acc {:.4f}\".format(\n","                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,\n","                        b_nfe_meter.avg, train_acc, val_acc\n","                    )\n","                )\n","                epoch_arr += [itr // batches_per_epoch]\n","                time_val_arr += [batch_time_meter.val]\n","                time_avg_arr += [batch_time_meter.avg]\n","                nfe_f_arr += [f_nfe_meter.avg]\n","                nfe_b_arr += [b_nfe_meter.avg]\n","                train_acc_arr += [train_acc]\n","                test_acc_arr += [val_acc]\n","                    \n","    epoch_arr = np.asarray(epoch_arr)\n","    time_val_arr = np.asarray(time_val_arr)\n","    time_avg_arr = np.asarray(time_avg_arr)\n","    nfe_f_arr = np.asarray(nfe_f_arr)\n","    nfe_b_arr = np.asarray(nfe_b_arr)\n","    train_acc_arr = np.asarray(train_acc_arr)\n","    test_acc_arr = np.asarray(test_acc_arr)\n","    \n","    np.save(os.path.join(args.save, 'epoch_arr.npy'), epoch_arr)\n","    np.save(os.path.join(args.save, 'time_val_arr.npy'), time_val_arr)\n","    np.save(os.path.join(args.save, 'time_avg_arr.npy'), time_avg_arr)\n","    np.save(os.path.join(args.save, 'nfe_f_arr.npy'), nfe_f_arr)\n","    np.save(os.path.join(args.save, 'nfe_b_arr.npy'), nfe_b_arr)\n","    np.save(os.path.join(args.save, 'train_acc_arr.npy'), train_acc_arr)\n","    np.save(os.path.join(args.save, 'test_acc_arr.npy'), test_acc_arr)"],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0m__file__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'logs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'makedirs' is not defined"]}]},{"cell_type":"code","metadata":{"id":"Ner61Yv1L9ZC","executionInfo":{"status":"ok","timestamp":1617964389098,"user_tz":-60,"elapsed":1452,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["%%bash\n","rm -r /content/experiment_node1"],"execution_count":58,"outputs":[]},{"cell_type":"code","metadata":{"id":"gl0nftCJOgzw"},"source":["\"\"\"\n","--------------------------------------------------------------------------------------------------------------------------------------------\n","The code for Hessian analysis.\n","--------------------------------------------------------------------------------------------------------------------------------------------\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m75L8B4eOjXq","executionInfo":{"status":"ok","timestamp":1617964978011,"user_tz":-60,"elapsed":441,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["def get_manual_hessian(grads, parameters, show_iters=True):\n","  \"\"\"\n","  Calculation of the Hessian using nested for loops.\n","  Inputs:   - grads:        tuple of gradient tensors. Created using something \n","                            like grads = torch.autograd.grad(loss, parameters, create_graph=True).\n","            - parameters:   List of parameter objects. Created using something \n","                            like parameters = optimizer.param_groups[0]['params'].\n","            - show_iters:   True or False, depending on if the iteration number is to be shown during training. \n","                            Note that the iteration updates are not provided every row, but instead periodically \n","                            (roughly according to the number of parameters in the system).\n","  \"\"\"\n","  start = time.time()        \n","\n","  n_params = 0\n","  for param in parameters:\n","    n_params += torch.numel(param)\n","  grads2 = torch.zeros(n_params,n_params)            #Create an matrix of zeros thas has the same shape as the Hessian.\n","\n","  y_counter = 0                             #y_direction refers to row number in the Hessian.\n","\n","  for grad in grads:\n","      grad = torch.reshape(grad, [-1])                                  #Rearrange the gradient information into a vector.        \n","\n","      for j, g in enumerate(grad):\n","        x_counter = 0                                                   #x_direction refers to column number in the Hessian.\n","\n","        for l, param in enumerate(parameters):\n","          g2 = torch.autograd.grad(g, param, retain_graph=True)[0]      #Calculate the gradient of an element of the gradient wrt one layer's parameters.\n","          g2 = torch.reshape(g2, [-1])                                  #Reshape this into a vector.\n","          len = g2.shape[0]                       \n","          grads2[j+y_counter, x_counter:x_counter+len] = g2             #Indexing ensures that the second order derivatives are placed in the correct positions.\n","          x_counter += len\n","\n","      grads2 = grads2.to(device)\n","      y_counter += grad.shape[0]\n","\n","      if show_iters:\n","        print(\"Gradients calculated for row number \" + str(y_counter) + \".\")\n","  \n","  print('Time used was ', time.time() - start)\n","\n","  return grads2"],"execution_count":65,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":426},"id":"G8w3mIvwOlz5","executionInfo":{"status":"error","timestamp":1617965363914,"user_tz":-60,"elapsed":699,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}},"outputId":"d0dcab47-382d-40d7-f612-eb3119ceb615"},"source":["train_loader, test_loader, train_eval_loader = get_mnist_loaders(\n","        args.data_aug, args.batch_size, args.test_batch_size)\n","\n","optimizer = torch.optim.SGD(model.parameters(), lr=1)\n","optimizer.zero_grad()\n","\n","criterion = nn.CrossEntropyLoss().to(device)\n","for param_group in optimizer.param_groups:\n","            param_group['lr'] = lr_fn(itr)\n","\n","optimizer.zero_grad()\n","x, y = data_gen.__next__()\n","x = x.to(device)\n","y = y.to(device)\n","logits = model(x)\n","loss = criterion(logits, y)\n","\n","target = torch.zeros(len(zN))\n","for i in range(len(zN)):\n","  if zN[i]==0:\n","    target[i] = 0\n","  else:\n","    target[i] = 1\n","target = target.long()\n","\n","loss = criterion(pred_y, target)\n","grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n","parameters = optimizer.param_groups[0]['params']\n","\n","print('Obtaining manual hessian...')\n","manual_hessian = get_manual_hessian(grads, parameters)           #get manual hessian."],"execution_count":67,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# unserialize the data after having released the lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/reductions.py\u001b[0m in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrebuild_storage_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m     \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mdetach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;34m'''Get the fd.  This should only be called once.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0m_resource_sharer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mget_connection\u001b[0;34m(ident)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mident\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mClient\u001b[0;34m(address, family, authkey)\u001b[0m\n\u001b[1;32m    491\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSocketClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mSocketClient\u001b[0;34m(address)\u001b[0m\n\u001b[1;32m    619\u001b[0m         \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetblocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m         \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mConnection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\u001b[0m in \u001b[0;36minf_generator\u001b[0;34m(iterable)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1183\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1146\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1149\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    997\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m                 \u001b[0mpids_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DataLoader worker (pid(s) {}) exited unexpectedly'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpids_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmpty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 448, 449) exited unexpectedly"]}]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mnist_node.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1sLUaZ_DEMKH30zwclvQKQ2FNZUI_mF3K","authorship_tag":"ABX9TyPy4w1yrlFYQDGrN0tP3duT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"sWmsXi5BIAtb"},"source":["%%capture\n","%%bash\n","pip install torchdiffeq"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H353RG8fIDd3"},"source":["import os\n","import argparse\n","import logging\n","import time\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","\n","\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')\n","parser.add_argument('--tol', type=float, default=1e-3)\n","parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])\n","parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])\n","parser.add_argument('--nepochs', type=int, default=120)\n","parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])\n","parser.add_argument('--lr', type=float, default=0.1)\n","parser.add_argument('--batch_size', type=int, default=128)\n","parser.add_argument('--test_batch_size', type=int, default=1000)\n","\n","parser.add_argument('--save', type=str, default='./experiment_node1')\n","parser.add_argument('--gpu', type=int, default=0)\n","parser.add_argument('--debug', action='store_true')\n","args = parser.parse_args(args=[])\n","\n","args.save = '/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/experiment_node1'\n","\n","if args.adjoint:\n","    from torchdiffeq import odeint_adjoint as odeint\n","else:\n","    from torchdiffeq import odeint"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"agpZtaKwIKnA"},"source":["def conv3x3(in_planes, out_planes, stride=1):\n","    \"\"\"3x3 convolution with padding\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","\n","def conv1x1(in_planes, out_planes, stride=1):\n","    \"\"\"1x1 convolution\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n","\n","\n","def norm(dim):\n","    return nn.GroupNorm(min(32, dim), dim)\n","\n","\n","class ResBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None):\n","        super(ResBlock, self).__init__()\n","        self.norm1 = norm(inplanes)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.downsample = downsample\n","        self.conv1 = conv3x3(inplanes, planes, stride)\n","        self.norm2 = norm(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","\n","    def forward(self, x):\n","        shortcut = x\n","\n","        out = self.relu(self.norm1(x))\n","\n","        if self.downsample is not None:\n","            shortcut = self.downsample(out)\n","\n","        out = self.conv1(out)\n","        out = self.norm2(out)\n","        out = self.relu(out)\n","        out = self.conv2(out)\n","\n","        return out + shortcut\n","\n","\n","class ConcatConv2d(nn.Module):\n","\n","    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):\n","        super(ConcatConv2d, self).__init__()\n","        module = nn.ConvTranspose2d if transpose else nn.Conv2d\n","        self._layer = module(\n","            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,\n","            bias=bias\n","        )\n","\n","    def forward(self, t, x):\n","        tt = torch.ones_like(x[:, :1, :, :]) * t\n","        ttx = torch.cat([tt, x], 1)\n","        return self._layer(ttx)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1-rlM_3sI-1k"},"source":["class ODEfunc(nn.Module):\n","\n","    def __init__(self, dim):\n","        super(ODEfunc, self).__init__()\n","        self.norm1 = norm(dim)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv1 = ConcatConv2d(dim, dim, 3, 1, 1)\n","        self.norm2 = norm(dim)\n","        self.conv2 = ConcatConv2d(dim, dim, 3, 1, 1)\n","        self.norm3 = norm(dim)\n","        self.nfe = 0\n","\n","    def forward(self, t, x):\n","        self.nfe += 1\n","        out = self.norm1(x)\n","        out = self.relu(out)\n","        out = self.conv1(t, out)\n","        out = self.norm2(out)\n","        out = self.relu(out)\n","        out = self.conv2(t, out)\n","        out = self.norm3(out)\n","        return out\n","\n","\n","class ODEBlock(nn.Module):\n","\n","    def __init__(self, odefunc):\n","        super(ODEBlock, self).__init__()\n","        self.odefunc = odefunc\n","        self.integration_time = torch.tensor([0, 1]).float()\n","\n","    def forward(self, x):\n","        self.integration_time = self.integration_time.type_as(x)\n","        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)\n","        return out[1]\n","\n","    @property\n","    def nfe(self):\n","        return self.odefunc.nfe\n","\n","    @nfe.setter\n","    def nfe(self, value):\n","        self.odefunc.nfe = value"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xhFiCzNaJC0x"},"source":["class Flatten(nn.Module):\n","\n","    def __init__(self):\n","        super(Flatten, self).__init__()\n","\n","    def forward(self, x):\n","        shape = torch.prod(torch.tensor(x.shape[1:])).item()\n","        return x.view(-1, shape)\n","\n","\n","class RunningAverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","\n","    def __init__(self, momentum=0.99):\n","        self.momentum = momentum\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = None\n","        self.avg = 0\n","\n","    def update(self, val):\n","        if self.val is None:\n","            self.avg = val\n","        else:\n","            self.avg = self.avg * self.momentum + val * (1 - self.momentum)\n","        self.val = val"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ltcgl1RIJG7U","executionInfo":{"status":"ok","timestamp":1617963610116,"user_tz":-60,"elapsed":397,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["def get_mnist_loaders(data_aug=False, batch_size=128, test_batch_size=1000, perc=1.0):\n","    if data_aug:\n","        transform_train = transforms.Compose([\n","            transforms.RandomCrop(28, padding=4),\n","            transforms.ToTensor(),\n","        ])\n","    else:\n","        transform_train = transforms.Compose([\n","            transforms.ToTensor(),\n","        ])\n","\n","    transform_test = transforms.Compose([\n","        transforms.ToTensor(),\n","    ])\n","\n","    train_loader = DataLoader(\n","        datasets.MNIST(root='.data/mnist', train=True, download=True, transform=transform_train), batch_size=batch_size,\n","        shuffle=True, num_workers=2, drop_last=True\n","    )\n","\n","    train_eval_loader = DataLoader(\n","        datasets.MNIST(root='.data/mnist', train=True, download=True, transform=transform_test),\n","        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True\n","    )\n","\n","    test_loader = DataLoader(\n","        datasets.MNIST(root='.data/mnist', train=False, download=True, transform=transform_test),\n","        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True\n","    )\n","\n","    return train_loader, test_loader, train_eval_loader\n","\n","\n","def inf_generator(iterable):\n","    \"\"\"Allows training with DataLoaders in a single infinite loop:\n","        for i, (x, y) in enumerate(inf_generator(train_loader)):\n","    \"\"\"\n","    iterator = iterable.__iter__()\n","    while True:\n","        try:\n","            yield iterator.__next__()\n","        except StopIteration:\n","            iterator = iterable.__iter__()\n","\n","\n","def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):\n","    initial_learning_rate = args.lr * batch_size / batch_denom\n","\n","    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]\n","    vals = [initial_learning_rate * decay for decay in decay_rates]\n","\n","    def learning_rate_fn(itr):\n","        lt = [itr < b for b in boundaries] + [True]\n","        i = np.argmax(lt)\n","        return vals[i]\n","\n","    return learning_rate_fn\n","\n","\n","def one_hot(x, K):\n","    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)\n","\n","\n","def accuracy(model, dataset_loader):\n","    total_correct = 0\n","    for x, y in dataset_loader:\n","        x = x.to(device)\n","        y = one_hot(np.array(y.numpy()), 10)\n","\n","        target_class = np.argmax(y, axis=1)\n","        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)\n","        total_correct += np.sum(predicted_class == target_class)\n","    return total_correct / len(dataset_loader.dataset)\n","\n","\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","\n","def makedirs(dirname):\n","    if not os.path.exists(dirname):\n","        os.makedirs(dirname)\n","\n","\n","def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):\n","    logger = logging.getLogger()\n","    if debug:\n","        level = logging.DEBUG\n","    else:\n","        level = logging.INFO\n","    logger.setLevel(level)\n","    if saving:\n","        info_file_handler = logging.FileHandler(logpath, mode=\"a\")\n","        info_file_handler.setLevel(level)\n","        logger.addHandler(info_file_handler)\n","    if displaying:\n","        console_handler = logging.StreamHandler()\n","        console_handler.setLevel(level)\n","        logger.addHandler(console_handler)\n","    logger.info(filepath)\n","    with open(filepath, \"r\") as f:\n","        logger.info(f.read())\n","\n","    for f in package_files:\n","        logger.info(f)\n","        with open(f, \"r\") as package_f:\n","            logger.info(package_f.read())\n","\n","    return logger\n"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":232},"id":"42RIwJ37JKQr","executionInfo":{"status":"error","timestamp":1617965459245,"user_tz":-60,"elapsed":1211,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}},"outputId":"6af21cd7-8912-4810-96ce-a23024a394f6"},"source":["if __name__ == '__main__':\n","    __file__ = '/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb'\n","    makedirs(args.save)\n","    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))\n","    logger.info(args)\n","\n","    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')\n","\n","    is_odenet = args.network == 'odenet'\n","\n","    if args.downsampling_method == 'conv':\n","        downsampling_layers = [\n","            nn.Conv2d(1, 64, 3, 1),\n","            norm(64),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(64, 64, 4, 2, 1),\n","            norm(64),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(64, 64, 4, 2, 1),\n","        ]\n","    elif args.downsampling_method == 'res':\n","        downsampling_layers = [\n","            nn.Conv2d(1, 64, 3, 1),\n","            ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\n","            ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\n","        ]\n","\n","    feature_layers = [ODEBlock(ODEfunc(64))] if is_odenet else [ResBlock(64, 64) for _ in range(6)]\n","    fc_layers = [norm(64), nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d((1, 1)), Flatten(), nn.Linear(64, 10)]\n","\n","    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)\n","\n","    logger.info(model)\n","    logger.info('Number of parameters: {}'.format(count_parameters(model)))\n","\n","    criterion = nn.CrossEntropyLoss().to(device)\n","\n","    train_loader, test_loader, train_eval_loader = get_mnist_loaders(\n","        args.data_aug, args.batch_size, args.test_batch_size\n","    )\n","\n","    data_gen = inf_generator(train_loader)\n","    batches_per_epoch = len(train_loader)\n","\n","    lr_fn = learning_rate_with_decay(\n","        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],\n","        decay_rates=[1, 0.1, 0.01, 0.001]\n","    )\n","\n","    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)\n","\n","    best_acc = 0\n","    batch_time_meter = RunningAverageMeter()\n","    f_nfe_meter = RunningAverageMeter()\n","    b_nfe_meter = RunningAverageMeter()\n","    end = time.time()\n","    \n","    epoch_arr = []\n","    time_val_arr = []\n","    time_avg_arr = []\n","    nfe_f_arr = []\n","    nfe_b_arr = []\n","    train_acc_arr = []\n","    test_acc_arr = []\n","\n","    for itr in range(args.nepochs * batches_per_epoch):\n","\n","        for param_group in optimizer.param_groups:\n","            param_group['lr'] = lr_fn(itr)\n","\n","        optimizer.zero_grad()\n","        x, y = data_gen.__next__()\n","        x = x.to(device)\n","        y = y.to(device)\n","        logits = model(x)\n","        loss = criterion(logits, y)\n","\n","        grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n","        parameters = optimizer.param_groups[0]['params']\n","\n","        print('Obtaining manual hessian...')\n","        manual_hessian = get_manual_hessian(grads, parameters)           #get manual hessian.\n","\n","        if is_odenet:\n","            nfe_forward = feature_layers[0].nfe\n","            feature_layers[0].nfe = 0\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        if is_odenet:\n","            nfe_backward = feature_layers[0].nfe\n","            feature_layers[0].nfe = 0\n","\n","        batch_time_meter.update(time.time() - end)\n","        if is_odenet:\n","            f_nfe_meter.update(nfe_forward)\n","            b_nfe_meter.update(nfe_backward)\n","        end = time.time()\n","\n","        if itr % batches_per_epoch == 0:\n","            with torch.no_grad():\n","                train_acc = accuracy(model, train_eval_loader)\n","                val_acc = accuracy(model, test_loader)\n","                if val_acc > best_acc:\n","                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))\n","                    best_acc = val_acc\n","                logger.info(\n","                    \"Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | \"\n","                    \"Train Acc {:.4f} | Test Acc {:.4f}\".format(\n","                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,\n","                        b_nfe_meter.avg, train_acc, val_acc\n","                    )\n","                )\n","                epoch_arr += [itr // batches_per_epoch]\n","                time_val_arr += [batch_time_meter.val]\n","                time_avg_arr += [batch_time_meter.avg]\n","                nfe_f_arr += [f_nfe_meter.avg]\n","                nfe_b_arr += [b_nfe_meter.avg]\n","                train_acc_arr += [train_acc]\n","                test_acc_arr += [val_acc]\n","                    \n","    epoch_arr = np.asarray(epoch_arr)\n","    time_val_arr = np.asarray(time_val_arr)\n","    time_avg_arr = np.asarray(time_avg_arr)\n","    nfe_f_arr = np.asarray(nfe_f_arr)\n","    nfe_b_arr = np.asarray(nfe_b_arr)\n","    train_acc_arr = np.asarray(train_acc_arr)\n","    test_acc_arr = np.asarray(test_acc_arr)\n","    \n","    np.save(os.path.join(args.save, 'epoch_arr.npy'), epoch_arr)\n","    np.save(os.path.join(args.save, 'time_val_arr.npy'), time_val_arr)\n","    np.save(os.path.join(args.save, 'time_avg_arr.npy'), time_avg_arr)\n","    np.save(os.path.join(args.save, 'nfe_f_arr.npy'), nfe_f_arr)\n","    np.save(os.path.join(args.save, 'nfe_b_arr.npy'), nfe_b_arr)\n","    np.save(os.path.join(args.save, 'train_acc_arr.npy'), train_acc_arr)\n","    np.save(os.path.join(args.save, 'test_acc_arr.npy'), test_acc_arr)"],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0m__file__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'logs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'makedirs' is not defined"]}]},{"cell_type":"code","metadata":{"id":"Ner61Yv1L9ZC","executionInfo":{"status":"ok","timestamp":1617964389098,"user_tz":-60,"elapsed":1452,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["%%bash\n","rm -r /content/experiment_node1"],"execution_count":58,"outputs":[]},{"cell_type":"code","metadata":{"id":"gl0nftCJOgzw"},"source":["\"\"\"\n","--------------------------------------------------------------------------------------------------------------------------------------------\n","The code for Hessian analysis.\n","--------------------------------------------------------------------------------------------------------------------------------------------\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m75L8B4eOjXq","executionInfo":{"status":"ok","timestamp":1617964978011,"user_tz":-60,"elapsed":441,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["def get_manual_hessian(grads, parameters, show_iters=True):\n","  \"\"\"\n","  Calculation of the Hessian using nested for loops.\n","  Inputs:   - grads:        tuple of gradient tensors. Created using something \n","                            like grads = torch.autograd.grad(loss, parameters, create_graph=True).\n","            - parameters:   List of parameter objects. Created using something \n","                            like parameters = optimizer.param_groups[0]['params'].\n","            - show_iters:   True or False, depending on if the iteration number is to be shown during training. \n","                            Note that the iteration updates are not provided every row, but instead periodically \n","                            (roughly according to the number of parameters in the system).\n","  \"\"\"\n","  start = time.time()        \n","\n","  n_params = 0\n","  for param in parameters:\n","    n_params += torch.numel(param)\n","  grads2 = torch.zeros(n_params,n_params)            #Create an matrix of zeros thas has the same shape as the Hessian.\n","\n","  y_counter = 0                             #y_direction refers to row number in the Hessian.\n","\n","  for grad in grads:\n","      grad = torch.reshape(grad, [-1])                                  #Rearrange the gradient information into a vector.        \n","\n","      for j, g in enumerate(grad):\n","        x_counter = 0                                                   #x_direction refers to column number in the Hessian.\n","\n","        for l, param in enumerate(parameters):\n","          g2 = torch.autograd.grad(g, param, retain_graph=True)[0]      #Calculate the gradient of an element of the gradient wrt one layer's parameters.\n","          g2 = torch.reshape(g2, [-1])                                  #Reshape this into a vector.\n","          len = g2.shape[0]                       \n","          grads2[j+y_counter, x_counter:x_counter+len] = g2             #Indexing ensures that the second order derivatives are placed in the correct positions.\n","          x_counter += len\n","\n","      grads2 = grads2.to(device)\n","      y_counter += grad.shape[0]\n","\n","      if show_iters:\n","        print(\"Gradients calculated for row number \" + str(y_counter) + \".\")\n","  \n","  print('Time used was ', time.time() - start)\n","\n","  return grads2"],"execution_count":65,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":426},"id":"G8w3mIvwOlz5","executionInfo":{"status":"error","timestamp":1617965363914,"user_tz":-60,"elapsed":699,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}},"outputId":"d0dcab47-382d-40d7-f612-eb3119ceb615"},"source":["train_loader, test_loader, train_eval_loader = get_mnist_loaders(\n","        args.data_aug, args.batch_size, args.test_batch_size)\n","\n","optimizer = torch.optim.SGD(model.parameters(), lr=1)\n","optimizer.zero_grad()\n","\n","criterion = nn.CrossEntropyLoss().to(device)\n","for param_group in optimizer.param_groups:\n","            param_group['lr'] = lr_fn(itr)\n","\n","optimizer.zero_grad()\n","x, y = data_gen.__next__()\n","x = x.to(device)\n","y = y.to(device)\n","logits = model(x)\n","loss = criterion(logits, y)\n","\n","target = torch.zeros(len(zN))\n","for i in range(len(zN)):\n","  if zN[i]==0:\n","    target[i] = 0\n","  else:\n","    target[i] = 1\n","target = target.long()\n","\n","loss = criterion(pred_y, target)\n","grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n","parameters = optimizer.param_groups[0]['params']\n","\n","print('Obtaining manual hessian...')\n","manual_hessian = get_manual_hessian(grads, parameters)           #get manual hessian."],"execution_count":67,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# unserialize the data after having released the lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/reductions.py\u001b[0m in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrebuild_storage_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m     \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mdetach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;34m'''Get the fd.  This should only be called once.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0m_resource_sharer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mget_connection\u001b[0;34m(ident)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mident\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mClient\u001b[0;34m(address, family, authkey)\u001b[0m\n\u001b[1;32m    491\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSocketClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mSocketClient\u001b[0;34m(address)\u001b[0m\n\u001b[1;32m    619\u001b[0m         \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetblocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m         \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mConnection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\u001b[0m in \u001b[0;36minf_generator\u001b[0;34m(iterable)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1183\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1146\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1149\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    997\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m                 \u001b[0mpids_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DataLoader worker (pid(s) {}) exited unexpectedly'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpids_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmpty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 448, 449) exited unexpectedly"]}]}]}
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.1, nepochs=120, network='odenet', save='/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/experiment_node1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.1, nepochs=120, network='odenet', save='/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/experiment_node1', test_batch_size=1000, tol=0.001)
Sequential(
  (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1))
  (1): GroupNorm(32, 64, eps=1e-05, affine=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): GroupNorm(32, 64, eps=1e-05, affine=True)
  (5): ReLU(inplace=True)
  (6): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (norm1): GroupNorm(32, 64, eps=1e-05, affine=True)
      (relu): ReLU(inplace=True)
      (conv1): ConcatConv2d(
        (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)
      (conv2): ConcatConv2d(
        (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (norm3): GroupNorm(32, 64, eps=1e-05, affine=True)
    )
  )
  (8): GroupNorm(32, 64, eps=1e-05, affine=True)
  (9): ReLU(inplace=True)
  (10): AdaptiveAvgPool2d(output_size=(1, 1))
  (11): Flatten()
  (12): Linear(in_features=64, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1))
  (1): GroupNorm(32, 64, eps=1e-05, affine=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): GroupNorm(32, 64, eps=1e-05, affine=True)
  (5): ReLU(inplace=True)
  (6): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (norm1): GroupNorm(32, 64, eps=1e-05, affine=True)
      (relu): ReLU(inplace=True)
      (conv1): ConcatConv2d(
        (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)
      (conv2): ConcatConv2d(
        (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (norm3): GroupNorm(32, 64, eps=1e-05, affine=True)
    )
  )
  (8): GroupNorm(32, 64, eps=1e-05, affine=True)
  (9): ReLU(inplace=True)
  (10): AdaptiveAvgPool2d(output_size=(1, 1))
  (11): Flatten()
  (12): Linear(in_features=64, out_features=10, bias=True)
)
Number of parameters: 208266
Number of parameters: 208266
/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mnist_node.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1sLUaZ_DEMKH30zwclvQKQ2FNZUI_mF3K","authorship_tag":"ABX9TyPcyCLDteDUeOs03JfQdmb8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"sWmsXi5BIAtb","executionInfo":{"status":"ok","timestamp":1617965469256,"user_tz":-60,"elapsed":3337,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["%%capture\n","%%bash\n","pip install torchdiffeq"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"H353RG8fIDd3","executionInfo":{"status":"ok","timestamp":1617965472807,"user_tz":-60,"elapsed":6734,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["import os\n","import argparse\n","import logging\n","import time\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","\n","\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')\n","parser.add_argument('--tol', type=float, default=1e-3)\n","parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])\n","parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])\n","parser.add_argument('--nepochs', type=int, default=120)\n","parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])\n","parser.add_argument('--lr', type=float, default=0.1)\n","parser.add_argument('--batch_size', type=int, default=128)\n","parser.add_argument('--test_batch_size', type=int, default=1000)\n","\n","parser.add_argument('--save', type=str, default='./experiment_node1')\n","parser.add_argument('--gpu', type=int, default=0)\n","parser.add_argument('--debug', action='store_true')\n","args = parser.parse_args(args=[])\n","\n","args.save = '/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/experiment_node1'\n","\n","if args.adjoint:\n","    from torchdiffeq import odeint_adjoint as odeint\n","else:\n","    from torchdiffeq import odeint"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"agpZtaKwIKnA","executionInfo":{"status":"ok","timestamp":1617965472811,"user_tz":-60,"elapsed":6577,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["def conv3x3(in_planes, out_planes, stride=1):\n","    \"\"\"3x3 convolution with padding\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","\n","def conv1x1(in_planes, out_planes, stride=1):\n","    \"\"\"1x1 convolution\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n","\n","\n","def norm(dim):\n","    return nn.GroupNorm(min(32, dim), dim)\n","\n","\n","class ResBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None):\n","        super(ResBlock, self).__init__()\n","        self.norm1 = norm(inplanes)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.downsample = downsample\n","        self.conv1 = conv3x3(inplanes, planes, stride)\n","        self.norm2 = norm(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","\n","    def forward(self, x):\n","        shortcut = x\n","\n","        out = self.relu(self.norm1(x))\n","\n","        if self.downsample is not None:\n","            shortcut = self.downsample(out)\n","\n","        out = self.conv1(out)\n","        out = self.norm2(out)\n","        out = self.relu(out)\n","        out = self.conv2(out)\n","\n","        return out + shortcut\n","\n","\n","class ConcatConv2d(nn.Module):\n","\n","    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):\n","        super(ConcatConv2d, self).__init__()\n","        module = nn.ConvTranspose2d if transpose else nn.Conv2d\n","        self._layer = module(\n","            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,\n","            bias=bias\n","        )\n","\n","    def forward(self, t, x):\n","        tt = torch.ones_like(x[:, :1, :, :]) * t\n","        ttx = torch.cat([tt, x], 1)\n","        return self._layer(ttx)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"1-rlM_3sI-1k","executionInfo":{"status":"ok","timestamp":1617965472812,"user_tz":-60,"elapsed":6439,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["class ODEfunc(nn.Module):\n","\n","    def __init__(self, dim):\n","        super(ODEfunc, self).__init__()\n","        self.norm1 = norm(dim)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv1 = ConcatConv2d(dim, dim, 3, 1, 1)\n","        self.norm2 = norm(dim)\n","        self.conv2 = ConcatConv2d(dim, dim, 3, 1, 1)\n","        self.norm3 = norm(dim)\n","        self.nfe = 0\n","\n","    def forward(self, t, x):\n","        self.nfe += 1\n","        out = self.norm1(x)\n","        out = self.relu(out)\n","        out = self.conv1(t, out)\n","        out = self.norm2(out)\n","        out = self.relu(out)\n","        out = self.conv2(t, out)\n","        out = self.norm3(out)\n","        return out\n","\n","\n","class ODEBlock(nn.Module):\n","\n","    def __init__(self, odefunc):\n","        super(ODEBlock, self).__init__()\n","        self.odefunc = odefunc\n","        self.integration_time = torch.tensor([0, 1]).float()\n","\n","    def forward(self, x):\n","        self.integration_time = self.integration_time.type_as(x)\n","        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)\n","        return out[1]\n","\n","    @property\n","    def nfe(self):\n","        return self.odefunc.nfe\n","\n","    @nfe.setter\n","    def nfe(self, value):\n","        self.odefunc.nfe = value"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"xhFiCzNaJC0x","executionInfo":{"status":"ok","timestamp":1617965472813,"user_tz":-60,"elapsed":6236,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["class Flatten(nn.Module):\n","\n","    def __init__(self):\n","        super(Flatten, self).__init__()\n","\n","    def forward(self, x):\n","        shape = torch.prod(torch.tensor(x.shape[1:])).item()\n","        return x.view(-1, shape)\n","\n","\n","class RunningAverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","\n","    def __init__(self, momentum=0.99):\n","        self.momentum = momentum\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = None\n","        self.avg = 0\n","\n","    def update(self, val):\n","        if self.val is None:\n","            self.avg = val\n","        else:\n","            self.avg = self.avg * self.momentum + val * (1 - self.momentum)\n","        self.val = val"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"ltcgl1RIJG7U","executionInfo":{"status":"ok","timestamp":1617965473748,"user_tz":-60,"elapsed":922,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["def get_mnist_loaders(data_aug=False, batch_size=128, test_batch_size=1000, perc=1.0):\n","    if data_aug:\n","        transform_train = transforms.Compose([\n","            transforms.RandomCrop(28, padding=4),\n","            transforms.ToTensor(),\n","        ])\n","    else:\n","        transform_train = transforms.Compose([\n","            transforms.ToTensor(),\n","        ])\n","\n","    transform_test = transforms.Compose([\n","        transforms.ToTensor(),\n","    ])\n","\n","    train_loader = DataLoader(\n","        datasets.MNIST(root='.data/mnist', train=True, download=True, transform=transform_train), batch_size=batch_size,\n","        shuffle=True, num_workers=2, drop_last=True\n","    )\n","\n","    train_eval_loader = DataLoader(\n","        datasets.MNIST(root='.data/mnist', train=True, download=True, transform=transform_test),\n","        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True\n","    )\n","\n","    test_loader = DataLoader(\n","        datasets.MNIST(root='.data/mnist', train=False, download=True, transform=transform_test),\n","        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True\n","    )\n","\n","    return train_loader, test_loader, train_eval_loader\n","\n","\n","def inf_generator(iterable):\n","    \"\"\"Allows training with DataLoaders in a single infinite loop:\n","        for i, (x, y) in enumerate(inf_generator(train_loader)):\n","    \"\"\"\n","    iterator = iterable.__iter__()\n","    while True:\n","        try:\n","            yield iterator.__next__()\n","        except StopIteration:\n","            iterator = iterable.__iter__()\n","\n","\n","def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):\n","    initial_learning_rate = args.lr * batch_size / batch_denom\n","\n","    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]\n","    vals = [initial_learning_rate * decay for decay in decay_rates]\n","\n","    def learning_rate_fn(itr):\n","        lt = [itr < b for b in boundaries] + [True]\n","        i = np.argmax(lt)\n","        return vals[i]\n","\n","    return learning_rate_fn\n","\n","\n","def one_hot(x, K):\n","    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)\n","\n","\n","def accuracy(model, dataset_loader):\n","    total_correct = 0\n","    for x, y in dataset_loader:\n","        x = x.to(device)\n","        y = one_hot(np.array(y.numpy()), 10)\n","\n","        target_class = np.argmax(y, axis=1)\n","        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)\n","        total_correct += np.sum(predicted_class == target_class)\n","    return total_correct / len(dataset_loader.dataset)\n","\n","\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","\n","def makedirs(dirname):\n","    if not os.path.exists(dirname):\n","        os.makedirs(dirname)\n","\n","\n","def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):\n","    logger = logging.getLogger()\n","    if debug:\n","        level = logging.DEBUG\n","    else:\n","        level = logging.INFO\n","    logger.setLevel(level)\n","    if saving:\n","        info_file_handler = logging.FileHandler(logpath, mode=\"a\")\n","        info_file_handler.setLevel(level)\n","        logger.addHandler(info_file_handler)\n","    if displaying:\n","        console_handler = logging.StreamHandler()\n","        console_handler.setLevel(level)\n","        logger.addHandler(console_handler)\n","    logger.info(filepath)\n","    with open(filepath, \"r\") as f:\n","        logger.info(f.read())\n","\n","    for f in package_files:\n","        logger.info(f)\n","        with open(f, \"r\") as package_f:\n","            logger.info(package_f.read())\n","\n","    return logger\n"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":232},"id":"42RIwJ37JKQr","executionInfo":{"status":"error","timestamp":1617965618625,"user_tz":-60,"elapsed":746,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}},"outputId":"b88040a4-5d98-4598-9959-01f561aa3c13"},"source":["if __name__ == '__main__':\n","    __file__ = '/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb'\n","    makedirs(args.save)\n","    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))\n","    logger.info(args)\n","\n","    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')\n","\n","    is_odenet = args.network == 'odenet'\n","\n","    if args.downsampling_method == 'conv':\n","        downsampling_layers = [\n","            nn.Conv2d(1, 64, 3, 1),\n","            norm(64),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(64, 64, 4, 2, 1),\n","            norm(64),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(64, 64, 4, 2, 1),\n","        ]\n","    elif args.downsampling_method == 'res':\n","        downsampling_layers = [\n","            nn.Conv2d(1, 64, 3, 1),\n","            ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\n","            ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\n","        ]\n","\n","    feature_layers = [ODEBlock(ODEfunc(64))] if is_odenet else [ResBlock(64, 64) for _ in range(6)]\n","    fc_layers = [norm(64), nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d((1, 1)), Flatten(), nn.Linear(64, 10)]\n","\n","    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)\n","\n","    logger.info(model)\n","    logger.info('Number of parameters: {}'.format(count_parameters(model)))\n","\n","    criterion = nn.CrossEntropyLoss().to(device)\n","\n","    train_loader, test_loader, train_eval_loader = get_mnist_loaders(\n","        args.data_aug, args.batch_size, args.test_batch_size\n","    )\n","\n","    data_gen = inf_generator(train_loader)\n","    batches_per_epoch = len(train_loader)\n","\n","    lr_fn = learning_rate_with_decay(\n","        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],\n","        decay_rates=[1, 0.1, 0.01, 0.001]\n","    )\n","\n","    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)\n","\n","    best_acc = 0\n","    batch_time_meter = RunningAverageMeter()\n","    f_nfe_meter = RunningAverageMeter()\n","    b_nfe_meter = RunningAverageMeter()\n","    end = time.time()\n","    \n","    epoch_arr = []\n","    time_val_arr = []\n","    time_avg_arr = []\n","    nfe_f_arr = []\n","    nfe_b_arr = []\n","    train_acc_arr = []\n","    test_acc_arr = []\n","\n","    for itr in range(args.nepochs * batches_per_epoch):\n","\n","        for param_group in optimizer.param_groups:\n","            param_group['lr'] = lr_fn(itr)\n","\n","        optimizer.zero_grad()\n","        x, y = data_gen.__next__()\n","        x = x.to(device)\n","        y = y.to(device)\n","        logits = model(x)\n","        loss = criterion(logits, y)\n","\n","        grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n","        parameters = optimizer.param_groups[0]['params']\n","\n","        print('Obtaining manual hessian...')\n","        manual_hessian = get_manual_hessian(grads, parameters)           #get manual hessian.\n","\n","        if is_odenet:\n","            nfe_forward = feature_layers[0].nfe\n","            feature_layers[0].nfe = 0\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        if is_odenet:\n","            nfe_backward = feature_layers[0].nfe\n","            feature_layers[0].nfe = 0\n","\n","        batch_time_meter.update(time.time() - end)\n","        if is_odenet:\n","            f_nfe_meter.update(nfe_forward)\n","            b_nfe_meter.update(nfe_backward)\n","        end = time.time()\n","\n","        if itr % batches_per_epoch == 0:\n","            with torch.no_grad():\n","                train_acc = accuracy(model, train_eval_loader)\n","                val_acc = accuracy(model, test_loader)\n","                if val_acc > best_acc:\n","                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))\n","                    best_acc = val_acc\n","                logger.info(\n","                    \"Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | \"\n","                    \"Train Acc {:.4f} | Test Acc {:.4f}\".format(\n","                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,\n","                        b_nfe_meter.avg, train_acc, val_acc\n","                    )\n","                )\n","                epoch_arr += [itr // batches_per_epoch]\n","                time_val_arr += [batch_time_meter.val]\n","                time_avg_arr += [batch_time_meter.avg]\n","                nfe_f_arr += [f_nfe_meter.avg]\n","                nfe_b_arr += [b_nfe_meter.avg]\n","                train_acc_arr += [train_acc]\n","                test_acc_arr += [val_acc]\n","                    \n","    epoch_arr = np.asarray(epoch_arr)\n","    time_val_arr = np.asarray(time_val_arr)\n","    time_avg_arr = np.asarray(time_avg_arr)\n","    nfe_f_arr = np.asarray(nfe_f_arr)\n","    nfe_b_arr = np.asarray(nfe_b_arr)\n","    train_acc_arr = np.asarray(train_acc_arr)\n","    test_acc_arr = np.asarray(test_acc_arr)\n","    \n","    np.save(os.path.join(args.save, 'epoch_arr.npy'), epoch_arr)\n","    np.save(os.path.join(args.save, 'time_val_arr.npy'), time_val_arr)\n","    np.save(os.path.join(args.save, 'time_avg_arr.npy'), time_avg_arr)\n","    np.save(os.path.join(args.save, 'nfe_f_arr.npy'), nfe_f_arr)\n","    np.save(os.path.join(args.save, 'nfe_b_arr.npy'), nfe_b_arr)\n","    np.save(os.path.join(args.save, 'train_acc_arr.npy'), train_acc_arr)\n","    np.save(os.path.join(args.save, 'test_acc_arr.npy'), test_acc_arr)"],"execution_count":2,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0m__file__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'logs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'makedirs' is not defined"]}]},{"cell_type":"code","metadata":{"id":"Ner61Yv1L9ZC","executionInfo":{"status":"ok","timestamp":1617964389098,"user_tz":-60,"elapsed":1452,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["%%bash\n","rm -r /content/experiment_node1"],"execution_count":58,"outputs":[]},{"cell_type":"code","metadata":{"id":"gl0nftCJOgzw"},"source":["\"\"\"\n","--------------------------------------------------------------------------------------------------------------------------------------------\n","The code for Hessian analysis.\n","--------------------------------------------------------------------------------------------------------------------------------------------\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m75L8B4eOjXq","executionInfo":{"status":"ok","timestamp":1617965493846,"user_tz":-60,"elapsed":445,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["def get_manual_hessian(grads, parameters, show_iters=True):\n","  \"\"\"\n","  Calculation of the Hessian using nested for loops.\n","  Inputs:   - grads:        tuple of gradient tensors. Created using something \n","                            like grads = torch.autograd.grad(loss, parameters, create_graph=True).\n","            - parameters:   List of parameter objects. Created using something \n","                            like parameters = optimizer.param_groups[0]['params'].\n","            - show_iters:   True or False, depending on if the iteration number is to be shown during training. \n","                            Note that the iteration updates are not provided every row, but instead periodically \n","                            (roughly according to the number of parameters in the system).\n","  \"\"\"\n","  start = time.time()        \n","\n","  n_params = 0\n","  for param in parameters:\n","    n_params += torch.numel(param)\n","  grads2 = torch.zeros(n_params,n_params)            #Create an matrix of zeros thas has the same shape as the Hessian.\n","\n","  y_counter = 0                             #y_direction refers to row number in the Hessian.\n","\n","  for grad in grads:\n","      grad = torch.reshape(grad, [-1])                                  #Rearrange the gradient information into a vector.        \n","\n","      for j, g in enumerate(grad):\n","        x_counter = 0                                                   #x_direction refers to column number in the Hessian.\n","\n","        for l, param in enumerate(parameters):\n","          g2 = torch.autograd.grad(g, param, retain_graph=True)[0]      #Calculate the gradient of an element of the gradient wrt one layer's parameters.\n","          g2 = torch.reshape(g2, [-1])                                  #Reshape this into a vector.\n","          len = g2.shape[0]                       \n","          grads2[j+y_counter, x_counter:x_counter+len] = g2             #Indexing ensures that the second order derivatives are placed in the correct positions.\n","          x_counter += len\n","\n","      grads2 = grads2.to(device)\n","      y_counter += grad.shape[0]\n","\n","      if show_iters:\n","        print(\"Gradients calculated for row number \" + str(y_counter) + \".\")\n","  \n","  print('Time used was ', time.time() - start)\n","\n","  return grads2"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":232},"id":"-QPba5QNQx6V","executionInfo":{"status":"error","timestamp":1617965601039,"user_tz":-60,"elapsed":1139,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}},"outputId":"008f10c7-272a-431f-a1bd-c9dfbfc275ef"},"source":["__file__ = '/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb'\n","makedirs(args.save)\n","logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))\n","logger.info(args)\n","\n","device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')\n","\n","is_odenet = args.network == 'odenet'\n","\n","if args.downsampling_method == 'conv':\n","    downsampling_layers = [\n","        nn.Conv2d(1, 64, 3, 1),\n","        norm(64),\n","        nn.ReLU(inplace=True),\n","        nn.Conv2d(64, 64, 4, 2, 1),\n","        norm(64),\n","        nn.ReLU(inplace=True),\n","        nn.Conv2d(64, 64, 4, 2, 1),\n","    ]\n","elif args.downsampling_method == 'res':\n","    downsampling_layers = [\n","        nn.Conv2d(1, 64, 3, 1),\n","        ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\n","        ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\n","    ]\n","\n","feature_layers = [ODEBlock(ODEfunc(64))] if is_odenet else [ResBlock(64, 64) for _ in range(6)]\n","fc_layers = [norm(64), nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d((1, 1)), Flatten(), nn.Linear(64, 10)]\n","\n","model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)\n","\n","logger.info(model)\n","logger.info('Number of parameters: {}'.format(count_parameters(model)))\n","\n","criterion = nn.CrossEntropyLoss().to(device)\n","\n","train_loader, test_loader, train_eval_loader = get_mnist_loaders(\n","    args.data_aug, args.batch_size, args.test_batch_size\n",")\n","\n","data_gen = inf_generator(train_loader)\n","batches_per_epoch = len(train_loader)\n","\n","lr_fn = learning_rate_with_decay(\n","    args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],\n","    decay_rates=[1, 0.1, 0.01, 0.001]\n",")\n","\n","optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)\n","\n","best_acc = 0\n","batch_time_meter = RunningAverageMeter()\n","f_nfe_meter = RunningAverageMeter()\n","b_nfe_meter = RunningAverageMeter()\n","end = time.time()\n","\n","epoch_arr = []\n","time_val_arr = []\n","time_avg_arr = []\n","nfe_f_arr = []\n","nfe_b_arr = []\n","train_acc_arr = []\n","test_acc_arr = []\n","\n","for param_group in optimizer.param_groups:\n","    param_group['lr'] = lr_fn(itr)\n","\n","optimizer.zero_grad()\n","x, y = data_gen.__next__()\n","x = x.to(device)\n","y = y.to(device)\n","logits = model(x)\n","loss = criterion(logits, y)\n","\n","grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n","parameters = optimizer.param_groups[0]['params']\n","\n","print('Obtaining manual hessian...')\n","manual_hessian = get_manual_hessian(grads, parameters)           #get manual hessian."],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0m__file__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'logs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'makedirs' is not defined"]}]}]}
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.1, nepochs=120, network='odenet', save='/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/experiment_node1', test_batch_size=1000, tol=0.001)
Sequential(
  (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1))
  (1): GroupNorm(32, 64, eps=1e-05, affine=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): GroupNorm(32, 64, eps=1e-05, affine=True)
  (5): ReLU(inplace=True)
  (6): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (norm1): GroupNorm(32, 64, eps=1e-05, affine=True)
      (relu): ReLU(inplace=True)
      (conv1): ConcatConv2d(
        (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)
      (conv2): ConcatConv2d(
        (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (norm3): GroupNorm(32, 64, eps=1e-05, affine=True)
    )
  )
  (8): GroupNorm(32, 64, eps=1e-05, affine=True)
  (9): ReLU(inplace=True)
  (10): AdaptiveAvgPool2d(output_size=(1, 1))
  (11): Flatten()
  (12): Linear(in_features=64, out_features=10, bias=True)
)
Number of parameters: 208266
/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb
/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mnist_node.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1sLUaZ_DEMKH30zwclvQKQ2FNZUI_mF3K","authorship_tag":"ABX9TyPcyCLDteDUeOs03JfQdmb8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"sWmsXi5BIAtb","executionInfo":{"status":"ok","timestamp":1617965636705,"user_tz":-60,"elapsed":3803,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["%%capture\n","%%bash\n","pip install torchdiffeq"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"H353RG8fIDd3","executionInfo":{"status":"ok","timestamp":1617965639560,"user_tz":-60,"elapsed":6489,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["import os\n","import argparse\n","import logging\n","import time\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","\n","\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')\n","parser.add_argument('--tol', type=float, default=1e-3)\n","parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])\n","parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])\n","parser.add_argument('--nepochs', type=int, default=120)\n","parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])\n","parser.add_argument('--lr', type=float, default=0.1)\n","parser.add_argument('--batch_size', type=int, default=128)\n","parser.add_argument('--test_batch_size', type=int, default=1000)\n","\n","parser.add_argument('--save', type=str, default='./experiment_node1')\n","parser.add_argument('--gpu', type=int, default=0)\n","parser.add_argument('--debug', action='store_true')\n","args = parser.parse_args(args=[])\n","\n","args.save = '/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/experiment_node1'\n","\n","if args.adjoint:\n","    from torchdiffeq import odeint_adjoint as odeint\n","else:\n","    from torchdiffeq import odeint"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"agpZtaKwIKnA","executionInfo":{"status":"ok","timestamp":1617965639567,"user_tz":-60,"elapsed":6348,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["def conv3x3(in_planes, out_planes, stride=1):\n","    \"\"\"3x3 convolution with padding\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","\n","def conv1x1(in_planes, out_planes, stride=1):\n","    \"\"\"1x1 convolution\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n","\n","\n","def norm(dim):\n","    return nn.GroupNorm(min(32, dim), dim)\n","\n","\n","class ResBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None):\n","        super(ResBlock, self).__init__()\n","        self.norm1 = norm(inplanes)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.downsample = downsample\n","        self.conv1 = conv3x3(inplanes, planes, stride)\n","        self.norm2 = norm(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","\n","    def forward(self, x):\n","        shortcut = x\n","\n","        out = self.relu(self.norm1(x))\n","\n","        if self.downsample is not None:\n","            shortcut = self.downsample(out)\n","\n","        out = self.conv1(out)\n","        out = self.norm2(out)\n","        out = self.relu(out)\n","        out = self.conv2(out)\n","\n","        return out + shortcut\n","\n","\n","class ConcatConv2d(nn.Module):\n","\n","    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):\n","        super(ConcatConv2d, self).__init__()\n","        module = nn.ConvTranspose2d if transpose else nn.Conv2d\n","        self._layer = module(\n","            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,\n","            bias=bias\n","        )\n","\n","    def forward(self, t, x):\n","        tt = torch.ones_like(x[:, :1, :, :]) * t\n","        ttx = torch.cat([tt, x], 1)\n","        return self._layer(ttx)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"1-rlM_3sI-1k","executionInfo":{"status":"ok","timestamp":1617965639568,"user_tz":-60,"elapsed":6172,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["class ODEfunc(nn.Module):\n","\n","    def __init__(self, dim):\n","        super(ODEfunc, self).__init__()\n","        self.norm1 = norm(dim)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv1 = ConcatConv2d(dim, dim, 3, 1, 1)\n","        self.norm2 = norm(dim)\n","        self.conv2 = ConcatConv2d(dim, dim, 3, 1, 1)\n","        self.norm3 = norm(dim)\n","        self.nfe = 0\n","\n","    def forward(self, t, x):\n","        self.nfe += 1\n","        out = self.norm1(x)\n","        out = self.relu(out)\n","        out = self.conv1(t, out)\n","        out = self.norm2(out)\n","        out = self.relu(out)\n","        out = self.conv2(t, out)\n","        out = self.norm3(out)\n","        return out\n","\n","\n","class ODEBlock(nn.Module):\n","\n","    def __init__(self, odefunc):\n","        super(ODEBlock, self).__init__()\n","        self.odefunc = odefunc\n","        self.integration_time = torch.tensor([0, 1]).float()\n","\n","    def forward(self, x):\n","        self.integration_time = self.integration_time.type_as(x)\n","        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)\n","        return out[1]\n","\n","    @property\n","    def nfe(self):\n","        return self.odefunc.nfe\n","\n","    @nfe.setter\n","    def nfe(self, value):\n","        self.odefunc.nfe = value"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"xhFiCzNaJC0x","executionInfo":{"status":"ok","timestamp":1617965639570,"user_tz":-60,"elapsed":6060,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["class Flatten(nn.Module):\n","\n","    def __init__(self):\n","        super(Flatten, self).__init__()\n","\n","    def forward(self, x):\n","        shape = torch.prod(torch.tensor(x.shape[1:])).item()\n","        return x.view(-1, shape)\n","\n","\n","class RunningAverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","\n","    def __init__(self, momentum=0.99):\n","        self.momentum = momentum\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = None\n","        self.avg = 0\n","\n","    def update(self, val):\n","        if self.val is None:\n","            self.avg = val\n","        else:\n","            self.avg = self.avg * self.momentum + val * (1 - self.momentum)\n","        self.val = val"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"ltcgl1RIJG7U","executionInfo":{"status":"ok","timestamp":1617965640246,"user_tz":-60,"elapsed":6577,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["def get_mnist_loaders(data_aug=False, batch_size=128, test_batch_size=1000, perc=1.0):\n","    if data_aug:\n","        transform_train = transforms.Compose([\n","            transforms.RandomCrop(28, padding=4),\n","            transforms.ToTensor(),\n","        ])\n","    else:\n","        transform_train = transforms.Compose([\n","            transforms.ToTensor(),\n","        ])\n","\n","    transform_test = transforms.Compose([\n","        transforms.ToTensor(),\n","    ])\n","\n","    train_loader = DataLoader(\n","        datasets.MNIST(root='.data/mnist', train=True, download=True, transform=transform_train), batch_size=batch_size,\n","        shuffle=True, num_workers=2, drop_last=True\n","    )\n","\n","    train_eval_loader = DataLoader(\n","        datasets.MNIST(root='.data/mnist', train=True, download=True, transform=transform_test),\n","        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True\n","    )\n","\n","    test_loader = DataLoader(\n","        datasets.MNIST(root='.data/mnist', train=False, download=True, transform=transform_test),\n","        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True\n","    )\n","\n","    return train_loader, test_loader, train_eval_loader\n","\n","\n","def inf_generator(iterable):\n","    \"\"\"Allows training with DataLoaders in a single infinite loop:\n","        for i, (x, y) in enumerate(inf_generator(train_loader)):\n","    \"\"\"\n","    iterator = iterable.__iter__()\n","    while True:\n","        try:\n","            yield iterator.__next__()\n","        except StopIteration:\n","            iterator = iterable.__iter__()\n","\n","\n","def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):\n","    initial_learning_rate = args.lr * batch_size / batch_denom\n","\n","    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]\n","    vals = [initial_learning_rate * decay for decay in decay_rates]\n","\n","    def learning_rate_fn(itr):\n","        lt = [itr < b for b in boundaries] + [True]\n","        i = np.argmax(lt)\n","        return vals[i]\n","\n","    return learning_rate_fn\n","\n","\n","def one_hot(x, K):\n","    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)\n","\n","\n","def accuracy(model, dataset_loader):\n","    total_correct = 0\n","    for x, y in dataset_loader:\n","        x = x.to(device)\n","        y = one_hot(np.array(y.numpy()), 10)\n","\n","        target_class = np.argmax(y, axis=1)\n","        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)\n","        total_correct += np.sum(predicted_class == target_class)\n","    return total_correct / len(dataset_loader.dataset)\n","\n","\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","\n","def makedirs(dirname):\n","    if not os.path.exists(dirname):\n","        os.makedirs(dirname)\n","\n","\n","def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):\n","    logger = logging.getLogger()\n","    if debug:\n","        level = logging.DEBUG\n","    else:\n","        level = logging.INFO\n","    logger.setLevel(level)\n","    if saving:\n","        info_file_handler = logging.FileHandler(logpath, mode=\"a\")\n","        info_file_handler.setLevel(level)\n","        logger.addHandler(info_file_handler)\n","    if displaying:\n","        console_handler = logging.StreamHandler()\n","        console_handler.setLevel(level)\n","        logger.addHandler(console_handler)\n","    logger.info(filepath)\n","    with open(filepath, \"r\") as f:\n","        logger.info(f.read())\n","\n","    for f in package_files:\n","        logger.info(f)\n","        with open(f, \"r\") as package_f:\n","            logger.info(package_f.read())\n","\n","    return logger\n"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":939},"id":"42RIwJ37JKQr","executionInfo":{"status":"error","timestamp":1617965648740,"user_tz":-60,"elapsed":14934,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}},"outputId":"1aaf2bb1-ee38-4c9c-bf3f-c2cd78a30ce2"},"source":["if __name__ == '__main__':\n","    __file__ = '/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb'\n","    makedirs(args.save)\n","    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))\n","    logger.info(args)\n","\n","    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')\n","\n","    is_odenet = args.network == 'odenet'\n","\n","    if args.downsampling_method == 'conv':\n","        downsampling_layers = [\n","            nn.Conv2d(1, 64, 3, 1),\n","            norm(64),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(64, 64, 4, 2, 1),\n","            norm(64),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(64, 64, 4, 2, 1),\n","        ]\n","    elif args.downsampling_method == 'res':\n","        downsampling_layers = [\n","            nn.Conv2d(1, 64, 3, 1),\n","            ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\n","            ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\n","        ]\n","\n","    feature_layers = [ODEBlock(ODEfunc(64))] if is_odenet else [ResBlock(64, 64) for _ in range(6)]\n","    fc_layers = [norm(64), nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d((1, 1)), Flatten(), nn.Linear(64, 10)]\n","\n","    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)\n","\n","    logger.info(model)\n","    logger.info('Number of parameters: {}'.format(count_parameters(model)))\n","\n","    criterion = nn.CrossEntropyLoss().to(device)\n","\n","    train_loader, test_loader, train_eval_loader = get_mnist_loaders(\n","        args.data_aug, args.batch_size, args.test_batch_size\n","    )\n","\n","    data_gen = inf_generator(train_loader)\n","    batches_per_epoch = len(train_loader)\n","\n","    lr_fn = learning_rate_with_decay(\n","        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],\n","        decay_rates=[1, 0.1, 0.01, 0.001]\n","    )\n","\n","    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)\n","\n","    best_acc = 0\n","    batch_time_meter = RunningAverageMeter()\n","    f_nfe_meter = RunningAverageMeter()\n","    b_nfe_meter = RunningAverageMeter()\n","    end = time.time()\n","    \n","    epoch_arr = []\n","    time_val_arr = []\n","    time_avg_arr = []\n","    nfe_f_arr = []\n","    nfe_b_arr = []\n","    train_acc_arr = []\n","    test_acc_arr = []\n","\n","    for itr in range(args.nepochs * batches_per_epoch):\n","\n","        for param_group in optimizer.param_groups:\n","            param_group['lr'] = lr_fn(itr)\n","\n","        optimizer.zero_grad()\n","        x, y = data_gen.__next__()\n","        x = x.to(device)\n","        y = y.to(device)\n","        logits = model(x)\n","        loss = criterion(logits, y)\n","\n","        grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n","        parameters = optimizer.param_groups[0]['params']\n","\n","        print('Obtaining manual hessian...')\n","        manual_hessian = get_manual_hessian(grads, parameters)           #get manual hessian.\n","\n","        if is_odenet:\n","            nfe_forward = feature_layers[0].nfe\n","            feature_layers[0].nfe = 0\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        if is_odenet:\n","            nfe_backward = feature_layers[0].nfe\n","            feature_layers[0].nfe = 0\n","\n","        batch_time_meter.update(time.time() - end)\n","        if is_odenet:\n","            f_nfe_meter.update(nfe_forward)\n","            b_nfe_meter.update(nfe_backward)\n","        end = time.time()\n","\n","        if itr % batches_per_epoch == 0:\n","            with torch.no_grad():\n","                train_acc = accuracy(model, train_eval_loader)\n","                val_acc = accuracy(model, test_loader)\n","                if val_acc > best_acc:\n","                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))\n","                    best_acc = val_acc\n","                logger.info(\n","                    \"Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | \"\n","                    \"Train Acc {:.4f} | Test Acc {:.4f}\".format(\n","                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,\n","                        b_nfe_meter.avg, train_acc, val_acc\n","                    )\n","                )\n","                epoch_arr += [itr // batches_per_epoch]\n","                time_val_arr += [batch_time_meter.val]\n","                time_avg_arr += [batch_time_meter.avg]\n","                nfe_f_arr += [f_nfe_meter.avg]\n","                nfe_b_arr += [b_nfe_meter.avg]\n","                train_acc_arr += [train_acc]\n","                test_acc_arr += [val_acc]\n","                    \n","    epoch_arr = np.asarray(epoch_arr)\n","    time_val_arr = np.asarray(time_val_arr)\n","    time_avg_arr = np.asarray(time_avg_arr)\n","    nfe_f_arr = np.asarray(nfe_f_arr)\n","    nfe_b_arr = np.asarray(nfe_b_arr)\n","    train_acc_arr = np.asarray(train_acc_arr)\n","    test_acc_arr = np.asarray(test_acc_arr)\n","    \n","    np.save(os.path.join(args.save, 'epoch_arr.npy'), epoch_arr)\n","    np.save(os.path.join(args.save, 'time_val_arr.npy'), time_val_arr)\n","    np.save(os.path.join(args.save, 'time_avg_arr.npy'), time_avg_arr)\n","    np.save(os.path.join(args.save, 'nfe_f_arr.npy'), nfe_f_arr)\n","    np.save(os.path.join(args.save, 'nfe_b_arr.npy'), nfe_b_arr)\n","    np.save(os.path.join(args.save, 'train_acc_arr.npy'), train_acc_arr)\n","    np.save(os.path.join(args.save, 'test_acc_arr.npy'), test_acc_arr)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\n","{\"nbformat\":4,\"nbformat_minor\":0,\"metadata\":{\"colab\":{\"name\":\"mnist_node.ipynb\",\"provenance\":[],\"collapsed_sections\":[],\"mount_file_id\":\"1sLUaZ_DEMKH30zwclvQKQ2FNZUI_mF3K\",\"authorship_tag\":\"ABX9TyPcyCLDteDUeOs03JfQdmb8\"},\"kernelspec\":{\"name\":\"python3\",\"display_name\":\"Python 3\"},\"language_info\":{\"name\":\"python\"}},\"cells\":[{\"cell_type\":\"code\",\"metadata\":{\"id\":\"sWmsXi5BIAtb\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617965469256,\"user_tz\":-60,\"elapsed\":3337,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"%%capture\\n\",\"%%bash\\n\",\"pip install torchdiffeq\"],\"execution_count\":2,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"H353RG8fIDd3\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617965472807,\"user_tz\":-60,\"elapsed\":6734,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"import os\\n\",\"import argparse\\n\",\"import logging\\n\",\"import time\\n\",\"import numpy as np\\n\",\"import torch\\n\",\"import torch.nn as nn\\n\",\"from torch.utils.data import DataLoader\\n\",\"import torchvision.datasets as datasets\\n\",\"import torchvision.transforms as transforms\\n\",\"\\n\",\"\\n\",\"parser = argparse.ArgumentParser()\\n\",\"parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')\\n\",\"parser.add_argument('--tol', type=float, default=1e-3)\\n\",\"parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])\\n\",\"parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])\\n\",\"parser.add_argument('--nepochs', type=int, default=120)\\n\",\"parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])\\n\",\"parser.add_argument('--lr', type=float, default=0.1)\\n\",\"parser.add_argument('--batch_size', type=int, default=128)\\n\",\"parser.add_argument('--test_batch_size', type=int, default=1000)\\n\",\"\\n\",\"parser.add_argument('--save', type=str, default='./experiment_node1')\\n\",\"parser.add_argument('--gpu', type=int, default=0)\\n\",\"parser.add_argument('--debug', action='store_true')\\n\",\"args = parser.parse_args(args=[])\\n\",\"\\n\",\"args.save = '/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/experiment_node1'\\n\",\"\\n\",\"if args.adjoint:\\n\",\"    from torchdiffeq import odeint_adjoint as odeint\\n\",\"else:\\n\",\"    from torchdiffeq import odeint\"],\"execution_count\":3,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"agpZtaKwIKnA\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617965472811,\"user_tz\":-60,\"elapsed\":6577,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"def conv3x3(in_planes, out_planes, stride=1):\\n\",\"    \\\"\\\"\\\"3x3 convolution with padding\\\"\\\"\\\"\\n\",\"    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\\n\",\"\\n\",\"\\n\",\"def conv1x1(in_planes, out_planes, stride=1):\\n\",\"    \\\"\\\"\\\"1x1 convolution\\\"\\\"\\\"\\n\",\"    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\\n\",\"\\n\",\"\\n\",\"def norm(dim):\\n\",\"    return nn.GroupNorm(min(32, dim), dim)\\n\",\"\\n\",\"\\n\",\"class ResBlock(nn.Module):\\n\",\"    expansion = 1\\n\",\"\\n\",\"    def __init__(self, inplanes, planes, stride=1, downsample=None):\\n\",\"        super(ResBlock, self).__init__()\\n\",\"        self.norm1 = norm(inplanes)\\n\",\"        self.relu = nn.ReLU(inplace=True)\\n\",\"        self.downsample = downsample\\n\",\"        self.conv1 = conv3x3(inplanes, planes, stride)\\n\",\"        self.norm2 = norm(planes)\\n\",\"        self.conv2 = conv3x3(planes, planes)\\n\",\"\\n\",\"    def forward(self, x):\\n\",\"        shortcut = x\\n\",\"\\n\",\"        out = self.relu(self.norm1(x))\\n\",\"\\n\",\"        if self.downsample is not None:\\n\",\"            shortcut = self.downsample(out)\\n\",\"\\n\",\"        out = self.conv1(out)\\n\",\"        out = self.norm2(out)\\n\",\"        out = self.relu(out)\\n\",\"        out = self.conv2(out)\\n\",\"\\n\",\"        return out + shortcut\\n\",\"\\n\",\"\\n\",\"class ConcatConv2d(nn.Module):\\n\",\"\\n\",\"    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):\\n\",\"        super(ConcatConv2d, self).__init__()\\n\",\"        module = nn.ConvTranspose2d if transpose else nn.Conv2d\\n\",\"        self._layer = module(\\n\",\"            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,\\n\",\"            bias=bias\\n\",\"        )\\n\",\"\\n\",\"    def forward(self, t, x):\\n\",\"        tt = torch.ones_like(x[:, :1, :, :]) * t\\n\",\"        ttx = torch.cat([tt, x], 1)\\n\",\"        return self._layer(ttx)\"],\"execution_count\":4,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"1-rlM_3sI-1k\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617965472812,\"user_tz\":-60,\"elapsed\":6439,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"class ODEfunc(nn.Module):\\n\",\"\\n\",\"    def __init__(self, dim):\\n\",\"        super(ODEfunc, self).__init__()\\n\",\"        self.norm1 = norm(dim)\\n\",\"        self.relu = nn.ReLU(inplace=True)\\n\",\"        self.conv1 = ConcatConv2d(dim, dim, 3, 1, 1)\\n\",\"        self.norm2 = norm(dim)\\n\",\"        self.conv2 = ConcatConv2d(dim, dim, 3, 1, 1)\\n\",\"        self.norm3 = norm(dim)\\n\",\"        self.nfe = 0\\n\",\"\\n\",\"    def forward(self, t, x):\\n\",\"        self.nfe += 1\\n\",\"        out = self.norm1(x)\\n\",\"        out = self.relu(out)\\n\",\"        out = self.conv1(t, out)\\n\",\"        out = self.norm2(out)\\n\",\"        out = self.relu(out)\\n\",\"        out = self.conv2(t, out)\\n\",\"        out = self.norm3(out)\\n\",\"        return out\\n\",\"\\n\",\"\\n\",\"class ODEBlock(nn.Module):\\n\",\"\\n\",\"    def __init__(self, odefunc):\\n\",\"        super(ODEBlock, self).__init__()\\n\",\"        self.odefunc = odefunc\\n\",\"        self.integration_time = torch.tensor([0, 1]).float()\\n\",\"\\n\",\"    def forward(self, x):\\n\",\"        self.integration_time = self.integration_time.type_as(x)\\n\",\"        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)\\n\",\"        return out[1]\\n\",\"\\n\",\"    @property\\n\",\"    def nfe(self):\\n\",\"        return self.odefunc.nfe\\n\",\"\\n\",\"    @nfe.setter\\n\",\"    def nfe(self, value):\\n\",\"        self.odefunc.nfe = value\"],\"execution_count\":5,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"xhFiCzNaJC0x\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617965472813,\"user_tz\":-60,\"elapsed\":6236,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"class Flatten(nn.Module):\\n\",\"\\n\",\"    def __init__(self):\\n\",\"        super(Flatten, self).__init__()\\n\",\"\\n\",\"    def forward(self, x):\\n\",\"        shape = torch.prod(torch.tensor(x.shape[1:])).item()\\n\",\"        return x.view(-1, shape)\\n\",\"\\n\",\"\\n\",\"class RunningAverageMeter(object):\\n\",\"    \\\"\\\"\\\"Computes and stores the average and current value\\\"\\\"\\\"\\n\",\"\\n\",\"    def __init__(self, momentum=0.99):\\n\",\"        self.momentum = momentum\\n\",\"        self.reset()\\n\",\"\\n\",\"    def reset(self):\\n\",\"        self.val = None\\n\",\"        self.avg = 0\\n\",\"\\n\",\"    def update(self, val):\\n\",\"        if self.val is None:\\n\",\"            self.avg = val\\n\",\"        else:\\n\",\"            self.avg = self.avg * self.momentum + val * (1 - self.momentum)\\n\",\"        self.val = val\"],\"execution_count\":6,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"ltcgl1RIJG7U\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617965473748,\"user_tz\":-60,\"elapsed\":922,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"def get_mnist_loaders(data_aug=False, batch_size=128, test_batch_size=1000, perc=1.0):\\n\",\"    if data_aug:\\n\",\"        transform_train = transforms.Compose([\\n\",\"            transforms.RandomCrop(28, padding=4),\\n\",\"            transforms.ToTensor(),\\n\",\"        ])\\n\",\"    else:\\n\",\"        transform_train = transforms.Compose([\\n\",\"            transforms.ToTensor(),\\n\",\"        ])\\n\",\"\\n\",\"    transform_test = transforms.Compose([\\n\",\"        transforms.ToTensor(),\\n\",\"    ])\\n\",\"\\n\",\"    train_loader = DataLoader(\\n\",\"        datasets.MNIST(root='.data/mnist', train=True, download=True, transform=transform_train), batch_size=batch_size,\\n\",\"        shuffle=True, num_workers=2, drop_last=True\\n\",\"    )\\n\",\"\\n\",\"    train_eval_loader = DataLoader(\\n\",\"        datasets.MNIST(root='.data/mnist', train=True, download=True, transform=transform_test),\\n\",\"        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True\\n\",\"    )\\n\",\"\\n\",\"    test_loader = DataLoader(\\n\",\"        datasets.MNIST(root='.data/mnist', train=False, download=True, transform=transform_test),\\n\",\"        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True\\n\",\"    )\\n\",\"\\n\",\"    return train_loader, test_loader, train_eval_loader\\n\",\"\\n\",\"\\n\",\"def inf_generator(iterable):\\n\",\"    \\\"\\\"\\\"Allows training with DataLoaders in a single infinite loop:\\n\",\"        for i, (x, y) in enumerate(inf_generator(train_loader)):\\n\",\"    \\\"\\\"\\\"\\n\",\"    iterator = iterable.__iter__()\\n\",\"    while True:\\n\",\"        try:\\n\",\"            yield iterator.__next__()\\n\",\"        except StopIteration:\\n\",\"            iterator = iterable.__iter__()\\n\",\"\\n\",\"\\n\",\"def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):\\n\",\"    initial_learning_rate = args.lr * batch_size / batch_denom\\n\",\"\\n\",\"    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]\\n\",\"    vals = [initial_learning_rate * decay for decay in decay_rates]\\n\",\"\\n\",\"    def learning_rate_fn(itr):\\n\",\"        lt = [itr < b for b in boundaries] + [True]\\n\",\"        i = np.argmax(lt)\\n\",\"        return vals[i]\\n\",\"\\n\",\"    return learning_rate_fn\\n\",\"\\n\",\"\\n\",\"def one_hot(x, K):\\n\",\"    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)\\n\",\"\\n\",\"\\n\",\"def accuracy(model, dataset_loader):\\n\",\"    total_correct = 0\\n\",\"    for x, y in dataset_loader:\\n\",\"        x = x.to(device)\\n\",\"        y = one_hot(np.array(y.numpy()), 10)\\n\",\"\\n\",\"        target_class = np.argmax(y, axis=1)\\n\",\"        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)\\n\",\"        total_correct += np.sum(predicted_class == target_class)\\n\",\"    return total_correct / len(dataset_loader.dataset)\\n\",\"\\n\",\"\\n\",\"def count_parameters(model):\\n\",\"    return sum(p.numel() for p in model.parameters() if p.requires_grad)\\n\",\"\\n\",\"\\n\",\"def makedirs(dirname):\\n\",\"    if not os.path.exists(dirname):\\n\",\"        os.makedirs(dirname)\\n\",\"\\n\",\"\\n\",\"def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):\\n\",\"    logger = logging.getLogger()\\n\",\"    if debug:\\n\",\"        level = logging.DEBUG\\n\",\"    else:\\n\",\"        level = logging.INFO\\n\",\"    logger.setLevel(level)\\n\",\"    if saving:\\n\",\"        info_file_handler = logging.FileHandler(logpath, mode=\\\"a\\\")\\n\",\"        info_file_handler.setLevel(level)\\n\",\"        logger.addHandler(info_file_handler)\\n\",\"    if displaying:\\n\",\"        console_handler = logging.StreamHandler()\\n\",\"        console_handler.setLevel(level)\\n\",\"        logger.addHandler(console_handler)\\n\",\"    logger.info(filepath)\\n\",\"    with open(filepath, \\\"r\\\") as f:\\n\",\"        logger.info(f.read())\\n\",\"\\n\",\"    for f in package_files:\\n\",\"        logger.info(f)\\n\",\"        with open(f, \\\"r\\\") as package_f:\\n\",\"            logger.info(package_f.read())\\n\",\"\\n\",\"    return logger\\n\"],\"execution_count\":7,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"colab\":{\"base_uri\":\"https://localhost:8080/\",\"height\":232},\"id\":\"42RIwJ37JKQr\",\"executionInfo\":{\"status\":\"error\",\"timestamp\":1617965618625,\"user_tz\":-60,\"elapsed\":746,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}},\"outputId\":\"b88040a4-5d98-4598-9959-01f561aa3c13\"},\"source\":[\"if __name__ == '__main__':\\n\",\"    __file__ = '/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb'\\n\",\"    makedirs(args.save)\\n\",\"    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))\\n\",\"    logger.info(args)\\n\",\"\\n\",\"    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')\\n\",\"\\n\",\"    is_odenet = args.network == 'odenet'\\n\",\"\\n\",\"    if args.downsampling_method == 'conv':\\n\",\"        downsampling_layers = [\\n\",\"            nn.Conv2d(1, 64, 3, 1),\\n\",\"            norm(64),\\n\",\"            nn.ReLU(inplace=True),\\n\",\"            nn.Conv2d(64, 64, 4, 2, 1),\\n\",\"            norm(64),\\n\",\"            nn.ReLU(inplace=True),\\n\",\"            nn.Conv2d(64, 64, 4, 2, 1),\\n\",\"        ]\\n\",\"    elif args.downsampling_method == 'res':\\n\",\"        downsampling_layers = [\\n\",\"            nn.Conv2d(1, 64, 3, 1),\\n\",\"            ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\\n\",\"            ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\\n\",\"        ]\\n\",\"\\n\",\"    feature_layers = [ODEBlock(ODEfunc(64))] if is_odenet else [ResBlock(64, 64) for _ in range(6)]\\n\",\"    fc_layers = [norm(64), nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d((1, 1)), Flatten(), nn.Linear(64, 10)]\\n\",\"\\n\",\"    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)\\n\",\"\\n\",\"    logger.info(model)\\n\",\"    logger.info('Number of parameters: {}'.format(count_parameters(model)))\\n\",\"\\n\",\"    criterion = nn.CrossEntropyLoss().to(device)\\n\",\"\\n\",\"    train_loader, test_loader, train_eval_loader = get_mnist_loaders(\\n\",\"        args.data_aug, args.batch_size, args.test_batch_size\\n\",\"    )\\n\",\"\\n\",\"    data_gen = inf_generator(train_loader)\\n\",\"    batches_per_epoch = len(train_loader)\\n\",\"\\n\",\"    lr_fn = learning_rate_with_decay(\\n\",\"        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],\\n\",\"        decay_rates=[1, 0.1, 0.01, 0.001]\\n\",\"    )\\n\",\"\\n\",\"    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)\\n\",\"\\n\",\"    best_acc = 0\\n\",\"    batch_time_meter = RunningAverageMeter()\\n\",\"    f_nfe_meter = RunningAverageMeter()\\n\",\"    b_nfe_meter = RunningAverageMeter()\\n\",\"    end = time.time()\\n\",\"    \\n\",\"    epoch_arr = []\\n\",\"    time_val_arr = []\\n\",\"    time_avg_arr = []\\n\",\"    nfe_f_arr = []\\n\",\"    nfe_b_arr = []\\n\",\"    train_acc_arr = []\\n\",\"    test_acc_arr = []\\n\",\"\\n\",\"    for itr in range(args.nepochs * batches_per_epoch):\\n\",\"\\n\",\"        for param_group in optimizer.param_groups:\\n\",\"            param_group['lr'] = lr_fn(itr)\\n\",\"\\n\",\"        optimizer.zero_grad()\\n\",\"        x, y = data_gen.__next__()\\n\",\"        x = x.to(device)\\n\",\"        y = y.to(device)\\n\",\"        logits = model(x)\\n\",\"        loss = criterion(logits, y)\\n\",\"\\n\",\"        grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)\\n\",\"        parameters = optimizer.param_groups[0]['params']\\n\",\"\\n\",\"        print('Obtaining manual hessian...')\\n\",\"        manual_hessian = get_manual_hessian(grads, parameters)           #get manual hessian.\\n\",\"\\n\",\"        if is_odenet:\\n\",\"            nfe_forward = feature_layers[0].nfe\\n\",\"            feature_layers[0].nfe = 0\\n\",\"\\n\",\"        loss.backward()\\n\",\"        optimizer.step()\\n\",\"\\n\",\"        if is_odenet:\\n\",\"            nfe_backward = feature_layers[0].nfe\\n\",\"            feature_layers[0].nfe = 0\\n\",\"\\n\",\"        batch_time_meter.update(time.time() - end)\\n\",\"        if is_odenet:\\n\",\"            f_nfe_meter.update(nfe_forward)\\n\",\"            b_nfe_meter.update(nfe_backward)\\n\",\"        end = time.time()\\n\",\"\\n\",\"        if itr % batches_per_epoch == 0:\\n\",\"            with torch.no_grad():\\n\",\"                train_acc = accuracy(model, train_eval_loader)\\n\",\"                val_acc = accuracy(model, test_loader)\\n\",\"                if val_acc > best_acc:\\n\",\"                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))\\n\",\"                    best_acc = val_acc\\n\",\"                logger.info(\\n\",\"                    \\\"Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | \\\"\\n\",\"                    \\\"Train Acc {:.4f} | Test Acc {:.4f}\\\".format(\\n\",\"                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,\\n\",\"                        b_nfe_meter.avg, train_acc, val_acc\\n\",\"                    )\\n\",\"                )\\n\",\"                epoch_arr += [itr // batches_per_epoch]\\n\",\"                time_val_arr += [batch_time_meter.val]\\n\",\"                time_avg_arr += [batch_time_meter.avg]\\n\",\"                nfe_f_arr += [f_nfe_meter.avg]\\n\",\"                nfe_b_arr += [b_nfe_meter.avg]\\n\",\"                train_acc_arr += [train_acc]\\n\",\"                test_acc_arr += [val_acc]\\n\",\"                    \\n\",\"    epoch_arr = np.asarray(epoch_arr)\\n\",\"    time_val_arr = np.asarray(time_val_arr)\\n\",\"    time_avg_arr = np.asarray(time_avg_arr)\\n\",\"    nfe_f_arr = np.asarray(nfe_f_arr)\\n\",\"    nfe_b_arr = np.asarray(nfe_b_arr)\\n\",\"    train_acc_arr = np.asarray(train_acc_arr)\\n\",\"    test_acc_arr = np.asarray(test_acc_arr)\\n\",\"    \\n\",\"    np.save(os.path.join(args.save, 'epoch_arr.npy'), epoch_arr)\\n\",\"    np.save(os.path.join(args.save, 'time_val_arr.npy'), time_val_arr)\\n\",\"    np.save(os.path.join(args.save, 'time_avg_arr.npy'), time_avg_arr)\\n\",\"    np.save(os.path.join(args.save, 'nfe_f_arr.npy'), nfe_f_arr)\\n\",\"    np.save(os.path.join(args.save, 'nfe_b_arr.npy'), nfe_b_arr)\\n\",\"    np.save(os.path.join(args.save, 'train_acc_arr.npy'), train_acc_arr)\\n\",\"    np.save(os.path.join(args.save, 'test_acc_arr.npy'), test_acc_arr)\"],\"execution_count\":2,\"outputs\":[{\"output_type\":\"error\",\"ename\":\"NameError\",\"evalue\":\"ignored\",\"traceback\":[\"\\u001b[0;31m---------------------------------------------------------------------------\\u001b[0m\",\"\\u001b[0;31mNameError\\u001b[0m                                 Traceback (most recent call last)\",\"\\u001b[0;32m/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\\u001b[0m in \\u001b[0;36m<module>\\u001b[0;34m()\\u001b[0m\\n\\u001b[1;32m      1\\u001b[0m \\u001b[0;32mif\\u001b[0m \\u001b[0m__name__\\u001b[0m \\u001b[0;34m==\\u001b[0m \\u001b[0;34m'__main__'\\u001b[0m\\u001b[0;34m:\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m      2\\u001b[0m     \\u001b[0m__file__\\u001b[0m \\u001b[0;34m=\\u001b[0m \\u001b[0;34m'/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb'\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m----> 3\\u001b[0;31m     \\u001b[0mmakedirs\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0margs\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0msave\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0m\\u001b[1;32m      4\\u001b[0m     \\u001b[0mlogger\\u001b[0m \\u001b[0;34m=\\u001b[0m \\u001b[0mget_logger\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mlogpath\\u001b[0m\\u001b[0;34m=\\u001b[0m\\u001b[0mos\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mpath\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mjoin\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0margs\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0msave\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0;34m'logs'\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0mfilepath\\u001b[0m\\u001b[0;34m=\\u001b[0m\\u001b[0mos\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mpath\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mabspath\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0m__file__\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m      5\\u001b[0m     \\u001b[0mlogger\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0minfo\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0margs\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\",\"\\u001b[0;31mNameError\\u001b[0m: name 'makedirs' is not defined\"]}]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"Ner61Yv1L9ZC\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617964389098,\"user_tz\":-60,\"elapsed\":1452,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"%%bash\\n\",\"rm -r /content/experiment_node1\"],\"execution_count\":58,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"gl0nftCJOgzw\"},\"source\":[\"\\\"\\\"\\\"\\n\",\"--------------------------------------------------------------------------------------------------------------------------------------------\\n\",\"The code for Hessian analysis.\\n\",\"--------------------------------------------------------------------------------------------------------------------------------------------\\n\",\"\\\"\\\"\\\"\"],\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"m75L8B4eOjXq\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617965493846,\"user_tz\":-60,\"elapsed\":445,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"def get_manual_hessian(grads, parameters, show_iters=True):\\n\",\"  \\\"\\\"\\\"\\n\",\"  Calculation of the Hessian using nested for loops.\\n\",\"  Inputs:   - grads:        tuple of gradient tensors. Created using something \\n\",\"                            like grads = torch.autograd.grad(loss, parameters, create_graph=True).\\n\",\"            - parameters:   List of parameter objects. Created using something \\n\",\"                            like parameters = optimizer.param_groups[0]['params'].\\n\",\"            - show_iters:   True or False, depending on if the iteration number is to be shown during training. \\n\",\"                            Note that the iteration updates are not provided every row, but instead periodically \\n\",\"                            (roughly according to the number of parameters in the system).\\n\",\"  \\\"\\\"\\\"\\n\",\"  start = time.time()        \\n\",\"\\n\",\"  n_params = 0\\n\",\"  for param in parameters:\\n\",\"    n_params += torch.numel(param)\\n\",\"  grads2 = torch.zeros(n_params,n_params)            #Create an matrix of zeros thas has the same shape as the Hessian.\\n\",\"\\n\",\"  y_counter = 0                             #y_direction refers to row number in the Hessian.\\n\",\"\\n\",\"  for grad in grads:\\n\",\"      grad = torch.reshape(grad, [-1])                                  #Rearrange the gradient information into a vector.        \\n\",\"\\n\",\"      for j, g in enumerate(grad):\\n\",\"        x_counter = 0                                                   #x_direction refers to column number in the Hessian.\\n\",\"\\n\",\"        for l, param in enumerate(parameters):\\n\",\"          g2 = torch.autograd.grad(g, param, retain_graph=True)[0]      #Calculate the gradient of an element of the gradient wrt one layer's parameters.\\n\",\"          g2 = torch.reshape(g2, [-1])                                  #Reshape this into a vector.\\n\",\"          len = g2.shape[0]                       \\n\",\"          grads2[j+y_counter, x_counter:x_counter+len] = g2             #Indexing ensures that the second order derivatives are placed in the correct positions.\\n\",\"          x_counter += len\\n\",\"\\n\",\"      grads2 = grads2.to(device)\\n\",\"      y_counter += grad.shape[0]\\n\",\"\\n\",\"      if show_iters:\\n\",\"        print(\\\"Gradients calculated for row number \\\" + str(y_counter) + \\\".\\\")\\n\",\"  \\n\",\"  print('Time used was ', time.time() - start)\\n\",\"\\n\",\"  return grads2\"],\"execution_count\":9,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"colab\":{\"base_uri\":\"https://localhost:8080/\",\"height\":232},\"id\":\"-QPba5QNQx6V\",\"executionInfo\":{\"status\":\"error\",\"timestamp\":1617965601039,\"user_tz\":-60,\"elapsed\":1139,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}},\"outputId\":\"008f10c7-272a-431f-a1bd-c9dfbfc275ef\"},\"source\":[\"__file__ = '/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb'\\n\",\"makedirs(args.save)\\n\",\"logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))\\n\",\"logger.info(args)\\n\",\"\\n\",\"device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')\\n\",\"\\n\",\"is_odenet = args.network == 'odenet'\\n\",\"\\n\",\"if args.downsampling_method == 'conv':\\n\",\"    downsampling_layers = [\\n\",\"        nn.Conv2d(1, 64, 3, 1),\\n\",\"        norm(64),\\n\",\"        nn.ReLU(inplace=True),\\n\",\"        nn.Conv2d(64, 64, 4, 2, 1),\\n\",\"        norm(64),\\n\",\"        nn.ReLU(inplace=True),\\n\",\"        nn.Conv2d(64, 64, 4, 2, 1),\\n\",\"    ]\\n\",\"elif args.downsampling_method == 'res':\\n\",\"    downsampling_layers = [\\n\",\"        nn.Conv2d(1, 64, 3, 1),\\n\",\"        ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\\n\",\"        ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\\n\",\"    ]\\n\",\"\\n\",\"feature_layers = [ODEBlock(ODEfunc(64))] if is_odenet else [ResBlock(64, 64) for _ in range(6)]\\n\",\"fc_layers = [norm(64), nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d((1, 1)), Flatten(), nn.Linear(64, 10)]\\n\",\"\\n\",\"model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)\\n\",\"\\n\",\"logger.info(model)\\n\",\"logger.info('Number of parameters: {}'.format(count_parameters(model)))\\n\",\"\\n\",\"criterion = nn.CrossEntropyLoss().to(device)\\n\",\"\\n\",\"train_loader, test_loader, train_eval_loader = get_mnist_loaders(\\n\",\"    args.data_aug, args.batch_size, args.test_batch_size\\n\",\")\\n\",\"\\n\",\"data_gen = inf_generator(train_loader)\\n\",\"batches_per_epoch = len(train_loader)\\n\",\"\\n\",\"lr_fn = learning_rate_with_decay(\\n\",\"    args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],\\n\",\"    decay_rates=[1, 0.1, 0.01, 0.001]\\n\",\")\\n\",\"\\n\",\"optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)\\n\",\"\\n\",\"best_acc = 0\\n\",\"batch_time_meter = RunningAverageMeter()\\n\",\"f_nfe_meter = RunningAverageMeter()\\n\",\"b_nfe_meter = RunningAverageMeter()\\n\",\"end = time.time()\\n\",\"\\n\",\"epoch_arr = []\\n\",\"time_val_arr = []\\n\",\"time_avg_arr = []\\n\",\"nfe_f_arr = []\\n\",\"nfe_b_arr = []\\n\",\"train_acc_arr = []\\n\",\"test_acc_arr = []\\n\",\"\\n\",\"for param_group in optimizer.param_groups:\\n\",\"    param_group['lr'] = lr_fn(itr)\\n\",\"\\n\",\"optimizer.zero_grad()\\n\",\"x, y = data_gen.__next__()\\n\",\"x = x.to(device)\\n\",\"y = y.to(device)\\n\",\"logits = model(x)\\n\",\"loss = criterion(logits, y)\\n\",\"\\n\",\"grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)\\n\",\"parameters = optimizer.param_groups[0]['params']\\n\",\"\\n\",\"print('Obtaining manual hessian...')\\n\",\"manual_hessian = get_manual_hessian(grads, parameters)           #get manual hessian.\"],\"execution_count\":1,\"outputs\":[{\"output_type\":\"error\",\"ename\":\"NameError\",\"evalue\":\"ignored\",\"traceback\":[\"\\u001b[0;31m---------------------------------------------------------------------------\\u001b[0m\",\"\\u001b[0;31mNameError\\u001b[0m                                 Traceback (most recent call last)\",\"\\u001b[0;32m/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\\u001b[0m in \\u001b[0;36m<module>\\u001b[0;34m()\\u001b[0m\\n\\u001b[1;32m      1\\u001b[0m \\u001b[0m__file__\\u001b[0m \\u001b[0;34m=\\u001b[0m \\u001b[0;34m'/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb'\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m----> 2\\u001b[0;31m \\u001b[0mmakedirs\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0margs\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0msave\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0m\\u001b[1;32m      3\\u001b[0m \\u001b[0mlogger\\u001b[0m \\u001b[0;34m=\\u001b[0m \\u001b[0mget_logger\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mlogpath\\u001b[0m\\u001b[0;34m=\\u001b[0m\\u001b[0mos\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mpath\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mjoin\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0margs\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0msave\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0;34m'logs'\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0mfilepath\\u001b[0m\\u001b[0;34m=\\u001b[0m\\u001b[0mos\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mpath\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mabspath\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0m__file__\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m      4\\u001b[0m \\u001b[0mlogger\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0minfo\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0margs\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m      5\\u001b[0m \\u001b[0;34m\\u001b[0m\\u001b[0m\\n\",\"\\u001b[0;31mNameError\\u001b[0m: name 'makedirs' is not defined\"]}]}]}\n","Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.1, nepochs=120, network='odenet', save='/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/experiment_node1', test_batch_size=1000, tol=0.001)\n","Sequential(\n","  (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1))\n","  (1): GroupNorm(32, 64, eps=1e-05, affine=True)\n","  (2): ReLU(inplace=True)\n","  (3): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","  (4): GroupNorm(32, 64, eps=1e-05, affine=True)\n","  (5): ReLU(inplace=True)\n","  (6): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","  (7): ODEBlock(\n","    (odefunc): ODEfunc(\n","      (norm1): GroupNorm(32, 64, eps=1e-05, affine=True)\n","      (relu): ReLU(inplace=True)\n","      (conv1): ConcatConv2d(\n","        (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","      (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)\n","      (conv2): ConcatConv2d(\n","        (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","      (norm3): GroupNorm(32, 64, eps=1e-05, affine=True)\n","    )\n","  )\n","  (8): GroupNorm(32, 64, eps=1e-05, affine=True)\n","  (9): ReLU(inplace=True)\n","  (10): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (11): Flatten()\n","  (12): Linear(in_features=64, out_features=10, bias=True)\n",")\n","Number of parameters: 208266\n"],"name":"stderr"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mitr\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatches_per_epoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_eval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m                 \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_acc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\u001b[0m in \u001b[0;36maccuracy\u001b[0;34m(model, dataset_loader)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mtarget_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mpredicted_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0mtotal_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtarget_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_correct\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegration_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegration_time\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0modeint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0modefunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegration_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchdiffeq/_impl/odeint.py\u001b[0m in \u001b[0;36modeint\u001b[0;34m(func, y0, t, rtol, atol, method, options, event_fn)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mevent_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0msolution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegrate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mevent_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegrate_until_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchdiffeq/_impl/solvers.py\u001b[0m in \u001b[0;36mintegrate\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_integrate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0msolution\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_advance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msolution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchdiffeq/_impl/rk_common.py\u001b[0m in \u001b[0;36m_advance\u001b[0;34m(self, next_t)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mnext_t\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrk_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mn_steps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_num_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'max_num_steps exceeded ({}>={})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_num_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrk_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adaptive_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrk_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m             \u001b[0mn_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_interp_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrk_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterp_coeff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrk_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrk_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchdiffeq/_impl/rk_common.py\u001b[0m in \u001b[0;36m_adaptive_step\u001b[0;34m(self, rk_state)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;31m# trigger both. (i.e. interleaving them would be wrong.)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_runge_kutta_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtableau\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtableau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0;31m# dtypes:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;31m# y1.dtype == self.y0.dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchdiffeq/_impl/rk_common.py\u001b[0m in \u001b[0;36m_runge_kutta_step\u001b[0;34m(func, y0, f0, t0, dt, t1, tableau)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mperturb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerturb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNONE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0myi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta_i\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mti\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperturb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mperturb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_UncheckedAssign\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchdiffeq/_impl/misc.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, t, y, perturb)\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0;31m# Do nothing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, t, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, t, x)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mtt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mttx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mttx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    394\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    395\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 396\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"Ner61Yv1L9ZC","executionInfo":{"status":"ok","timestamp":1617964389098,"user_tz":-60,"elapsed":1452,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["%%bash\n","rm -r /content/experiment_node1"],"execution_count":58,"outputs":[]},{"cell_type":"code","metadata":{"id":"gl0nftCJOgzw"},"source":["\"\"\"\n","--------------------------------------------------------------------------------------------------------------------------------------------\n","The code for Hessian analysis.\n","--------------------------------------------------------------------------------------------------------------------------------------------\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m75L8B4eOjXq","executionInfo":{"status":"ok","timestamp":1617965653332,"user_tz":-60,"elapsed":763,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["def get_manual_hessian(grads, parameters, show_iters=True):\n","  \"\"\"\n","  Calculation of the Hessian using nested for loops.\n","  Inputs:   - grads:        tuple of gradient tensors. Created using something \n","                            like grads = torch.autograd.grad(loss, parameters, create_graph=True).\n","            - parameters:   List of parameter objects. Created using something \n","                            like parameters = optimizer.param_groups[0]['params'].\n","            - show_iters:   True or False, depending on if the iteration number is to be shown during training. \n","                            Note that the iteration updates are not provided every row, but instead periodically \n","                            (roughly according to the number of parameters in the system).\n","  \"\"\"\n","  start = time.time()        \n","\n","  n_params = 0\n","  for param in parameters:\n","    n_params += torch.numel(param)\n","  grads2 = torch.zeros(n_params,n_params)            #Create an matrix of zeros thas has the same shape as the Hessian.\n","\n","  y_counter = 0                             #y_direction refers to row number in the Hessian.\n","\n","  for grad in grads:\n","      grad = torch.reshape(grad, [-1])                                  #Rearrange the gradient information into a vector.        \n","\n","      for j, g in enumerate(grad):\n","        x_counter = 0                                                   #x_direction refers to column number in the Hessian.\n","\n","        for l, param in enumerate(parameters):\n","          g2 = torch.autograd.grad(g, param, retain_graph=True)[0]      #Calculate the gradient of an element of the gradient wrt one layer's parameters.\n","          g2 = torch.reshape(g2, [-1])                                  #Reshape this into a vector.\n","          len = g2.shape[0]                       \n","          grads2[j+y_counter, x_counter:x_counter+len] = g2             #Indexing ensures that the second order derivatives are placed in the correct positions.\n","          x_counter += len\n","\n","      grads2 = grads2.to(device)\n","      y_counter += grad.shape[0]\n","\n","      if show_iters:\n","        print(\"Gradients calculated for row number \" + str(y_counter) + \".\")\n","  \n","  print('Time used was ', time.time() - start)\n","\n","  return grads2"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":232},"id":"-QPba5QNQx6V","executionInfo":{"status":"error","timestamp":1617965601039,"user_tz":-60,"elapsed":1139,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}},"outputId":"008f10c7-272a-431f-a1bd-c9dfbfc275ef"},"source":["__file__ = '/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb'\n","makedirs(args.save)\n","logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))\n","logger.info(args)\n","\n","device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')\n","\n","is_odenet = args.network == 'odenet'\n","\n","if args.downsampling_method == 'conv':\n","    downsampling_layers = [\n","        nn.Conv2d(1, 64, 3, 1),\n","        norm(64),\n","        nn.ReLU(inplace=True),\n","        nn.Conv2d(64, 64, 4, 2, 1),\n","        norm(64),\n","        nn.ReLU(inplace=True),\n","        nn.Conv2d(64, 64, 4, 2, 1),\n","    ]\n","elif args.downsampling_method == 'res':\n","    downsampling_layers = [\n","        nn.Conv2d(1, 64, 3, 1),\n","        ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\n","        ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\n","    ]\n","\n","feature_layers = [ODEBlock(ODEfunc(64))] if is_odenet else [ResBlock(64, 64) for _ in range(6)]\n","fc_layers = [norm(64), nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d((1, 1)), Flatten(), nn.Linear(64, 10)]\n","\n","model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)\n","\n","logger.info(model)\n","logger.info('Number of parameters: {}'.format(count_parameters(model)))\n","\n","criterion = nn.CrossEntropyLoss().to(device)\n","\n","train_loader, test_loader, train_eval_loader = get_mnist_loaders(\n","    args.data_aug, args.batch_size, args.test_batch_size\n",")\n","\n","data_gen = inf_generator(train_loader)\n","batches_per_epoch = len(train_loader)\n","\n","lr_fn = learning_rate_with_decay(\n","    args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],\n","    decay_rates=[1, 0.1, 0.01, 0.001]\n",")\n","\n","optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)\n","\n","best_acc = 0\n","batch_time_meter = RunningAverageMeter()\n","f_nfe_meter = RunningAverageMeter()\n","b_nfe_meter = RunningAverageMeter()\n","end = time.time()\n","\n","epoch_arr = []\n","time_val_arr = []\n","time_avg_arr = []\n","nfe_f_arr = []\n","nfe_b_arr = []\n","train_acc_arr = []\n","test_acc_arr = []\n","\n","for param_group in optimizer.param_groups:\n","    param_group['lr'] = lr_fn(itr)\n","\n","optimizer.zero_grad()\n","x, y = data_gen.__next__()\n","x = x.to(device)\n","y = y.to(device)\n","logits = model(x)\n","loss = criterion(logits, y)\n","\n","grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n","parameters = optimizer.param_groups[0]['params']\n","\n","print('Obtaining manual hessian...')\n","manual_hessian = get_manual_hessian(grads, parameters)           #get manual hessian."],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0m__file__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'logs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'makedirs' is not defined"]}]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mnist_node.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1sLUaZ_DEMKH30zwclvQKQ2FNZUI_mF3K","authorship_tag":"ABX9TyPcyCLDteDUeOs03JfQdmb8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"sWmsXi5BIAtb","executionInfo":{"status":"ok","timestamp":1617965636705,"user_tz":-60,"elapsed":3803,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["%%capture\n","%%bash\n","pip install torchdiffeq"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"H353RG8fIDd3","executionInfo":{"status":"ok","timestamp":1617965639560,"user_tz":-60,"elapsed":6489,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["import os\n","import argparse\n","import logging\n","import time\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","\n","\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')\n","parser.add_argument('--tol', type=float, default=1e-3)\n","parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])\n","parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])\n","parser.add_argument('--nepochs', type=int, default=120)\n","parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])\n","parser.add_argument('--lr', type=float, default=0.1)\n","parser.add_argument('--batch_size', type=int, default=128)\n","parser.add_argument('--test_batch_size', type=int, default=1000)\n","\n","parser.add_argument('--save', type=str, default='./experiment_node1')\n","parser.add_argument('--gpu', type=int, default=0)\n","parser.add_argument('--debug', action='store_true')\n","args = parser.parse_args(args=[])\n","\n","args.save = '/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/experiment_node1'\n","\n","if args.adjoint:\n","    from torchdiffeq import odeint_adjoint as odeint\n","else:\n","    from torchdiffeq import odeint"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"agpZtaKwIKnA","executionInfo":{"status":"ok","timestamp":1617965639567,"user_tz":-60,"elapsed":6348,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["def conv3x3(in_planes, out_planes, stride=1):\n","    \"\"\"3x3 convolution with padding\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","\n","def conv1x1(in_planes, out_planes, stride=1):\n","    \"\"\"1x1 convolution\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n","\n","\n","def norm(dim):\n","    return nn.GroupNorm(min(32, dim), dim)\n","\n","\n","class ResBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None):\n","        super(ResBlock, self).__init__()\n","        self.norm1 = norm(inplanes)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.downsample = downsample\n","        self.conv1 = conv3x3(inplanes, planes, stride)\n","        self.norm2 = norm(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","\n","    def forward(self, x):\n","        shortcut = x\n","\n","        out = self.relu(self.norm1(x))\n","\n","        if self.downsample is not None:\n","            shortcut = self.downsample(out)\n","\n","        out = self.conv1(out)\n","        out = self.norm2(out)\n","        out = self.relu(out)\n","        out = self.conv2(out)\n","\n","        return out + shortcut\n","\n","\n","class ConcatConv2d(nn.Module):\n","\n","    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):\n","        super(ConcatConv2d, self).__init__()\n","        module = nn.ConvTranspose2d if transpose else nn.Conv2d\n","        self._layer = module(\n","            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,\n","            bias=bias\n","        )\n","\n","    def forward(self, t, x):\n","        tt = torch.ones_like(x[:, :1, :, :]) * t\n","        ttx = torch.cat([tt, x], 1)\n","        return self._layer(ttx)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"1-rlM_3sI-1k","executionInfo":{"status":"ok","timestamp":1617965639568,"user_tz":-60,"elapsed":6172,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["class ODEfunc(nn.Module):\n","\n","    def __init__(self, dim):\n","        super(ODEfunc, self).__init__()\n","        self.norm1 = norm(dim)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv1 = ConcatConv2d(dim, dim, 3, 1, 1)\n","        self.norm2 = norm(dim)\n","        self.conv2 = ConcatConv2d(dim, dim, 3, 1, 1)\n","        self.norm3 = norm(dim)\n","        self.nfe = 0\n","\n","    def forward(self, t, x):\n","        self.nfe += 1\n","        out = self.norm1(x)\n","        out = self.relu(out)\n","        out = self.conv1(t, out)\n","        out = self.norm2(out)\n","        out = self.relu(out)\n","        out = self.conv2(t, out)\n","        out = self.norm3(out)\n","        return out\n","\n","\n","class ODEBlock(nn.Module):\n","\n","    def __init__(self, odefunc):\n","        super(ODEBlock, self).__init__()\n","        self.odefunc = odefunc\n","        self.integration_time = torch.tensor([0, 1]).float()\n","\n","    def forward(self, x):\n","        self.integration_time = self.integration_time.type_as(x)\n","        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)\n","        return out[1]\n","\n","    @property\n","    def nfe(self):\n","        return self.odefunc.nfe\n","\n","    @nfe.setter\n","    def nfe(self, value):\n","        self.odefunc.nfe = value"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"xhFiCzNaJC0x","executionInfo":{"status":"ok","timestamp":1617965639570,"user_tz":-60,"elapsed":6060,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["class Flatten(nn.Module):\n","\n","    def __init__(self):\n","        super(Flatten, self).__init__()\n","\n","    def forward(self, x):\n","        shape = torch.prod(torch.tensor(x.shape[1:])).item()\n","        return x.view(-1, shape)\n","\n","\n","class RunningAverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","\n","    def __init__(self, momentum=0.99):\n","        self.momentum = momentum\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = None\n","        self.avg = 0\n","\n","    def update(self, val):\n","        if self.val is None:\n","            self.avg = val\n","        else:\n","            self.avg = self.avg * self.momentum + val * (1 - self.momentum)\n","        self.val = val"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"ltcgl1RIJG7U","executionInfo":{"status":"ok","timestamp":1617965640246,"user_tz":-60,"elapsed":6577,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["def get_mnist_loaders(data_aug=False, batch_size=128, test_batch_size=1000, perc=1.0):\n","    if data_aug:\n","        transform_train = transforms.Compose([\n","            transforms.RandomCrop(28, padding=4),\n","            transforms.ToTensor(),\n","        ])\n","    else:\n","        transform_train = transforms.Compose([\n","            transforms.ToTensor(),\n","        ])\n","\n","    transform_test = transforms.Compose([\n","        transforms.ToTensor(),\n","    ])\n","\n","    train_loader = DataLoader(\n","        datasets.MNIST(root='.data/mnist', train=True, download=True, transform=transform_train), batch_size=batch_size,\n","        shuffle=True, num_workers=2, drop_last=True\n","    )\n","\n","    train_eval_loader = DataLoader(\n","        datasets.MNIST(root='.data/mnist', train=True, download=True, transform=transform_test),\n","        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True\n","    )\n","\n","    test_loader = DataLoader(\n","        datasets.MNIST(root='.data/mnist', train=False, download=True, transform=transform_test),\n","        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True\n","    )\n","\n","    return train_loader, test_loader, train_eval_loader\n","\n","\n","def inf_generator(iterable):\n","    \"\"\"Allows training with DataLoaders in a single infinite loop:\n","        for i, (x, y) in enumerate(inf_generator(train_loader)):\n","    \"\"\"\n","    iterator = iterable.__iter__()\n","    while True:\n","        try:\n","            yield iterator.__next__()\n","        except StopIteration:\n","            iterator = iterable.__iter__()\n","\n","\n","def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):\n","    initial_learning_rate = args.lr * batch_size / batch_denom\n","\n","    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]\n","    vals = [initial_learning_rate * decay for decay in decay_rates]\n","\n","    def learning_rate_fn(itr):\n","        lt = [itr < b for b in boundaries] + [True]\n","        i = np.argmax(lt)\n","        return vals[i]\n","\n","    return learning_rate_fn\n","\n","\n","def one_hot(x, K):\n","    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)\n","\n","\n","def accuracy(model, dataset_loader):\n","    total_correct = 0\n","    for x, y in dataset_loader:\n","        x = x.to(device)\n","        y = one_hot(np.array(y.numpy()), 10)\n","\n","        target_class = np.argmax(y, axis=1)\n","        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)\n","        total_correct += np.sum(predicted_class == target_class)\n","    return total_correct / len(dataset_loader.dataset)\n","\n","\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","\n","def makedirs(dirname):\n","    if not os.path.exists(dirname):\n","        os.makedirs(dirname)\n","\n","\n","def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):\n","    logger = logging.getLogger()\n","    if debug:\n","        level = logging.DEBUG\n","    else:\n","        level = logging.INFO\n","    logger.setLevel(level)\n","    if saving:\n","        info_file_handler = logging.FileHandler(logpath, mode=\"a\")\n","        info_file_handler.setLevel(level)\n","        logger.addHandler(info_file_handler)\n","    if displaying:\n","        console_handler = logging.StreamHandler()\n","        console_handler.setLevel(level)\n","        logger.addHandler(console_handler)\n","    logger.info(filepath)\n","    with open(filepath, \"r\") as f:\n","        logger.info(f.read())\n","\n","    for f in package_files:\n","        logger.info(f)\n","        with open(f, \"r\") as package_f:\n","            logger.info(package_f.read())\n","\n","    return logger\n"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":939},"id":"42RIwJ37JKQr","executionInfo":{"status":"error","timestamp":1617965648740,"user_tz":-60,"elapsed":14934,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}},"outputId":"1aaf2bb1-ee38-4c9c-bf3f-c2cd78a30ce2"},"source":["if __name__ == '__main__':\n","    __file__ = '/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb'\n","    makedirs(args.save)\n","    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))\n","    logger.info(args)\n","\n","    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')\n","\n","    is_odenet = args.network == 'odenet'\n","\n","    if args.downsampling_method == 'conv':\n","        downsampling_layers = [\n","            nn.Conv2d(1, 64, 3, 1),\n","            norm(64),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(64, 64, 4, 2, 1),\n","            norm(64),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(64, 64, 4, 2, 1),\n","        ]\n","    elif args.downsampling_method == 'res':\n","        downsampling_layers = [\n","            nn.Conv2d(1, 64, 3, 1),\n","            ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\n","            ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\n","        ]\n","\n","    feature_layers = [ODEBlock(ODEfunc(64))] if is_odenet else [ResBlock(64, 64) for _ in range(6)]\n","    fc_layers = [norm(64), nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d((1, 1)), Flatten(), nn.Linear(64, 10)]\n","\n","    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)\n","\n","    logger.info(model)\n","    logger.info('Number of parameters: {}'.format(count_parameters(model)))\n","\n","    criterion = nn.CrossEntropyLoss().to(device)\n","\n","    train_loader, test_loader, train_eval_loader = get_mnist_loaders(\n","        args.data_aug, args.batch_size, args.test_batch_size\n","    )\n","\n","    data_gen = inf_generator(train_loader)\n","    batches_per_epoch = len(train_loader)\n","\n","    lr_fn = learning_rate_with_decay(\n","        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],\n","        decay_rates=[1, 0.1, 0.01, 0.001]\n","    )\n","\n","    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)\n","\n","    best_acc = 0\n","    batch_time_meter = RunningAverageMeter()\n","    f_nfe_meter = RunningAverageMeter()\n","    b_nfe_meter = RunningAverageMeter()\n","    end = time.time()\n","    \n","    epoch_arr = []\n","    time_val_arr = []\n","    time_avg_arr = []\n","    nfe_f_arr = []\n","    nfe_b_arr = []\n","    train_acc_arr = []\n","    test_acc_arr = []\n","\n","    for itr in range(args.nepochs * batches_per_epoch):\n","\n","        for param_group in optimizer.param_groups:\n","            param_group['lr'] = lr_fn(itr)\n","\n","        optimizer.zero_grad()\n","        x, y = data_gen.__next__()\n","        x = x.to(device)\n","        y = y.to(device)\n","        logits = model(x)\n","        loss = criterion(logits, y)\n","\n","        grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n","        parameters = optimizer.param_groups[0]['params']\n","\n","        print('Obtaining manual hessian...')\n","        manual_hessian = get_manual_hessian(grads, parameters)           #get manual hessian.\n","\n","        if is_odenet:\n","            nfe_forward = feature_layers[0].nfe\n","            feature_layers[0].nfe = 0\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        if is_odenet:\n","            nfe_backward = feature_layers[0].nfe\n","            feature_layers[0].nfe = 0\n","\n","        batch_time_meter.update(time.time() - end)\n","        if is_odenet:\n","            f_nfe_meter.update(nfe_forward)\n","            b_nfe_meter.update(nfe_backward)\n","        end = time.time()\n","\n","        if itr % batches_per_epoch == 0:\n","            with torch.no_grad():\n","                train_acc = accuracy(model, train_eval_loader)\n","                val_acc = accuracy(model, test_loader)\n","                if val_acc > best_acc:\n","                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))\n","                    best_acc = val_acc\n","                logger.info(\n","                    \"Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | \"\n","                    \"Train Acc {:.4f} | Test Acc {:.4f}\".format(\n","                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,\n","                        b_nfe_meter.avg, train_acc, val_acc\n","                    )\n","                )\n","                epoch_arr += [itr // batches_per_epoch]\n","                time_val_arr += [batch_time_meter.val]\n","                time_avg_arr += [batch_time_meter.avg]\n","                nfe_f_arr += [f_nfe_meter.avg]\n","                nfe_b_arr += [b_nfe_meter.avg]\n","                train_acc_arr += [train_acc]\n","                test_acc_arr += [val_acc]\n","                    \n","    epoch_arr = np.asarray(epoch_arr)\n","    time_val_arr = np.asarray(time_val_arr)\n","    time_avg_arr = np.asarray(time_avg_arr)\n","    nfe_f_arr = np.asarray(nfe_f_arr)\n","    nfe_b_arr = np.asarray(nfe_b_arr)\n","    train_acc_arr = np.asarray(train_acc_arr)\n","    test_acc_arr = np.asarray(test_acc_arr)\n","    \n","    np.save(os.path.join(args.save, 'epoch_arr.npy'), epoch_arr)\n","    np.save(os.path.join(args.save, 'time_val_arr.npy'), time_val_arr)\n","    np.save(os.path.join(args.save, 'time_avg_arr.npy'), time_avg_arr)\n","    np.save(os.path.join(args.save, 'nfe_f_arr.npy'), nfe_f_arr)\n","    np.save(os.path.join(args.save, 'nfe_b_arr.npy'), nfe_b_arr)\n","    np.save(os.path.join(args.save, 'train_acc_arr.npy'), train_acc_arr)\n","    np.save(os.path.join(args.save, 'test_acc_arr.npy'), test_acc_arr)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\n","{\"nbformat\":4,\"nbformat_minor\":0,\"metadata\":{\"colab\":{\"name\":\"mnist_node.ipynb\",\"provenance\":[],\"collapsed_sections\":[],\"mount_file_id\":\"1sLUaZ_DEMKH30zwclvQKQ2FNZUI_mF3K\",\"authorship_tag\":\"ABX9TyPcyCLDteDUeOs03JfQdmb8\"},\"kernelspec\":{\"name\":\"python3\",\"display_name\":\"Python 3\"},\"language_info\":{\"name\":\"python\"}},\"cells\":[{\"cell_type\":\"code\",\"metadata\":{\"id\":\"sWmsXi5BIAtb\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617965469256,\"user_tz\":-60,\"elapsed\":3337,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"%%capture\\n\",\"%%bash\\n\",\"pip install torchdiffeq\"],\"execution_count\":2,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"H353RG8fIDd3\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617965472807,\"user_tz\":-60,\"elapsed\":6734,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"import os\\n\",\"import argparse\\n\",\"import logging\\n\",\"import time\\n\",\"import numpy as np\\n\",\"import torch\\n\",\"import torch.nn as nn\\n\",\"from torch.utils.data import DataLoader\\n\",\"import torchvision.datasets as datasets\\n\",\"import torchvision.transforms as transforms\\n\",\"\\n\",\"\\n\",\"parser = argparse.ArgumentParser()\\n\",\"parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')\\n\",\"parser.add_argument('--tol', type=float, default=1e-3)\\n\",\"parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])\\n\",\"parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])\\n\",\"parser.add_argument('--nepochs', type=int, default=120)\\n\",\"parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])\\n\",\"parser.add_argument('--lr', type=float, default=0.1)\\n\",\"parser.add_argument('--batch_size', type=int, default=128)\\n\",\"parser.add_argument('--test_batch_size', type=int, default=1000)\\n\",\"\\n\",\"parser.add_argument('--save', type=str, default='./experiment_node1')\\n\",\"parser.add_argument('--gpu', type=int, default=0)\\n\",\"parser.add_argument('--debug', action='store_true')\\n\",\"args = parser.parse_args(args=[])\\n\",\"\\n\",\"args.save = '/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/experiment_node1'\\n\",\"\\n\",\"if args.adjoint:\\n\",\"    from torchdiffeq import odeint_adjoint as odeint\\n\",\"else:\\n\",\"    from torchdiffeq import odeint\"],\"execution_count\":3,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"agpZtaKwIKnA\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617965472811,\"user_tz\":-60,\"elapsed\":6577,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"def conv3x3(in_planes, out_planes, stride=1):\\n\",\"    \\\"\\\"\\\"3x3 convolution with padding\\\"\\\"\\\"\\n\",\"    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\\n\",\"\\n\",\"\\n\",\"def conv1x1(in_planes, out_planes, stride=1):\\n\",\"    \\\"\\\"\\\"1x1 convolution\\\"\\\"\\\"\\n\",\"    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\\n\",\"\\n\",\"\\n\",\"def norm(dim):\\n\",\"    return nn.GroupNorm(min(32, dim), dim)\\n\",\"\\n\",\"\\n\",\"class ResBlock(nn.Module):\\n\",\"    expansion = 1\\n\",\"\\n\",\"    def __init__(self, inplanes, planes, stride=1, downsample=None):\\n\",\"        super(ResBlock, self).__init__()\\n\",\"        self.norm1 = norm(inplanes)\\n\",\"        self.relu = nn.ReLU(inplace=True)\\n\",\"        self.downsample = downsample\\n\",\"        self.conv1 = conv3x3(inplanes, planes, stride)\\n\",\"        self.norm2 = norm(planes)\\n\",\"        self.conv2 = conv3x3(planes, planes)\\n\",\"\\n\",\"    def forward(self, x):\\n\",\"        shortcut = x\\n\",\"\\n\",\"        out = self.relu(self.norm1(x))\\n\",\"\\n\",\"        if self.downsample is not None:\\n\",\"            shortcut = self.downsample(out)\\n\",\"\\n\",\"        out = self.conv1(out)\\n\",\"        out = self.norm2(out)\\n\",\"        out = self.relu(out)\\n\",\"        out = self.conv2(out)\\n\",\"\\n\",\"        return out + shortcut\\n\",\"\\n\",\"\\n\",\"class ConcatConv2d(nn.Module):\\n\",\"\\n\",\"    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):\\n\",\"        super(ConcatConv2d, self).__init__()\\n\",\"        module = nn.ConvTranspose2d if transpose else nn.Conv2d\\n\",\"        self._layer = module(\\n\",\"            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,\\n\",\"            bias=bias\\n\",\"        )\\n\",\"\\n\",\"    def forward(self, t, x):\\n\",\"        tt = torch.ones_like(x[:, :1, :, :]) * t\\n\",\"        ttx = torch.cat([tt, x], 1)\\n\",\"        return self._layer(ttx)\"],\"execution_count\":4,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"1-rlM_3sI-1k\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617965472812,\"user_tz\":-60,\"elapsed\":6439,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"class ODEfunc(nn.Module):\\n\",\"\\n\",\"    def __init__(self, dim):\\n\",\"        super(ODEfunc, self).__init__()\\n\",\"        self.norm1 = norm(dim)\\n\",\"        self.relu = nn.ReLU(inplace=True)\\n\",\"        self.conv1 = ConcatConv2d(dim, dim, 3, 1, 1)\\n\",\"        self.norm2 = norm(dim)\\n\",\"        self.conv2 = ConcatConv2d(dim, dim, 3, 1, 1)\\n\",\"        self.norm3 = norm(dim)\\n\",\"        self.nfe = 0\\n\",\"\\n\",\"    def forward(self, t, x):\\n\",\"        self.nfe += 1\\n\",\"        out = self.norm1(x)\\n\",\"        out = self.relu(out)\\n\",\"        out = self.conv1(t, out)\\n\",\"        out = self.norm2(out)\\n\",\"        out = self.relu(out)\\n\",\"        out = self.conv2(t, out)\\n\",\"        out = self.norm3(out)\\n\",\"        return out\\n\",\"\\n\",\"\\n\",\"class ODEBlock(nn.Module):\\n\",\"\\n\",\"    def __init__(self, odefunc):\\n\",\"        super(ODEBlock, self).__init__()\\n\",\"        self.odefunc = odefunc\\n\",\"        self.integration_time = torch.tensor([0, 1]).float()\\n\",\"\\n\",\"    def forward(self, x):\\n\",\"        self.integration_time = self.integration_time.type_as(x)\\n\",\"        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)\\n\",\"        return out[1]\\n\",\"\\n\",\"    @property\\n\",\"    def nfe(self):\\n\",\"        return self.odefunc.nfe\\n\",\"\\n\",\"    @nfe.setter\\n\",\"    def nfe(self, value):\\n\",\"        self.odefunc.nfe = value\"],\"execution_count\":5,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"xhFiCzNaJC0x\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617965472813,\"user_tz\":-60,\"elapsed\":6236,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"class Flatten(nn.Module):\\n\",\"\\n\",\"    def __init__(self):\\n\",\"        super(Flatten, self).__init__()\\n\",\"\\n\",\"    def forward(self, x):\\n\",\"        shape = torch.prod(torch.tensor(x.shape[1:])).item()\\n\",\"        return x.view(-1, shape)\\n\",\"\\n\",\"\\n\",\"class RunningAverageMeter(object):\\n\",\"    \\\"\\\"\\\"Computes and stores the average and current value\\\"\\\"\\\"\\n\",\"\\n\",\"    def __init__(self, momentum=0.99):\\n\",\"        self.momentum = momentum\\n\",\"        self.reset()\\n\",\"\\n\",\"    def reset(self):\\n\",\"        self.val = None\\n\",\"        self.avg = 0\\n\",\"\\n\",\"    def update(self, val):\\n\",\"        if self.val is None:\\n\",\"            self.avg = val\\n\",\"        else:\\n\",\"            self.avg = self.avg * self.momentum + val * (1 - self.momentum)\\n\",\"        self.val = val\"],\"execution_count\":6,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"ltcgl1RIJG7U\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617965473748,\"user_tz\":-60,\"elapsed\":922,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"def get_mnist_loaders(data_aug=False, batch_size=128, test_batch_size=1000, perc=1.0):\\n\",\"    if data_aug:\\n\",\"        transform_train = transforms.Compose([\\n\",\"            transforms.RandomCrop(28, padding=4),\\n\",\"            transforms.ToTensor(),\\n\",\"        ])\\n\",\"    else:\\n\",\"        transform_train = transforms.Compose([\\n\",\"            transforms.ToTensor(),\\n\",\"        ])\\n\",\"\\n\",\"    transform_test = transforms.Compose([\\n\",\"        transforms.ToTensor(),\\n\",\"    ])\\n\",\"\\n\",\"    train_loader = DataLoader(\\n\",\"        datasets.MNIST(root='.data/mnist', train=True, download=True, transform=transform_train), batch_size=batch_size,\\n\",\"        shuffle=True, num_workers=2, drop_last=True\\n\",\"    )\\n\",\"\\n\",\"    train_eval_loader = DataLoader(\\n\",\"        datasets.MNIST(root='.data/mnist', train=True, download=True, transform=transform_test),\\n\",\"        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True\\n\",\"    )\\n\",\"\\n\",\"    test_loader = DataLoader(\\n\",\"        datasets.MNIST(root='.data/mnist', train=False, download=True, transform=transform_test),\\n\",\"        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True\\n\",\"    )\\n\",\"\\n\",\"    return train_loader, test_loader, train_eval_loader\\n\",\"\\n\",\"\\n\",\"def inf_generator(iterable):\\n\",\"    \\\"\\\"\\\"Allows training with DataLoaders in a single infinite loop:\\n\",\"        for i, (x, y) in enumerate(inf_generator(train_loader)):\\n\",\"    \\\"\\\"\\\"\\n\",\"    iterator = iterable.__iter__()\\n\",\"    while True:\\n\",\"        try:\\n\",\"            yield iterator.__next__()\\n\",\"        except StopIteration:\\n\",\"            iterator = iterable.__iter__()\\n\",\"\\n\",\"\\n\",\"def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):\\n\",\"    initial_learning_rate = args.lr * batch_size / batch_denom\\n\",\"\\n\",\"    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]\\n\",\"    vals = [initial_learning_rate * decay for decay in decay_rates]\\n\",\"\\n\",\"    def learning_rate_fn(itr):\\n\",\"        lt = [itr < b for b in boundaries] + [True]\\n\",\"        i = np.argmax(lt)\\n\",\"        return vals[i]\\n\",\"\\n\",\"    return learning_rate_fn\\n\",\"\\n\",\"\\n\",\"def one_hot(x, K):\\n\",\"    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)\\n\",\"\\n\",\"\\n\",\"def accuracy(model, dataset_loader):\\n\",\"    total_correct = 0\\n\",\"    for x, y in dataset_loader:\\n\",\"        x = x.to(device)\\n\",\"        y = one_hot(np.array(y.numpy()), 10)\\n\",\"\\n\",\"        target_class = np.argmax(y, axis=1)\\n\",\"        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)\\n\",\"        total_correct += np.sum(predicted_class == target_class)\\n\",\"    return total_correct / len(dataset_loader.dataset)\\n\",\"\\n\",\"\\n\",\"def count_parameters(model):\\n\",\"    return sum(p.numel() for p in model.parameters() if p.requires_grad)\\n\",\"\\n\",\"\\n\",\"def makedirs(dirname):\\n\",\"    if not os.path.exists(dirname):\\n\",\"        os.makedirs(dirname)\\n\",\"\\n\",\"\\n\",\"def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):\\n\",\"    logger = logging.getLogger()\\n\",\"    if debug:\\n\",\"        level = logging.DEBUG\\n\",\"    else:\\n\",\"        level = logging.INFO\\n\",\"    logger.setLevel(level)\\n\",\"    if saving:\\n\",\"        info_file_handler = logging.FileHandler(logpath, mode=\\\"a\\\")\\n\",\"        info_file_handler.setLevel(level)\\n\",\"        logger.addHandler(info_file_handler)\\n\",\"    if displaying:\\n\",\"        console_handler = logging.StreamHandler()\\n\",\"        console_handler.setLevel(level)\\n\",\"        logger.addHandler(console_handler)\\n\",\"    logger.info(filepath)\\n\",\"    with open(filepath, \\\"r\\\") as f:\\n\",\"        logger.info(f.read())\\n\",\"\\n\",\"    for f in package_files:\\n\",\"        logger.info(f)\\n\",\"        with open(f, \\\"r\\\") as package_f:\\n\",\"            logger.info(package_f.read())\\n\",\"\\n\",\"    return logger\\n\"],\"execution_count\":7,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"colab\":{\"base_uri\":\"https://localhost:8080/\",\"height\":232},\"id\":\"42RIwJ37JKQr\",\"executionInfo\":{\"status\":\"error\",\"timestamp\":1617965618625,\"user_tz\":-60,\"elapsed\":746,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}},\"outputId\":\"b88040a4-5d98-4598-9959-01f561aa3c13\"},\"source\":[\"if __name__ == '__main__':\\n\",\"    __file__ = '/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb'\\n\",\"    makedirs(args.save)\\n\",\"    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))\\n\",\"    logger.info(args)\\n\",\"\\n\",\"    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')\\n\",\"\\n\",\"    is_odenet = args.network == 'odenet'\\n\",\"\\n\",\"    if args.downsampling_method == 'conv':\\n\",\"        downsampling_layers = [\\n\",\"            nn.Conv2d(1, 64, 3, 1),\\n\",\"            norm(64),\\n\",\"            nn.ReLU(inplace=True),\\n\",\"            nn.Conv2d(64, 64, 4, 2, 1),\\n\",\"            norm(64),\\n\",\"            nn.ReLU(inplace=True),\\n\",\"            nn.Conv2d(64, 64, 4, 2, 1),\\n\",\"        ]\\n\",\"    elif args.downsampling_method == 'res':\\n\",\"        downsampling_layers = [\\n\",\"            nn.Conv2d(1, 64, 3, 1),\\n\",\"            ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\\n\",\"            ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\\n\",\"        ]\\n\",\"\\n\",\"    feature_layers = [ODEBlock(ODEfunc(64))] if is_odenet else [ResBlock(64, 64) for _ in range(6)]\\n\",\"    fc_layers = [norm(64), nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d((1, 1)), Flatten(), nn.Linear(64, 10)]\\n\",\"\\n\",\"    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)\\n\",\"\\n\",\"    logger.info(model)\\n\",\"    logger.info('Number of parameters: {}'.format(count_parameters(model)))\\n\",\"\\n\",\"    criterion = nn.CrossEntropyLoss().to(device)\\n\",\"\\n\",\"    train_loader, test_loader, train_eval_loader = get_mnist_loaders(\\n\",\"        args.data_aug, args.batch_size, args.test_batch_size\\n\",\"    )\\n\",\"\\n\",\"    data_gen = inf_generator(train_loader)\\n\",\"    batches_per_epoch = len(train_loader)\\n\",\"\\n\",\"    lr_fn = learning_rate_with_decay(\\n\",\"        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],\\n\",\"        decay_rates=[1, 0.1, 0.01, 0.001]\\n\",\"    )\\n\",\"\\n\",\"    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)\\n\",\"\\n\",\"    best_acc = 0\\n\",\"    batch_time_meter = RunningAverageMeter()\\n\",\"    f_nfe_meter = RunningAverageMeter()\\n\",\"    b_nfe_meter = RunningAverageMeter()\\n\",\"    end = time.time()\\n\",\"    \\n\",\"    epoch_arr = []\\n\",\"    time_val_arr = []\\n\",\"    time_avg_arr = []\\n\",\"    nfe_f_arr = []\\n\",\"    nfe_b_arr = []\\n\",\"    train_acc_arr = []\\n\",\"    test_acc_arr = []\\n\",\"\\n\",\"    for itr in range(args.nepochs * batches_per_epoch):\\n\",\"\\n\",\"        for param_group in optimizer.param_groups:\\n\",\"            param_group['lr'] = lr_fn(itr)\\n\",\"\\n\",\"        optimizer.zero_grad()\\n\",\"        x, y = data_gen.__next__()\\n\",\"        x = x.to(device)\\n\",\"        y = y.to(device)\\n\",\"        logits = model(x)\\n\",\"        loss = criterion(logits, y)\\n\",\"\\n\",\"        grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)\\n\",\"        parameters = optimizer.param_groups[0]['params']\\n\",\"\\n\",\"        print('Obtaining manual hessian...')\\n\",\"        manual_hessian = get_manual_hessian(grads, parameters)           #get manual hessian.\\n\",\"\\n\",\"        if is_odenet:\\n\",\"            nfe_forward = feature_layers[0].nfe\\n\",\"            feature_layers[0].nfe = 0\\n\",\"\\n\",\"        loss.backward()\\n\",\"        optimizer.step()\\n\",\"\\n\",\"        if is_odenet:\\n\",\"            nfe_backward = feature_layers[0].nfe\\n\",\"            feature_layers[0].nfe = 0\\n\",\"\\n\",\"        batch_time_meter.update(time.time() - end)\\n\",\"        if is_odenet:\\n\",\"            f_nfe_meter.update(nfe_forward)\\n\",\"            b_nfe_meter.update(nfe_backward)\\n\",\"        end = time.time()\\n\",\"\\n\",\"        if itr % batches_per_epoch == 0:\\n\",\"            with torch.no_grad():\\n\",\"                train_acc = accuracy(model, train_eval_loader)\\n\",\"                val_acc = accuracy(model, test_loader)\\n\",\"                if val_acc > best_acc:\\n\",\"                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))\\n\",\"                    best_acc = val_acc\\n\",\"                logger.info(\\n\",\"                    \\\"Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | \\\"\\n\",\"                    \\\"Train Acc {:.4f} | Test Acc {:.4f}\\\".format(\\n\",\"                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,\\n\",\"                        b_nfe_meter.avg, train_acc, val_acc\\n\",\"                    )\\n\",\"                )\\n\",\"                epoch_arr += [itr // batches_per_epoch]\\n\",\"                time_val_arr += [batch_time_meter.val]\\n\",\"                time_avg_arr += [batch_time_meter.avg]\\n\",\"                nfe_f_arr += [f_nfe_meter.avg]\\n\",\"                nfe_b_arr += [b_nfe_meter.avg]\\n\",\"                train_acc_arr += [train_acc]\\n\",\"                test_acc_arr += [val_acc]\\n\",\"                    \\n\",\"    epoch_arr = np.asarray(epoch_arr)\\n\",\"    time_val_arr = np.asarray(time_val_arr)\\n\",\"    time_avg_arr = np.asarray(time_avg_arr)\\n\",\"    nfe_f_arr = np.asarray(nfe_f_arr)\\n\",\"    nfe_b_arr = np.asarray(nfe_b_arr)\\n\",\"    train_acc_arr = np.asarray(train_acc_arr)\\n\",\"    test_acc_arr = np.asarray(test_acc_arr)\\n\",\"    \\n\",\"    np.save(os.path.join(args.save, 'epoch_arr.npy'), epoch_arr)\\n\",\"    np.save(os.path.join(args.save, 'time_val_arr.npy'), time_val_arr)\\n\",\"    np.save(os.path.join(args.save, 'time_avg_arr.npy'), time_avg_arr)\\n\",\"    np.save(os.path.join(args.save, 'nfe_f_arr.npy'), nfe_f_arr)\\n\",\"    np.save(os.path.join(args.save, 'nfe_b_arr.npy'), nfe_b_arr)\\n\",\"    np.save(os.path.join(args.save, 'train_acc_arr.npy'), train_acc_arr)\\n\",\"    np.save(os.path.join(args.save, 'test_acc_arr.npy'), test_acc_arr)\"],\"execution_count\":2,\"outputs\":[{\"output_type\":\"error\",\"ename\":\"NameError\",\"evalue\":\"ignored\",\"traceback\":[\"\\u001b[0;31m---------------------------------------------------------------------------\\u001b[0m\",\"\\u001b[0;31mNameError\\u001b[0m                                 Traceback (most recent call last)\",\"\\u001b[0;32m/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\\u001b[0m in \\u001b[0;36m<module>\\u001b[0;34m()\\u001b[0m\\n\\u001b[1;32m      1\\u001b[0m \\u001b[0;32mif\\u001b[0m \\u001b[0m__name__\\u001b[0m \\u001b[0;34m==\\u001b[0m \\u001b[0;34m'__main__'\\u001b[0m\\u001b[0;34m:\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m      2\\u001b[0m     \\u001b[0m__file__\\u001b[0m \\u001b[0;34m=\\u001b[0m \\u001b[0;34m'/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb'\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m----> 3\\u001b[0;31m     \\u001b[0mmakedirs\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0margs\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0msave\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0m\\u001b[1;32m      4\\u001b[0m     \\u001b[0mlogger\\u001b[0m \\u001b[0;34m=\\u001b[0m \\u001b[0mget_logger\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mlogpath\\u001b[0m\\u001b[0;34m=\\u001b[0m\\u001b[0mos\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mpath\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mjoin\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0margs\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0msave\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0;34m'logs'\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0mfilepath\\u001b[0m\\u001b[0;34m=\\u001b[0m\\u001b[0mos\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mpath\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mabspath\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0m__file__\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m      5\\u001b[0m     \\u001b[0mlogger\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0minfo\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0margs\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\",\"\\u001b[0;31mNameError\\u001b[0m: name 'makedirs' is not defined\"]}]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"Ner61Yv1L9ZC\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617964389098,\"user_tz\":-60,\"elapsed\":1452,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"%%bash\\n\",\"rm -r /content/experiment_node1\"],\"execution_count\":58,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"gl0nftCJOgzw\"},\"source\":[\"\\\"\\\"\\\"\\n\",\"--------------------------------------------------------------------------------------------------------------------------------------------\\n\",\"The code for Hessian analysis.\\n\",\"--------------------------------------------------------------------------------------------------------------------------------------------\\n\",\"\\\"\\\"\\\"\"],\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"m75L8B4eOjXq\",\"executionInfo\":{\"status\":\"ok\",\"timestamp\":1617965493846,\"user_tz\":-60,\"elapsed\":445,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}}},\"source\":[\"def get_manual_hessian(grads, parameters, show_iters=True):\\n\",\"  \\\"\\\"\\\"\\n\",\"  Calculation of the Hessian using nested for loops.\\n\",\"  Inputs:   - grads:        tuple of gradient tensors. Created using something \\n\",\"                            like grads = torch.autograd.grad(loss, parameters, create_graph=True).\\n\",\"            - parameters:   List of parameter objects. Created using something \\n\",\"                            like parameters = optimizer.param_groups[0]['params'].\\n\",\"            - show_iters:   True or False, depending on if the iteration number is to be shown during training. \\n\",\"                            Note that the iteration updates are not provided every row, but instead periodically \\n\",\"                            (roughly according to the number of parameters in the system).\\n\",\"  \\\"\\\"\\\"\\n\",\"  start = time.time()        \\n\",\"\\n\",\"  n_params = 0\\n\",\"  for param in parameters:\\n\",\"    n_params += torch.numel(param)\\n\",\"  grads2 = torch.zeros(n_params,n_params)            #Create an matrix of zeros thas has the same shape as the Hessian.\\n\",\"\\n\",\"  y_counter = 0                             #y_direction refers to row number in the Hessian.\\n\",\"\\n\",\"  for grad in grads:\\n\",\"      grad = torch.reshape(grad, [-1])                                  #Rearrange the gradient information into a vector.        \\n\",\"\\n\",\"      for j, g in enumerate(grad):\\n\",\"        x_counter = 0                                                   #x_direction refers to column number in the Hessian.\\n\",\"\\n\",\"        for l, param in enumerate(parameters):\\n\",\"          g2 = torch.autograd.grad(g, param, retain_graph=True)[0]      #Calculate the gradient of an element of the gradient wrt one layer's parameters.\\n\",\"          g2 = torch.reshape(g2, [-1])                                  #Reshape this into a vector.\\n\",\"          len = g2.shape[0]                       \\n\",\"          grads2[j+y_counter, x_counter:x_counter+len] = g2             #Indexing ensures that the second order derivatives are placed in the correct positions.\\n\",\"          x_counter += len\\n\",\"\\n\",\"      grads2 = grads2.to(device)\\n\",\"      y_counter += grad.shape[0]\\n\",\"\\n\",\"      if show_iters:\\n\",\"        print(\\\"Gradients calculated for row number \\\" + str(y_counter) + \\\".\\\")\\n\",\"  \\n\",\"  print('Time used was ', time.time() - start)\\n\",\"\\n\",\"  return grads2\"],\"execution_count\":9,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"colab\":{\"base_uri\":\"https://localhost:8080/\",\"height\":232},\"id\":\"-QPba5QNQx6V\",\"executionInfo\":{\"status\":\"error\",\"timestamp\":1617965601039,\"user_tz\":-60,\"elapsed\":1139,\"user\":{\"displayName\":\"L. Atkins\",\"photoUrl\":\"\",\"userId\":\"13582269488947613307\"}},\"outputId\":\"008f10c7-272a-431f-a1bd-c9dfbfc275ef\"},\"source\":[\"__file__ = '/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb'\\n\",\"makedirs(args.save)\\n\",\"logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))\\n\",\"logger.info(args)\\n\",\"\\n\",\"device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')\\n\",\"\\n\",\"is_odenet = args.network == 'odenet'\\n\",\"\\n\",\"if args.downsampling_method == 'conv':\\n\",\"    downsampling_layers = [\\n\",\"        nn.Conv2d(1, 64, 3, 1),\\n\",\"        norm(64),\\n\",\"        nn.ReLU(inplace=True),\\n\",\"        nn.Conv2d(64, 64, 4, 2, 1),\\n\",\"        norm(64),\\n\",\"        nn.ReLU(inplace=True),\\n\",\"        nn.Conv2d(64, 64, 4, 2, 1),\\n\",\"    ]\\n\",\"elif args.downsampling_method == 'res':\\n\",\"    downsampling_layers = [\\n\",\"        nn.Conv2d(1, 64, 3, 1),\\n\",\"        ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\\n\",\"        ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\\n\",\"    ]\\n\",\"\\n\",\"feature_layers = [ODEBlock(ODEfunc(64))] if is_odenet else [ResBlock(64, 64) for _ in range(6)]\\n\",\"fc_layers = [norm(64), nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d((1, 1)), Flatten(), nn.Linear(64, 10)]\\n\",\"\\n\",\"model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)\\n\",\"\\n\",\"logger.info(model)\\n\",\"logger.info('Number of parameters: {}'.format(count_parameters(model)))\\n\",\"\\n\",\"criterion = nn.CrossEntropyLoss().to(device)\\n\",\"\\n\",\"train_loader, test_loader, train_eval_loader = get_mnist_loaders(\\n\",\"    args.data_aug, args.batch_size, args.test_batch_size\\n\",\")\\n\",\"\\n\",\"data_gen = inf_generator(train_loader)\\n\",\"batches_per_epoch = len(train_loader)\\n\",\"\\n\",\"lr_fn = learning_rate_with_decay(\\n\",\"    args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],\\n\",\"    decay_rates=[1, 0.1, 0.01, 0.001]\\n\",\")\\n\",\"\\n\",\"optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)\\n\",\"\\n\",\"best_acc = 0\\n\",\"batch_time_meter = RunningAverageMeter()\\n\",\"f_nfe_meter = RunningAverageMeter()\\n\",\"b_nfe_meter = RunningAverageMeter()\\n\",\"end = time.time()\\n\",\"\\n\",\"epoch_arr = []\\n\",\"time_val_arr = []\\n\",\"time_avg_arr = []\\n\",\"nfe_f_arr = []\\n\",\"nfe_b_arr = []\\n\",\"train_acc_arr = []\\n\",\"test_acc_arr = []\\n\",\"\\n\",\"for param_group in optimizer.param_groups:\\n\",\"    param_group['lr'] = lr_fn(itr)\\n\",\"\\n\",\"optimizer.zero_grad()\\n\",\"x, y = data_gen.__next__()\\n\",\"x = x.to(device)\\n\",\"y = y.to(device)\\n\",\"logits = model(x)\\n\",\"loss = criterion(logits, y)\\n\",\"\\n\",\"grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)\\n\",\"parameters = optimizer.param_groups[0]['params']\\n\",\"\\n\",\"print('Obtaining manual hessian...')\\n\",\"manual_hessian = get_manual_hessian(grads, parameters)           #get manual hessian.\"],\"execution_count\":1,\"outputs\":[{\"output_type\":\"error\",\"ename\":\"NameError\",\"evalue\":\"ignored\",\"traceback\":[\"\\u001b[0;31m---------------------------------------------------------------------------\\u001b[0m\",\"\\u001b[0;31mNameError\\u001b[0m                                 Traceback (most recent call last)\",\"\\u001b[0;32m/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\\u001b[0m in \\u001b[0;36m<module>\\u001b[0;34m()\\u001b[0m\\n\\u001b[1;32m      1\\u001b[0m \\u001b[0m__file__\\u001b[0m \\u001b[0;34m=\\u001b[0m \\u001b[0;34m'/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb'\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m----> 2\\u001b[0;31m \\u001b[0mmakedirs\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0margs\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0msave\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0m\\u001b[1;32m      3\\u001b[0m \\u001b[0mlogger\\u001b[0m \\u001b[0;34m=\\u001b[0m \\u001b[0mget_logger\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mlogpath\\u001b[0m\\u001b[0;34m=\\u001b[0m\\u001b[0mos\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mpath\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mjoin\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0margs\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0msave\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0;34m'logs'\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m,\\u001b[0m \\u001b[0mfilepath\\u001b[0m\\u001b[0;34m=\\u001b[0m\\u001b[0mos\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mpath\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0mabspath\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0m__file__\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m      4\\u001b[0m \\u001b[0mlogger\\u001b[0m\\u001b[0;34m.\\u001b[0m\\u001b[0minfo\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0margs\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m      5\\u001b[0m \\u001b[0;34m\\u001b[0m\\u001b[0m\\n\",\"\\u001b[0;31mNameError\\u001b[0m: name 'makedirs' is not defined\"]}]}]}\n","Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.1, nepochs=120, network='odenet', save='/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/experiment_node1', test_batch_size=1000, tol=0.001)\n","Sequential(\n","  (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1))\n","  (1): GroupNorm(32, 64, eps=1e-05, affine=True)\n","  (2): ReLU(inplace=True)\n","  (3): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","  (4): GroupNorm(32, 64, eps=1e-05, affine=True)\n","  (5): ReLU(inplace=True)\n","  (6): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","  (7): ODEBlock(\n","    (odefunc): ODEfunc(\n","      (norm1): GroupNorm(32, 64, eps=1e-05, affine=True)\n","      (relu): ReLU(inplace=True)\n","      (conv1): ConcatConv2d(\n","        (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","      (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)\n","      (conv2): ConcatConv2d(\n","        (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","      (norm3): GroupNorm(32, 64, eps=1e-05, affine=True)\n","    )\n","  )\n","  (8): GroupNorm(32, 64, eps=1e-05, affine=True)\n","  (9): ReLU(inplace=True)\n","  (10): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (11): Flatten()\n","  (12): Linear(in_features=64, out_features=10, bias=True)\n",")\n","Number of parameters: 208266\n"],"name":"stderr"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mitr\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatches_per_epoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_eval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m                 \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_acc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\u001b[0m in \u001b[0;36maccuracy\u001b[0;34m(model, dataset_loader)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mtarget_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mpredicted_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0mtotal_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtarget_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_correct\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegration_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegration_time\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0modeint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0modefunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegration_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchdiffeq/_impl/odeint.py\u001b[0m in \u001b[0;36modeint\u001b[0;34m(func, y0, t, rtol, atol, method, options, event_fn)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mevent_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0msolution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegrate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mevent_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegrate_until_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchdiffeq/_impl/solvers.py\u001b[0m in \u001b[0;36mintegrate\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_integrate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0msolution\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_advance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msolution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchdiffeq/_impl/rk_common.py\u001b[0m in \u001b[0;36m_advance\u001b[0;34m(self, next_t)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mnext_t\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrk_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mn_steps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_num_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'max_num_steps exceeded ({}>={})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_num_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrk_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adaptive_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrk_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m             \u001b[0mn_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_interp_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrk_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterp_coeff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrk_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrk_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchdiffeq/_impl/rk_common.py\u001b[0m in \u001b[0;36m_adaptive_step\u001b[0;34m(self, rk_state)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;31m# trigger both. (i.e. interleaving them would be wrong.)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_runge_kutta_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtableau\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtableau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0;31m# dtypes:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;31m# y1.dtype == self.y0.dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchdiffeq/_impl/rk_common.py\u001b[0m in \u001b[0;36m_runge_kutta_step\u001b[0;34m(func, y0, f0, t0, dt, t1, tableau)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mperturb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerturb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNONE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0myi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta_i\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mti\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperturb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mperturb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_UncheckedAssign\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchdiffeq/_impl/misc.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, t, y, perturb)\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0;31m# Do nothing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, t, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, t, x)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mtt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mttx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mttx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    394\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    395\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 396\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"Ner61Yv1L9ZC","executionInfo":{"status":"ok","timestamp":1617964389098,"user_tz":-60,"elapsed":1452,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["%%bash\n","rm -r /content/experiment_node1"],"execution_count":58,"outputs":[]},{"cell_type":"code","metadata":{"id":"gl0nftCJOgzw"},"source":["\"\"\"\n","--------------------------------------------------------------------------------------------------------------------------------------------\n","The code for Hessian analysis.\n","--------------------------------------------------------------------------------------------------------------------------------------------\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m75L8B4eOjXq","executionInfo":{"status":"ok","timestamp":1617965653332,"user_tz":-60,"elapsed":763,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["def get_manual_hessian(grads, parameters, show_iters=True):\n","  \"\"\"\n","  Calculation of the Hessian using nested for loops.\n","  Inputs:   - grads:        tuple of gradient tensors. Created using something \n","                            like grads = torch.autograd.grad(loss, parameters, create_graph=True).\n","            - parameters:   List of parameter objects. Created using something \n","                            like parameters = optimizer.param_groups[0]['params'].\n","            - show_iters:   True or False, depending on if the iteration number is to be shown during training. \n","                            Note that the iteration updates are not provided every row, but instead periodically \n","                            (roughly according to the number of parameters in the system).\n","  \"\"\"\n","  start = time.time()        \n","\n","  n_params = 0\n","  for param in parameters:\n","    n_params += torch.numel(param)\n","  grads2 = torch.zeros(n_params,n_params)            #Create an matrix of zeros thas has the same shape as the Hessian.\n","\n","  y_counter = 0                             #y_direction refers to row number in the Hessian.\n","\n","  for grad in grads:\n","      grad = torch.reshape(grad, [-1])                                  #Rearrange the gradient information into a vector.        \n","\n","      for j, g in enumerate(grad):\n","        x_counter = 0                                                   #x_direction refers to column number in the Hessian.\n","\n","        for l, param in enumerate(parameters):\n","          g2 = torch.autograd.grad(g, param, retain_graph=True)[0]      #Calculate the gradient of an element of the gradient wrt one layer's parameters.\n","          g2 = torch.reshape(g2, [-1])                                  #Reshape this into a vector.\n","          len = g2.shape[0]                       \n","          grads2[j+y_counter, x_counter:x_counter+len] = g2             #Indexing ensures that the second order derivatives are placed in the correct positions.\n","          x_counter += len\n","\n","      grads2 = grads2.to(device)\n","      y_counter += grad.shape[0]\n","\n","      if show_iters:\n","        print(\"Gradients calculated for row number \" + str(y_counter) + \".\")\n","  \n","  print('Time used was ', time.time() - start)\n","\n","  return grads2"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":232},"id":"-QPba5QNQx6V","executionInfo":{"status":"error","timestamp":1617965601039,"user_tz":-60,"elapsed":1139,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}},"outputId":"008f10c7-272a-431f-a1bd-c9dfbfc275ef"},"source":["__file__ = '/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb'\n","makedirs(args.save)\n","logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))\n","logger.info(args)\n","\n","device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')\n","\n","is_odenet = args.network == 'odenet'\n","\n","if args.downsampling_method == 'conv':\n","    downsampling_layers = [\n","        nn.Conv2d(1, 64, 3, 1),\n","        norm(64),\n","        nn.ReLU(inplace=True),\n","        nn.Conv2d(64, 64, 4, 2, 1),\n","        norm(64),\n","        nn.ReLU(inplace=True),\n","        nn.Conv2d(64, 64, 4, 2, 1),\n","    ]\n","elif args.downsampling_method == 'res':\n","    downsampling_layers = [\n","        nn.Conv2d(1, 64, 3, 1),\n","        ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\n","        ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\n","    ]\n","\n","feature_layers = [ODEBlock(ODEfunc(64))] if is_odenet else [ResBlock(64, 64) for _ in range(6)]\n","fc_layers = [norm(64), nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d((1, 1)), Flatten(), nn.Linear(64, 10)]\n","\n","model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)\n","\n","logger.info(model)\n","logger.info('Number of parameters: {}'.format(count_parameters(model)))\n","\n","criterion = nn.CrossEntropyLoss().to(device)\n","\n","train_loader, test_loader, train_eval_loader = get_mnist_loaders(\n","    args.data_aug, args.batch_size, args.test_batch_size\n",")\n","\n","data_gen = inf_generator(train_loader)\n","batches_per_epoch = len(train_loader)\n","\n","lr_fn = learning_rate_with_decay(\n","    args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],\n","    decay_rates=[1, 0.1, 0.01, 0.001]\n",")\n","\n","optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)\n","\n","best_acc = 0\n","batch_time_meter = RunningAverageMeter()\n","f_nfe_meter = RunningAverageMeter()\n","b_nfe_meter = RunningAverageMeter()\n","end = time.time()\n","\n","epoch_arr = []\n","time_val_arr = []\n","time_avg_arr = []\n","nfe_f_arr = []\n","nfe_b_arr = []\n","train_acc_arr = []\n","test_acc_arr = []\n","\n","for param_group in optimizer.param_groups:\n","    param_group['lr'] = lr_fn(itr)\n","\n","optimizer.zero_grad()\n","x, y = data_gen.__next__()\n","x = x.to(device)\n","y = y.to(device)\n","logits = model(x)\n","loss = criterion(logits, y)\n","\n","grads = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n","parameters = optimizer.param_groups[0]['params']\n","\n","print('Obtaining manual hessian...')\n","manual_hessian = get_manual_hessian(grads, parameters)           #get manual hessian."],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0m__file__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/mnist_node.ipynb'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'logs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'makedirs' is not defined"]}]}]}
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.1, nepochs=120, network='odenet', save='/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/experiment_node1', test_batch_size=1000, tol=0.001)
Namespace(adjoint=False, batch_size=128, data_aug=True, debug=False, downsampling_method='conv', gpu=0, lr=0.1, nepochs=120, network='odenet', save='/content/drive/MyDrive/colab_notebooks/NODE_Hessian/MNIST/experiment_node1', test_batch_size=1000, tol=0.001)
Sequential(
  (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1))
  (1): GroupNorm(32, 64, eps=1e-05, affine=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): GroupNorm(32, 64, eps=1e-05, affine=True)
  (5): ReLU(inplace=True)
  (6): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (norm1): GroupNorm(32, 64, eps=1e-05, affine=True)
      (relu): ReLU(inplace=True)
      (conv1): ConcatConv2d(
        (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)
      (conv2): ConcatConv2d(
        (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (norm3): GroupNorm(32, 64, eps=1e-05, affine=True)
    )
  )
  (8): GroupNorm(32, 64, eps=1e-05, affine=True)
  (9): ReLU(inplace=True)
  (10): AdaptiveAvgPool2d(output_size=(1, 1))
  (11): Flatten()
  (12): Linear(in_features=64, out_features=10, bias=True)
)
Sequential(
  (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1))
  (1): GroupNorm(32, 64, eps=1e-05, affine=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): GroupNorm(32, 64, eps=1e-05, affine=True)
  (5): ReLU(inplace=True)
  (6): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (norm1): GroupNorm(32, 64, eps=1e-05, affine=True)
      (relu): ReLU(inplace=True)
      (conv1): ConcatConv2d(
        (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)
      (conv2): ConcatConv2d(
        (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (norm3): GroupNorm(32, 64, eps=1e-05, affine=True)
    )
  )
  (8): GroupNorm(32, 64, eps=1e-05, affine=True)
  (9): ReLU(inplace=True)
  (10): AdaptiveAvgPool2d(output_size=(1, 1))
  (11): Flatten()
  (12): Linear(in_features=64, out_features=10, bias=True)
)
Number of parameters: 208266
Number of parameters: 208266
