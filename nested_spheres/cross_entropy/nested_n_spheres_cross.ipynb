{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"nested_n_spheres_cross.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1qP29nTrzM_R3MZKa0gUzaljPc8A8NfH5","authorship_tag":"ABX9TyMEJZ38Ua7h6Unn6rtba5B3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"Q0ucFGRjvWzv","executionInfo":{"status":"ok","timestamp":1617611603846,"user_tz":-60,"elapsed":513,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["%%capture\n","\"\"\"\n","------------------------------------------------------------------------------------------------------------------------\n","Code used to perform the nested N-spheres experiment with NODEs and ANODEs. This was taken from Alex's GitHub:\n","https://github.com/a-norcliffe/sonode/blob/master/experiments/nested-n-spheres/nested-n-spheres_anode.py\n","\n","It was then adapted to calculate the output in a slightly different way, by using cross entropy loss.\n","I have also made some small modifications that save the model at given iterations, which can then be used for\n","Hessian analysis.\n","------------------------------------------------------------------------------------------------------------------------\n","\"\"\""],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"cs8sH0JjwGam","executionInfo":{"status":"ok","timestamp":1617611608513,"user_tz":-60,"elapsed":5172,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["%%capture\n","pip install torchdiffeq"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"qMe4eo_4vptM","executionInfo":{"status":"ok","timestamp":1617614710040,"user_tz":-60,"elapsed":535,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["import time\n","import os\n","import argparse\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--tol', type=float, default=1e-3)\n","parser.add_argument('--adjoint', type=eval, default=False)\n","parser.add_argument('--visualise', type=eval, default=True)\n","parser.add_argument('--niters', type=int, default=600)\n","parser.add_argument('--lr', type=float, default=0.01)\n","parser.add_argument('--gpu', type=int, default=0)\n","parser.add_argument('--extra_dim', type=int, default=1)\n","parser.add_argument('--data_dimension', type=int, default=2)\n","parser.add_argument('--npoints', type=int, default=50)\n","parser.add_argument('--ntest', type=int, default=10)\n","parser.add_argument('--hessian_freq', type=int, default=20)\n","args = parser.parse_args(args=[])\n","\n","exp_name = 'experiment_3'\n","args.hessian_freq = 1\n","args.data_dimension = 3\n","args.npoints = 200        #npoints and ntest vary according to the data_dimension.\n","args.ntest = 40\n","args.extra_dim = 1\n","args.visualise = False\n","\n","if args.adjoint:\n","    from torchdiffeq import odeint_adjoint as odeint\n","else:\n","    from torchdiffeq import odeint"],"execution_count":61,"outputs":[]},{"cell_type":"code","metadata":{"id":"NT18Ots9v6Tj","executionInfo":{"status":"ok","timestamp":1617612023270,"user_tz":-60,"elapsed":522,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["class ODEfunc(nn.Module):\n","    \"\"\"\n","    Neural network to parametrise the derivative of the state vector. Maps from [dim] to [dim] dimensions.\n","    \"\"\"\n","    def __init__(self, dim, nhidden):\n","        super(ODEfunc, self).__init__()\n","        self.elu = nn.ELU(inplace=True)\n","        self.fc1 = nn.Linear(dim, nhidden)\n","        self.fc2 = nn.Linear(nhidden, nhidden)\n","        self.fc3 = nn.Linear(nhidden, dim)\n","        self.nfe = 0\n","\n","    def forward(self, t, x):\n","        self.nfe += 1\n","        out = self.fc1(x)\n","        out = self.elu(out)\n","        out = self.fc2(out)\n","        out = self.elu(out)\n","        out = self.fc3(out)\n","        return out\n","    \n","\n","class ODEBlock(nn.Module):\n","    \"\"\"\n","    Defines the entire ODE block that acts on the state vector, i.e. it perfoms integration on the state vector\n","    with the derivative given by an ODEFunc() object, and interval given by [t0, tN].\n","    \"\"\"\n","    def __init__(self, odefunc, t0_, tN_):\n","        super(ODEBlock, self).__init__()\n","        self.odefunc = odefunc\n","        self.integration_times = torch.tensor([t0_, tN_]).float()\n","        \n","    def forward(self, x):\n","        out = odeint(self.odefunc, x, self.integration_times, rtol=args.tol, atol=args.tol)\n","        out = out[1]\n","        return out\n","\n","    @property\n","    def nfe(self):\n","        return self.odefunc.nfe\n","\n","    @nfe.setter\n","    def nfe(self, value):\n","        self.odefunc.nfe = value"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"K9x36nN_v-3M","executionInfo":{"status":"ok","timestamp":1617612026218,"user_tz":-60,"elapsed":471,"user":{"displayName":"L. Atkins","photoUrl":"","userId":"13582269488947613307"}}},"source":["class Decoder(nn.Module):\n","    \"\"\"\n","    Function that maps 2D output to another vector. \n","    \"\"\"\n","    def __init__(self, in_dim, out_dim):          #out_dim = 1.\n","        super(Decoder, self).__init__()\n","        self.fc = nn.Linear(in_dim, out_dim)\n","\n","    def forward(self, z):\n","        out = self.fc(z)\n","        return out\n","\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"jSqPmS8Av_s0"},"source":["if __name__ == '__main__':\n","    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')\n","    print(exp_name.title())\n","    if args.extra_dim == 0:\n","        print('NODE')\n","        filename = ('/content/drive/MyDrive/colab_notebooks/NODE_Hessian/nested_spheres/cross_entropy/' + str(args.data_dimension) \n","                  + '_dims/' + exp_name + '/node/')\n","        models_filename = ('/content/drive/MyDrive/colab_notebooks/NODE_Hessian/nested_spheres/cross_entropy/' + str(args.data_dimension) \n","                          + '_dims/' + exp_name + '/node/models/')\n","    else:\n","        print('ANODE(' + str(args.extra_dim) + ')')\n","        filename = ('/content/drive/MyDrive/colab_notebooks/NODE_Hessian/nested_spheres/cross_entropy/' + str(args.data_dimension) + '_dims/' \n","                    + exp_name + '/anode('+str(args.extra_dim)+')/')\n","        models_filename = ('/content/drive/MyDrive/colab_notebooks/NODE_Hessian/nested_spheres/cross_entropy/' + str(args.data_dimension) + '_dims/'\n","                          + exp_name + '/anode('+str(args.extra_dim)+')/models/')\n","    try:\n","        os.makedirs(filename)\n","        \n","    except FileExistsError:\n","        pass\n","\n","    try:\n","        os.makedirs(models_filename)\n","        \n","    except FileExistsError:\n","        pass\n","\n","    dim = args.data_dimension + args.extra_dim\n","    \n","    #Download data\n","    name_in = str(args.data_dimension)+'din_'+str(args.npoints)+'_train.npy'        #Only use the training data.\n","    name_out = str(args.data_dimension)+'dout_'+str(args.npoints)+'_train.npy'\n","    folder_name = '/content/drive/MyDrive/colab_notebooks/NODE_Hessian/nested_spheres/cross_entropy/2_dims/' + exp_name + '/data/'\n","    z0 = torch.tensor(np.load(folder_name+name_in)).float().to(device)\n","    zN = torch.tensor(np.load(folder_name+name_out)).float().to(device)\n","    target = torch.zeros(len(zN))\n","    for i in range(len(zN)):\n","      if zN[i]==0:\n","        target[i] = 0\n","      else:\n","        target[i] = 1\n","    target = target.long()\n","    \n","    #Augment z0\n","    zeros = torch.zeros(args.npoints, args.extra_dim).float()\n","    z0 = torch.cat((z0, zeros), dim=1).to(device)\n","\n","    # model\n","    t0, tN = 0, 1\n","    nhidden = 20\n","    feature_layers = [ODEBlock(ODEfunc(dim, nhidden), t0, tN), Decoder(dim, 2)]\n","    model = nn.Sequential(*feature_layers).to(device)         #the * bit means that only the elements of the list are taken.\n","    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n","    loss_func = nn.CrossEntropyLoss()\n","    \n","    itr_arr = np.empty(args.niters)\n","    loss_arr = np.empty(args.niters)\n","    nfe_arr = np.empty(args.niters)\n","    time_arr = np.empty(args.niters)\n","\n","    # training\n","    start_time = time.time()\n","    torch.save(model, models_filename + 'model_0.pt')\n","    for itr in range(1, args.niters + 1):\n","        feature_layers[0].nfe = 0\n","        iter_start_time = time.time()\n","        optimizer.zero_grad()\n","\n","        # forward in time and solve ode\n","        pred_z = model(z0)\n","        # compute loss\n","        loss = loss_func(pred_z, target)\n","        loss.backward()\n","        optimizer.step()\n","        iter_end_time = time.time()\n","        #make arrays\n","        itr_arr[itr-1] = itr\n","        loss_arr[itr-1] = loss\n","        nfe_arr[itr-1] = feature_layers[0].nfe\n","        time_arr[itr-1] = iter_end_time-iter_start_time \n","\n","        if itr % args.hessian_freq == 0:\n","          torch.save(model, models_filename + 'model_' + str(itr) + '.pt')\n","        else:\n","          pass\n","\n","        print('Iter: {}, running loss: {:.4f}'.format(itr, loss))\n","            \n","\n","    end_time = time.time()\n","    print('\\n')\n","    print('Training complete after {} iters.'.format(itr))\n","    print('Time = ' + str(end_time-start_time))\n","    loss = loss_func(pred_z, target).detach().numpy()\n","    print('Train Loss = ' +str(loss))\n","    print('NFE = ' +str(feature_layers[0].nfe))\n","    print('Parameters = '+str(count_parameters(model)))\n","    \n","    np.save(filename+'itr_arr.npy', itr_arr)\n","    np.save(filename+'nfe_arr.npy', nfe_arr)\n","    np.save(filename+'loss_arr.npy', loss_arr)\n","    np.save(filename+'time_arr.npy', time_arr)\n","    torch.save(model, filename+'model.pth')\n","    \n","    # get test data\n","    name_in = str(args.data_dimension)+'din_'+str(args.ntest)+'_test.npy'\n","    name_out = str(args.data_dimension)+'dout_'+str(args.ntest)+'_test.npy'\n","    folder_name = '/content/drive/MyDrive/colab_notebooks/NODE_Hessian/nested_spheres/cross_entropy/2_dims/' + exp_name + '/data/'\n","    z0 = torch.tensor(np.load(folder_name+name_in)).float().to(device)\n","    zN = torch.tensor(np.load(folder_name+name_out)).float().to(device)\n","    target = torch.zeros(len(zN))\n","    for i in range(len(zN)):\n","      if zN[i]==0:\n","        target[i] = 0\n","      else:\n","        target[i] = 1\n","    target = target.long()\n","\n","    # augment z0\n","    zeros = torch.zeros(args.ntest, args.extra_dim).float()\n","    z0 = torch.cat((z0, zeros), dim=1).to(device)\n","    \n","    # run test data through network\n","    pred_z = model(z0)\n","\n","    # compute loss\n","    loss = loss_func(pred_z, target).detach().numpy()\n","    print('Test Loss = ' +str(loss))\n","    \n","    if args.visualise:\n","        try:\n","            os.makedirs('/content/drive/MyDrive/colab_notebooks/NODE_Hessian/nested_spheres/cross_entropy/' + str(args.data_dimension) + '_dims/' \n","                          + exp_name + '/figure_data/')\n","        except FileExistsError:\n","            pass\n","        #Generate a sequence of timepoints, such that the model can be evaluated at intermediate times during integration.   \n","        samp_ts = torch.linspace(t0, tN, 30)\n","        if args.data_dimension == 1:\n","            z0 = torch.tensor(np.load(  '/content/drive/MyDrive/colab_notebooks/NODE_Hessian/nested_spheres/cross_entropy/2_dims/' + exp_name + '/data'\n","                                      + '/vis_data/1d_vis_data.npy')).float()\n","            ntotal = len(z0)\n","            #Augment z0\n","            zeros = torch.zeros(ntotal, args.extra_dim).float()\n","            z0 = torch.cat((z0, zeros), dim=1).to(device)\n","            pred_z = odeint(feature_layers[0].odefunc, z0, samp_ts)\n","            pred_z = pred_z.detach().numpy()\n","            if args.extra_dim == 0:\n","                name =  ('/content/drive/MyDrive/colab_notebooks/NODE_Hessian/nested_spheres/cross_entropy/' + str(args.data_dimension) + '_dims/'\n","                 + exp_name + '/figure_data' + '/node_film_1d.npy')\n","            else:\n","                name =  ('/content/drive/MyDrive/colab_notebooks/NODE_Hessian/nested_spheres/cross_entropy/' + str(args.data_dimension) + '_dims/'\n","                    + exp_name + '' + '/figure_data/anode_film_(1+'+str(args.extra_dim)+')d.npy')\n","            np.save(name, pred_z)\n","        elif args.data_dimension == 2:\n","            z0 = torch.tensor(np.load(  '/content/drive/MyDrive/colab_notebooks/NODE_Hessian/nested_spheres/cross_entropy/2_dims/' + exp_name + '/data'\n","                                      + '/vis_data/2d_vis_data.npy')).float()\n","            ntotal = len(z0)\n","            #Augment z0\n","            zeros = torch.zeros(ntotal, args.extra_dim).float()\n","            z0 = torch.cat((z0, zeros), dim=1).to(device)\n","            pred_z = odeint(feature_layers[0].odefunc, z0, samp_ts)\n","            pred_z = pred_z.detach().numpy()\n","            if args.extra_dim == 0:\n","                name = ('/content/drive/MyDrive/colab_notebooks/NODE_Hessian/nested_spheres/cross_entropy/' + str(args.data_dimension) + '_dims/'\n","                + exp_name + '/figure_data/node_film_2d.npy')\n","            else:\n","                name = ('/content/drive/MyDrive/colab_notebooks/NODE_Hessian/nested_spheres/cross_entropy/' + str(args.data_dimension) + '_dims/' \n","                        + exp_name + '/figure_data/anode_film_(2+'+str(args.extra_dim)+')d.npy')\n","            np.save(name, pred_z)\n","        elif args.data_dimension == 3:\n","            z0 = torch.tensor(np.load(  '/content/drive/MyDrive/colab_notebooks/NODE_Hessian/nested_spheres/cross_entropy/2_dims/' + exp_name + '/data'\n","                                      + '/vis_data/3d_vis_data.npy')).float()\n","            ntotal = len(z0)\n","            #Augment z0\n","            zeros = torch.zeros(ntotal, args.extra_dim).float()\n","            z0 = torch.cat((z0, zeros), dim=1).to(device)\n","            pred_z = odeint(feature_layers[0].odefunc, z0, samp_ts)\n","            pred_z = pred_z.detach().numpy()\n","            if args.extra_dim == 0:\n","                name = ('/content/drive/MyDrive/colab_notebooks/NODE_Hessian/nested_spheres/cross_entropy/' + str(args.data_dimension) + '_dims/' \n","                        + exp_name + '/figure_data/node_film_3d.npy')\n","            else:\n","                pass\n","            np.save(name, pred_z)\n","        else:\n","            pass\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-zCAM75wLjYr"},"source":["#Generate loss curve plot.\n","exp_name = 'experiment_3'\n","node_type = 'anode(1)'\n","save = True\n","log = True\n","print('Data dimension: ' + str(args.data_dimension))\n","\n","loss_data = np.load('/content/drive/MyDrive/colab_notebooks/NODE_Hessian/nested_spheres/cross_entropy/' + str(args.data_dimension) + '_dims/'\n","                    + exp_name + '/' + node_type + '/loss_arr.npy')\n","\n","iters = np.linspace(1,600,600)\n","\n","plt.figure(figsize=[10,10])\n","plt.plot(iters, loss_data)\n","plt.xlabel('Iteration')\n","plt.ylabel('Loss')\n","plt.title('Loss Curve for Nested Spheres Experiment\\n' + str(node_type.upper()) + ', Cross Entropy')\n","if log:\n","  plt.yscale('log')\n","  if save:\n","    plt.savefig('/content/drive/MyDrive/colab_notebooks/NODE_Hessian/nested_spheres/cross_entropy/' + str(args.data_dimension) + '_dims/'\n","                      + exp_name + '/' + node_type + '/log_loss_curve.pdf')\n","else:\n","  if save:\n","    plt.savefig('/content/drive/MyDrive/colab_notebooks/NODE_Hessian/nested_spheres/cross_entropy/' + str(args.data_dimension) + '_dims/'\n","                      + exp_name + '/' + node_type + '/loss_curve.pdf')\n","plt.show()\n"],"execution_count":null,"outputs":[]}]}