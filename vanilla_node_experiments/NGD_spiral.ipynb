{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NGD_spiral.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"14k2Ppous_LDeM8trgM5S6ycJ8XwFZbrf","authorship_tag":"ABX9TyNy7clVWJYWowvAzjfWOxlW"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"7JRQSt7ov_cR"},"source":["#Libraries\n","import os\n","import argparse\n","import time\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nQ1KbcXrwHqY"},"source":["#This is used to set the arguments that can be passed when the file is run from the command line.\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--method', type=str, choices=['dopri5', 'adams'], default='dopri5')\n","parser.add_argument('--data_size', type=int, default=1000)\n","parser.add_argument('--batch_time', type=int, default=10)\n","parser.add_argument('--batch_size', type=int, default=20)\n","parser.add_argument('--niters', type=int, default=100)\n","parser.add_argument('--test_freq', type=int, default=20)\n","parser.add_argument('--lr', type=float, default=1e-3)\n","parser.add_argument('--damping', type=float, default=1e-8)\n","parser.add_argument('--viz', action='store_true')\n","parser.add_argument('--gpu', type=int, default=0)\n","parser.add_argument('--adjoint', action='store_true')\n","parser.add_argument('--ngd', action='store_true')\n","args = parser.parse_args()\n","print('Learning rate is ' + str(args.lr))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0HcKPVWxwJ1i"},"source":["#Decides which ODE solver to use according to whether the adjoint method is required.\n","#autograd is just using the chain rule backprop through the network and each module used will (usually implicitly) define a way to do so.\n","#This means that importing odeint_adjoint ensures that backpropagation is later done in the by defining an adjoint state, etc.\n","\n","if args.adjoint:\n","    from torchdiffeq import odeint_adjoint as odeint\n","    print(\"Using the adjoint method\")\n","else:\n","    from torchdiffeq import odeint\n","    print(\"Not using the adjoint method\")\n","\n","#A torch.device is an object representing the device on which a torch.Tensor is or will be allocated.\n","#torch.device is a class, and the line of code below creates an instance of that class.\n","\n","device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mxdrn8zCwM8Q"},"source":["#torch.tensor is a class which constructs a tensor with the data that is input.\n","#torch.tensor.to() is a method which performs tensor dtype and/or device conversion.\n","#torch.linspace creates a 1 dimensional tensor of evenly spaced values.\n","\n","true_y0 = torch.tensor([[2., 0.]]).to(device)\n","t = torch.linspace(0., 25., args.data_size).to(device)\n","true_A = torch.tensor([[-0.1, 2.0], [-2.0, -0.1]]).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W42XLBXAwRJr"},"source":["#torch.mm performs matrix multiplication of the 2 inputs. \n","#Lambda is the function that defines the ODE, i.e. dy/dt = Lambda(y). It is clearly non-linear.\n","\n","class Lambda(nn.Module):\n","\n","    def forward(self, t, y):\n","        return torch.mm(y**3, true_A)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LnYjgL1OwRru"},"source":["#torch.no_grad() disables gradient calculation, i.e. it disables the autograd engine.\n","#This will reduce memory usage and speed up computations but you won’t be able to backprop.\n","#This is useful for all tensors that don't require gradients.\n","#odeint() solves an ode up to a time, t.\n","\n","#It's not obvious, but this true_y solution defines a spiral in the x-y plane (I verified this computationally).\n","with torch.no_grad():\n","    true_y = odeint(Lambda(), true_y0, t, method='dopri5')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xkiw3UaxwULU"},"source":["#torch.from_numpy() creates a tensor from a numpy ndarray.\n","#torch.stack concatenates a sequence of tensors along a new dimension.\n","\n","def get_batch():\n","\n","    #Generates a random list of integers in the range (args.data_size - args.batch_time), of length (args.batch_size).\n","    s = torch.from_numpy(np.random.choice(np.arange(args.data_size - args.batch_time, dtype=np.int64), args.batch_size, replace=False)) \n","\n","    #Creates the random batch. batch_y will be our ground truth when optimising the neural net.\n","    batch_y0 = true_y[s]  # (M, D)\n","    batch_t = t[:args.batch_time]  # (T)\n","    batch_y = torch.stack([true_y[s + i] for i in range(args.batch_time)], dim=0)  # (T, M, D)\n","    return batch_y0.to(device), batch_t.to(device), batch_y.to(device)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IcDIT89TwWUL"},"source":["#This is used to make a new directory if the results of the experiment are to be saved.\n","\n","def makedirs(dirname):\n","    if not os.path.exists(dirname):\n","        os.makedirs(dirname)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sZ7lrZd9xGjW"},"source":["if args.viz:\n","    makedirs('png')\n","    import matplotlib.pyplot as plt\n","    fig = plt.figure(figsize=(12, 4), facecolor='white')  #'facecolor is the background colour.\n","    ax_traj = fig.add_subplot(131, frameon=False)         #add axes to the figure as part of the subplot arrangement.\n","    ax_phase = fig.add_subplot(132, frameon=False)\n","    ax_vecfield = fig.add_subplot(133, frameon=False)\n","    #plt.show(block=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q606FpwvxJsX"},"source":["def visualize_3(true_y, pred_y, odefunc, itr):\n","\n","  \"\"\"\n","  This slightly altered version of the function visualize() seems to work fine. The only change is that I have moved the plt.figure() part of the code\n","  inside the function itself, i.e. I am creating a new figure environment for every figure, instead of editing the same environment multiple times.\n","  \"\"\"\n","\n","  if args.viz:\n","\n","    fig = plt.figure(figsize=(12, 4), facecolor='white')  #'facecolor is the background colour.\n","    ax_traj = fig.add_subplot(131, frameon=False)         #add axes to the figure as part of the subplot arrangement.\n","    ax_phase = fig.add_subplot(132, frameon=False)\n","    ax_vecfield = fig.add_subplot(133, frameon=False)\n","\n","    ax_traj.set_title('Trajectories')\n","    ax_traj.set_xlabel('t')\n","    ax_traj.set_ylabel('x,y')\n","    ax_traj.plot(t.cpu().numpy(), true_y.cpu().numpy()[:, 0, 0], t.cpu().numpy(), true_y.cpu().numpy()[:, 0, 1], 'g-')\n","    ax_traj.plot(t.cpu().numpy(), pred_y.cpu().numpy()[:, 0, 0], '--', t.cpu().numpy(), pred_y.cpu().numpy()[:, 0, 1], 'b--')\n","    ax_traj.set_xlim(t.cpu().min(), t.cpu().max())\n","    ax_traj.set_ylim(-2, 2)\n","\n","    ax_phase.set_title('Phase Portrait')\n","    ax_phase.set_xlabel('x')\n","    ax_phase.set_ylabel('y')\n","    ax_phase.plot(true_y.cpu().numpy()[:, 0, 0], true_y.cpu().numpy()[:, 0, 1], 'g-')\n","    ax_phase.plot(pred_y.cpu().numpy()[:, 0, 0], pred_y.cpu().numpy()[:, 0, 1], 'b--')\n","    ax_phase.set_xlim(-2, 2)\n","    ax_phase.set_ylim(-2, 2)\n","\n","    ax_vecfield.set_title('Learned Vector Field')\n","    ax_vecfield.set_xlabel('x')\n","    ax_vecfield.set_ylabel('y')\n","\n","    y, x = np.mgrid[-2:2:21j, -2:2:21j]\n","    dydt = odefunc(0, torch.Tensor(np.stack([x, y], -1).reshape(21 * 21, 2)).to(device)).cpu().detach().numpy()\n","    mag = np.sqrt(dydt[:, 0]**2 + dydt[:, 1]**2).reshape(-1, 1)\n","    dydt = (dydt / mag)\n","    dydt = dydt.reshape(21, 21, 2)\n","\n","    ax_vecfield.streamplot(x, y, dydt[:, :, 0], dydt[:, :, 1], color=\"black\")\n","    ax_vecfield.set_xlim(-2, 2)\n","    ax_vecfield.set_ylim(-2, 2)\n","\n","    fig.tight_layout()\n","    plt.savefig('png/{:03d}'.format(itr))\n","    plt.draw()\n","    plt.pause(0.001)\n","    plt.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mSB5TYdsxKRe"},"source":["class ODEFunc(nn.Module):\n","\n","    def __init__(self):\n","        super(ODEFunc, self).__init__()\n","\n","        #Define a very simple neural network architecture with 1 hidden layer.\n","        self.net = nn.Sequential(\n","            nn.Linear(2, 50),\n","            nn.Tanh(),\n","            nn.Linear(50, 2),\n","        )\n","\n","        #Initialise the weights and biases for the linear layers.\n","        #The isinstance functions checks that the first input is an instance or subclass of the second argument.\n","\n","        for m in self.net.modules():\n","            if isinstance(m, nn.Linear):\n","                nn.init.normal_(m.weight, mean=0, std=0.1)\n","                nn.init.constant_(m.bias, val=0)\n","\n","    #The forward function defines how the data is passed through the neural net. \n","    #In particular, it is called when you apply the neural net to an input variable.\n","    #We act the net on y**3 such that it is only learning to represent the matrix (see class Lambda)\n","\n","    def forward(self, t, y):\n","        return self.net(y**3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"owHwY8fBxMZn"},"source":["class RunningAverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","\n","    def __init__(self, momentum=0.99):\n","        self.momentum = momentum\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = None\n","        self.avg = 0\n","\n","    def update(self, val):\n","        if self.val is None:\n","            self.avg = val\n","        else:\n","            self.avg = self.avg * self.momentum + val * (1 - self.momentum)\n","        self.val = val"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LmtgP4iYyOcE"},"source":["class NGD:\n","  \"\"\"\n","  Implements Natural Gradient Descent.\n","  \"\"\"\n","  def __init__(self, net, lr, damping):\n","    self.net = net\n","    self.lr = lr\n","    self.grads = torch.tensor([]).to(device)     #Empty tensor in which to store gradient values.\n","    self.damping = damping            #Damping parameter.\n","\n","  def zero_grad(self):\n","    \"\"\"\n","    Function used to set all gradients to 0. I have checked this and it works as expected.\n","    \"\"\"\n","    for param in self.net.parameters():\n","      shape = param.grad.data.shape\n","      param.grad.data = torch.zeros(shape).to(device)\n","\n","  def get_FIM(self):\n","    \"\"\"\n","    Evaluates the Fisher Information Matrix. I have checked this, and as far as I can tell it works as expected.\n","    \"\"\"\n","    grads = self.grads\n","    for param in self.net.parameters():\n","        grad = torch.reshape(param.grad.data.to(device), (-1,))  #Reshapes the gradient to a 1D vector.\n","        grads = torch.cat((grad, grads), 0)           #Stores the gradient for the individual parameter to the overall gradient vector.\n","\n","    FIM = torch.matmul(grads.reshape(-1,1), grads.reshape(1,-1))\n","    D = FIM.shape[0]\n","    FIM += self.damping * np.eye(D)\n","\n","    return FIM\n","\n","  def step(self):\n","    \"\"\"\n","    Performs an optimization step by Natural Gradient Descent. This method doesn't work because I iterate over\n","    self.params - a generator object - multiple times. This also means that calling zero_grad() or get_FIM() before this\n","    method means that it doesn't work at all. I need to find a way to fix this!\n","    \"\"\"\n","    grads = self.grads\n","\n","    for param in self.net.parameters():\n","        grad = torch.reshape(param.grad.data.to(device), (-1,))\n","        grads = torch.cat((grad, grads), 0)\n","\n","    FIM = torch.matmul(grads.reshape(-1,1), grads.reshape(1,-1))\n","    D = FIM.shape[0]\n","    identity = torch.from_numpy(np.eye(D))\n","    FIM += self.damping * identity.to(device)       \n","    IFIM = torch.inverse(FIM)                         #Calculates the inverse Fisher.\n","\n","    grads = torch.reshape(grads, (-1,1))              #Reshapes the gradient vector to be a (-1 x 1) tensor.\n","    natural_grads = torch.matmul(IFIM.float(), grads.float())\n","    natural_grads = torch.reshape(natural_grads, (-1,))   #Reshapes the natural gradient from a (-1 x 1) tensor to a vector.\n","\n","    for param in self.net.parameters():\n","      num_els = torch.numel(param.grad.data)\n","      a = natural_grads[:num_els].reshape(param.grad.data.shape)\n","      param.data.sub_(a * self.lr)\n","      natural_grads = natural_grads[num_els:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-TIzRRGrxklx"},"source":["def get_optimizer(net):\n","  \"\"\"\n","  Returns the optimizer to be used for gradient descent.\n","  \"\"\"\n","  if args.ngd:\n","    optimizer = NGD(net, lr=args.lr, damping=args.damping)\n","    print(\"Using Natural Gradient Descent\")\n","  else:\n","    optimizer = optim.RMSprop(net.parameters(), lr=args.lr)\n","    print('Using Stochastic Gradient Descent')\n","\n","  return optimizer\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uD26Jv9wxPI4"},"source":["if __name__ == '__main__':\n","\n","    ii = 0\n","\n","    func = ODEFunc().to(device)\n","    \n","    optimizer = get_optimizer(func) #func.parameters are the parameters to optimise.\n","    end = time.time()\n","\n","    time_meter = RunningAverageMeter(0.97)\n","    \n","    loss_meter = RunningAverageMeter(0.97)\n","\n","    for itr in range(1, args.niters + 1):\n","        func.zero_grad()                                      #Clears x.grad for every parameter x. It’s important to call this before loss.backward(), otherwise you’ll accumulate the gradients from multiple passes.\n","        batch_y0, batch_t, batch_y = get_batch()              #Data that we will use.\n","        pred_y = odeint(func, batch_y0, batch_t).to(device)   #Calculate output values from the NODE system. In other words, we compute a forward pass.\n","        loss = torch.mean(torch.abs(pred_y - batch_y))        #Calculate the loss.\n","        loss.backward()                                       #Calculates the gradient of the loss surface. These are accumulated into x.grad for every parameter x.\n","        optimizer.step()                                      #Updates the value of x using the gradient x.grad.  \n","        \n","        time_meter.update(time.time() - end)\n","        loss_meter.update(loss.item())\n","\n","        #This essentially prints the loss, etc. at regular intervals. It does so by evaluating the predicted values over all time steps of the ODE\n","        #instead of just a batch sample. I assume this means it gives the exact value of the loss at that stage.\n","\n","        if itr % args.test_freq == 0:\n","            with torch.no_grad():\n","                pred_y = odeint(func, true_y0, t)\n","                loss = torch.mean(torch.abs(pred_y - true_y))\n","                print('Iter {:04d} | Total Loss {:.6f}'.format(itr, loss.item()))\n","                visualize_3(true_y, pred_y, func, ii)\n","                ii += 1\n","\n","        end = time.time()"],"execution_count":null,"outputs":[]}]}