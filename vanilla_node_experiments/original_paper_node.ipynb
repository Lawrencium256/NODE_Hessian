{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"original_paper_node.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"14YrwMMBIZMtBAQUc6APD3DCMbhnn7Pjf","authorship_tag":"ABX9TyPGfhBD43mOMpG5Hz/iFW4c"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"uqZsIwjwmG2p"},"source":["%%bash \n","pip install torchdiffeq"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_iqOcAXt2wMy"},"source":["#Libraries\n","import os\n","import argparse\n","import time\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N4O95yfu3BLY"},"source":["#This is used to set the arguments that can be passed when the file is run from the command line.\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--method', type=str, choices=['dopri5', 'adams'], default='dopri5')\n","parser.add_argument('--data_size', type=int, default=1000)\n","parser.add_argument('--batch_time', type=int, default=10)\n","parser.add_argument('--batch_size', type=int, default=20)\n","parser.add_argument('--niters', type=int, default=100)\n","parser.add_argument('--test_freq', type=int, default=20)\n","parser.add_argument('--viz', action='store_true')\n","parser.add_argument('--gpu', type=int, default=0)\n","parser.add_argument('--adjoint', action='store_true')\n","args = parser.parse_args(args=[])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P2TQOgZL-ncp"},"source":["#Decides which ODE solver to use according to whether the adjoint method is required.\n","#autograd is just using the chain rule backprop through the network and each module used will (usually implicitly) define a way to do so.\n","#This means that importing odeint_adjoint ensures that backpropagation is later done in the by defining an adjoint state, etc.\n","\n","if args.adjoint:\n","    from torchdiffeq import odeint_adjoint as odeint\n","else:\n","    from torchdiffeq import odeint"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5lVmWz-Y-oKR"},"source":["#A torch.device is an object representing the device on which a torch.Tensor is or will be allocated.\n","#torch.device is a class, and the line of code below creates an instance of that class.\n","\n","device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kCN4uPGo_a8F"},"source":["#torch.tensor is a class which constructs a tensor with the data that is input.\n","#torch.tensor.to() is a method which performs tensor dtype and/or device conversion.\n","#torch.linspace creates a 1 dimensional tensor of evenly spaced values.\n","\n","true_y0 = torch.tensor([[2., 0.]]).to(device)\n","t = torch.linspace(0., 25., args.data_size).to(device)\n","true_A = torch.tensor([[-0.1, 2.0], [-2.0, -0.1]]).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w4vkZRHo_bGG"},"source":["#torch.mm performs matrix multiplication of the 2 inputs. \n","#Lambda is the function that defines the ODE, i.e. dy/dt = Lambda(y). It is clearly non-linear.\n","\n","class Lambda(nn.Module):\n","\n","    def forward(self, t, y):\n","        return torch.mm(y**3, true_A)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BBET4TlB_bNW"},"source":["#torch.no_grad() disables gradient calculation, i.e. it disables the autograd engine.\n","#This will reduce memory usage and speed up computations but you won’t be able to backprop.\n","#This is useful for all tensors that don't require gradients.\n","#odeint() solves an ode up to a time, t.\n","\n","#It's not obvious, but this true_y solution defines a spiral in the x-y plane (I verified this computationally).\n","with torch.no_grad():\n","    true_y = odeint(Lambda(), true_y0, t, method='dopri5')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B3grts7P_bUL"},"source":["#torch.from_numpy() creates a tensor from a numpy ndarray.\n","#torch.stack concatenates a sequence of tensors along a new dimension.\n","\n","def get_batch():\n","\n","    #Generates a random list of integers in the range (args.data_size - args.batch_time), of length (args.batch_size).\n","    s = torch.from_numpy(np.random.choice(np.arange(args.data_size - args.batch_time, dtype=np.int64), args.batch_size, replace=False)) \n","\n","    #Creates the random batch. batch_y will be our ground truth when optimising the neural net.\n","    batch_y0 = true_y[s]  # (M, D)\n","    batch_t = t[:args.batch_time]  # (T)\n","    batch_y = torch.stack([true_y[s + i] for i in range(args.batch_time)], dim=0)  # (T, M, D)\n","    return batch_y0.to(device), batch_t.to(device), batch_y.to(device)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ktvy_1rE_bax"},"source":["#This is used to make a new directory if the results of the experiment are to be saved.\n","\n","def makedirs(dirname):\n","    if not os.path.exists(dirname):\n","        os.makedirs(dirname)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b-3krz_a_bgU"},"source":["if args.viz:\n","    makedirs('png')\n","    import matplotlib.pyplot as plt\n","    fig = plt.figure(figsize=(12, 4), facecolor='white')  #'facecolor is the background colour.\n","    ax_traj = fig.add_subplot(131, frameon=False)         #add axes to the figure as part of the subplot arrangement.\n","    ax_phase = fig.add_subplot(132, frameon=False)\n","    ax_vecfield = fig.add_subplot(133, frameon=False)\n","    #plt.show(block=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TdPlb7nbTXM-"},"source":["def visualize(true_y, pred_y, odefunc, itr):\n","    \"\"\"\n","    This function was created by the authors and is used to visualise our solution. It didn't quite seem to work for me, so I made a slight modification with \n","    the function visualize_3().\n","    \"\"\"\n","\n","    if args.viz:\n","\n","        ax_traj.cla()                       #Clear subplots.\n","        ax_traj.set_title('Trajectories')\n","        ax_traj.set_xlabel('t')\n","        ax_traj.set_ylabel('x,y')\n","        ax_traj.plot(t.cpu().numpy(), true_y.cpu().numpy()[:, 0, 0], t.cpu().numpy(), true_y.cpu().numpy()[:, 0, 1], 'g-')\n","        ax_traj.plot(t.cpu().numpy(), pred_y.cpu().numpy()[:, 0, 0], '--', t.cpu().numpy(), pred_y.cpu().numpy()[:, 0, 1], 'b--')\n","        ax_traj.set_xlim(t.cpu().min(), t.cpu().max())\n","        ax_traj.set_ylim(-2, 2)\n","        #ax_traj.legend()\n","\n","        ax_phase.cla()\n","        ax_phase.set_title('Phase Portrait')\n","        ax_phase.set_xlabel('x')\n","        ax_phase.set_ylabel('y')\n","        ax_phase.plot(true_y.cpu().numpy()[:, 0, 0], true_y.cpu().numpy()[:, 0, 1], 'g-')\n","        ax_phase.plot(pred_y.cpu().numpy()[:, 0, 0], pred_y.cpu().numpy()[:, 0, 1], 'b--')\n","        ax_phase.set_xlim(-2, 2)\n","        ax_phase.set_ylim(-2, 2)\n","\n","        ax_vecfield.cla()\n","        ax_vecfield.set_title('Learned Vector Field')\n","        ax_vecfield.set_xlabel('x')\n","        ax_vecfield.set_ylabel('y')\n","\n","        y, x = np.mgrid[-2:2:21j, -2:2:21j]\n","        dydt = odefunc(0, torch.Tensor(np.stack([x, y], -1).reshape(21 * 21, 2)).to(device)).cpu().detach().numpy()\n","        mag = np.sqrt(dydt[:, 0]**2 + dydt[:, 1]**2).reshape(-1, 1)\n","        dydt = (dydt / mag)\n","        dydt = dydt.reshape(21, 21, 2)\n","\n","        ax_vecfield.streamplot(x, y, dydt[:, :, 0], dydt[:, :, 1], color=\"black\")\n","        ax_vecfield.set_xlim(-2, 2)\n","        ax_vecfield.set_ylim(-2, 2)\n","\n","        fig.tight_layout()\n","        plt.savefig('png/{:03d}'.format(itr))\n","        plt.draw()\n","        plt.pause(0.001)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3sRdq-Qykbpt"},"source":["def visualize_2(true_y, pred_y, odefunc, itr):\n","\n","  \"\"\"\n","  I made this function to aid help me understand why the visualize() function wasn't working. It isn't particularly useful for anything!\n","  \"\"\"\n","\n","  if args.viz:\n","    makedirs('png_2')\n","    import matplotlib.pyplot as plt\n","    fig, ax = plt.subplots()\n","    ax.plot(true_y.cpu().numpy()[:, 0, 0], true_y.cpu().numpy()[:, 0, 1], 'g-')\n","    ax.plot(pred_y.cpu().numpy()[:, 0, 0], pred_y.cpu().numpy()[:, 0, 1], 'b--')\n","    ax.set_xlim(-2, 2)\n","    ax.set_ylim(-2, 2)\n","    ax.set(xlabel='x', ylabel='y', title='Phase Portrait')\n","    plt.savefig('png_2/{:03d}'.format(itr))\n","    plt.close()\n","    \n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CT3q5POMuWKs"},"source":["def visualize_3(true_y, pred_y, odefunc, itr):\n","\n","  \"\"\"\n","  This slightly altered version of the function visualize() seems to work fine. The only change is that I have moved the plt.figure() part of the code\n","  inside the function itself, i.e. I am creating a new figure environment for every figure, instead of editing the same environment multiple times.\n","  \"\"\"\n","\n","  if args.viz:\n","\n","    fig = plt.figure(figsize=(12, 4), facecolor='white')  #'facecolor is the background colour.\n","    ax_traj = fig.add_subplot(131, frameon=False)         #add axes to the figure as part of the subplot arrangement.\n","    ax_phase = fig.add_subplot(132, frameon=False)\n","    ax_vecfield = fig.add_subplot(133, frameon=False)\n","\n","    ax_traj.set_title('Trajectories')\n","    ax_traj.set_xlabel('t')\n","    ax_traj.set_ylabel('x,y')\n","    ax_traj.plot(t.cpu().numpy(), true_y.cpu().numpy()[:, 0, 0], t.cpu().numpy(), true_y.cpu().numpy()[:, 0, 1], 'g-')\n","    ax_traj.plot(t.cpu().numpy(), pred_y.cpu().numpy()[:, 0, 0], '--', t.cpu().numpy(), pred_y.cpu().numpy()[:, 0, 1], 'b--')\n","    ax_traj.set_xlim(t.cpu().min(), t.cpu().max())\n","    ax_traj.set_ylim(-2, 2)\n","\n","    ax_phase.set_title('Phase Portrait')\n","    ax_phase.set_xlabel('x')\n","    ax_phase.set_ylabel('y')\n","    ax_phase.plot(true_y.cpu().numpy()[:, 0, 0], true_y.cpu().numpy()[:, 0, 1], 'g-')\n","    ax_phase.plot(pred_y.cpu().numpy()[:, 0, 0], pred_y.cpu().numpy()[:, 0, 1], 'b--')\n","    ax_phase.set_xlim(-2, 2)\n","    ax_phase.set_ylim(-2, 2)\n","\n","    ax_vecfield.set_title('Learned Vector Field')\n","    ax_vecfield.set_xlabel('x')\n","    ax_vecfield.set_ylabel('y')\n","\n","    y, x = np.mgrid[-2:2:21j, -2:2:21j]\n","    dydt = odefunc(0, torch.Tensor(np.stack([x, y], -1).reshape(21 * 21, 2)).to(device)).cpu().detach().numpy()\n","    mag = np.sqrt(dydt[:, 0]**2 + dydt[:, 1]**2).reshape(-1, 1)\n","    dydt = (dydt / mag)\n","    dydt = dydt.reshape(21, 21, 2)\n","\n","    ax_vecfield.streamplot(x, y, dydt[:, :, 0], dydt[:, :, 1], color=\"black\")\n","    ax_vecfield.set_xlim(-2, 2)\n","    ax_vecfield.set_ylim(-2, 2)\n","\n","    fig.tight_layout()\n","    plt.savefig('png/{:03d}'.format(itr))\n","    plt.draw()\n","    plt.pause(0.001)\n","    plt.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uKwSmv5KzJwq"},"source":["\"\"\"\n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","It is from this point on that the details are not specific to this example. The method relates to how to implement a NODE and is therefore more \n","important to understand.\n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Uo8kpZybTYMh"},"source":["class ODEFunc(nn.Module):\n","\n","    def __init__(self):\n","        super(ODEFunc, self).__init__()\n","\n","        #Define a very simple neural network architecture with 1 hidden layer.\n","        self.net = nn.Sequential(\n","            nn.Linear(2, 50),\n","            nn.Tanh(),\n","            nn.Linear(50, 2),\n","        )\n","\n","        #Initialise the weights and biases for the linear layers.\n","        #The isinstance functions checks that the first input is an instance or subclass of the second argument.\n","\n","        for m in self.net.modules():\n","            if isinstance(m, nn.Linear):\n","                nn.init.normal_(m.weight, mean=0, std=0.1)\n","                nn.init.constant_(m.bias, val=0)\n","\n","    #The forward function defines how the data is passed through the neural net. \n","    #In particular, it is called when you apply the neural net to an input variable.\n","    #We act the net on y**3 such that it is only learning to represent the matrix (see class Lambda)\n","\n","    def forward(self, t, y):\n","        return self.net(y**3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GXOq7FquTYbd"},"source":["class RunningAverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","\n","    def __init__(self, momentum=0.99):\n","        self.momentum = momentum\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = None\n","        self.avg = 0\n","\n","    def update(self, val):\n","        if self.val is None:\n","            self.avg = val\n","        else:\n","            self.avg = self.avg * self.momentum + val * (1 - self.momentum)\n","        self.val = val"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GP337SbeTYl6"},"source":["if __name__ == '__main__':\n","\n","    ii = 0\n","\n","    func = ODEFunc().to(device)\n","    \n","    optimizer = optim.RMSprop(func.parameters(), lr=1e-3) #func.parameters are the parameters to optimise.\n","    end = time.time()\n","\n","    time_meter = RunningAverageMeter(0.97)\n","    \n","    loss_meter = RunningAverageMeter(0.97)\n","\n","    for itr in range(1, args.niters + 1):\n","        optimizer.zero_grad()                                 #Clears x.grad for every parameter x. It’s important to call this before loss.backward(), otherwise you’ll accumulate the gradients from multiple passes.\n","        batch_y0, batch_t, batch_y = get_batch()              #Data that we will use.\n","        pred_y = odeint(func, batch_y0, batch_t).to(device)   #Calculate output values from the NODE system. In other words, we compute a forward pass.\n","        loss = torch.mean(torch.abs(pred_y - batch_y))        #Calculate the loss.\n","        loss.backward()                                       #Calculates the gradient of the loss surface. These are accumulated into x.grad for every parameter x.\n","        optimizer.step()                                      #Updates the value of x using the gradient x.grad.  \n","\n","        time_meter.update(time.time() - end)\n","        loss_meter.update(loss.item())\n","\n","        #This essentially prints the loss, etc. at regular intervals. It does so by evaluating the predicted values over all time steps of the ODE\n","        #instead of just a batch sample. I assume this means it gives the exact value of the loss at that stage.\n","\n","        if itr % args.test_freq == 0:\n","            with torch.no_grad():\n","                pred_y = odeint(func, true_y0, t)\n","                loss = torch.mean(torch.abs(pred_y - true_y))\n","                print('Iter {:04d} | Total Loss {:.6f}'.format(itr, loss.item()))\n","                visualize_3(true_y, pred_y, func, ii)\n","                ii += 1\n","\n","        end = time.time()"],"execution_count":null,"outputs":[]}]}